[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nTopic\nReadings ISLP\nMaterial*\nSupplementary Materials\n\n\n\n\nWeek 1\nSyllabus, Logistics, and Introduction.\nCh. 1; Ch. 2;\nslidesbook lab\n- Video: Statistical Learning: 2.1 Introduction to Regression Models- Video: Statistical Learning: 2.2 Dimensionality and Structured Models- Video: Statistical Learning: 2.3 Model Selection and Bias Variance Tradeoff- Video: Statistical Learning: 2.4 Classification- Video: Statistical Learning: 2.Py Data Types, Arrays, and Basics - 2023- Video: Statistical Learning: 2.Py.3 Graphics - 2023- Video: Statistical Learning: 2.Py Indexing and Dataframes - 2023\n\n\nWeek 2\nLinear Regression\nCh. 3.\nslidesbook lab\n- Video: Statistical Learning: 3.1 Simple linear regression- Video: Statistical Learning: 3.2 Hypothesis Testing and Confidence Intervals- Video: Statistical Learning: 3.3 Multiple Linear Regression- Video: Statistical Learning: 3.4 Some important questions- Video: Statistical Learning: 3.5 Extensions of the Linear Model- Video: Statistical Learning: 3.Py Linear Regression and statsmodels Package - 2023- Video: Statistical Learning: 3.Py Multiple Linear Regression Package - 2023- Video: Statistical Learning: 3.Py Interactions, Qualitative Predictors and Other Details I 2023\n\n\nWeek 3\nClassification\nCh. 4\nslidesbook lab\n- Video: Statistical Learning: 4.1 Introduction to Classification Problems- Video: Statistical Learning: 4.2 Logistic Regression- Video: Statistical Learning: 4.3 Multivariate Logistic Regression- Video: Statistical Learning: 4.4 Logistic Regression Case Control Sampling and Multiclass- Video: Statistical Learning: 4.5 Discriminant Analysis- Video: Statistical Learning: 4.6 Gaussian Discriminant Analysis (One Variable)- Video: Statistical Learning: 4.7 Gaussian Discriminant Analysis (Many Variables)- Video: Statistical Learning: 4.8 Generalized Linear Models- Video: Statistical Learning: 4.9 Quadratic Discriminant Analysis and Naive Bayes- Video: Statistical Learning: 4.Py Logistic Regression I 2023- Video: Statistical Learning: 4.Py Linear Discriminant Analysis (LDA) I 2023- Video: Statistical Learning: 4.Py K-Nearest Neighbors (KNN) I 2023\n\n\nWeek 4\nResampling Methods\nCh. 5\nslidesbook lab\n- Video: Statistical Learning: 5.1 Cross Validation- Video: Statistical Learning: 5.2 K-fold Cross Validation- Video: Statistical Learning: 5.3 Cross Validation the wrong and right way- Video: Statistical Learning: 5.4 The Bootstrap- Video: Statistical Learning: 5.5 More on the Bootstrap- Video: Statistical Learning: 5.Py Cross-Validation I 2023- Video: Statistical Learning: 5.Py Bootstrap I 2023- Book Chapter: Modern Dive -Bootstrapping and Confidence Intervals\n\n\nWeek 5\nLinear Model Selection & Regularization\nCh. 6\nslidesbook lab\n- Video: Statistical Learning: 6.1 Introduction and Best Subset Selection- Video: Statistical Learning: 6.2 Stepwise Selection- Video: Statistical Learning: 6.3 Backward stepwise selection- Video: Statistical Learning: 6.4 Estimating test error- Video: Statistical Learning: 6.5 Validation and cross validation- Video: Statistical Learning: 6.6 Shrinkage methods and ridge regression- Video: Statistical Learning: 6.7 The Lasso- Video: Statistical Learning: 6.8 Tuning parameter selection- Video: Statistical Learning: 6.9 Dimension Reduction Methods- Video: Statistical Learning: 6.10 Principal Components Regression and Partial Least Squares- Video: Statistical Learning: 6.Py Stepwise Regression I 2023- Video: Statistical Learning: 6.Py Ridge Regression and the Lasso I 2023\n\n\nWeek 6\nBeyond Linearity\nCh. 7\nslidesbook lab\n- Video: Statistical Learning: 7.1 Polynomials and Step Functions- Video: Statistical Learning: 7.2 Piecewise Polynomials and Splines- Video: Statistical Learning: 7.3 Smoothing Splines- Video: Statistical Learning: 7.4 Generalized Additive Models and Local Regression- Video: Statistical Learning: 7.Py Polynomial Regressions and Step Functions I 2023- Video: Statistical Learning: 7.Py Splines I 2023- Video: Statistical Learning: 7.Py Generalized Additive Models (GAMs) I 2023\n\n\nWeek 7\nTree-Based Methods\nCh. 8\nslidesbook lab\n- Video: Statistical Learning: 8.1 Tree based methods- Video: Statistical Learning: 8.2 More details on Trees- Video: Statistical Learning: 8.3 Classification Trees- Video: Statistical Learning: 8.4 Bagging- Video: Statistical Learning: 8.5 Boosting- Video: Statistical Learning: 8.6 Bayesian Additive Regression Trees- Video: Statistical Learning: 8.Py Tree-Based Methods I 2023\n\n\nWeek 8\nSupport Vector Machines\nCh. 9\nslidesbook lab\n- Video: Statistical Learning: 9.1 Optimal Separating Hyperplane- Video: Statistical Learning: 9.2.Support Vector Classifier- Video: Statistical Learning: 9.3 Feature Expansion and the SVM- Video: Statistical Learning: 9.4 Example and Comparison with Logistic Regression- Video: Statistical Learning: 9.Py Support Vector Machines I 2023- Video: Statistical Learning: 9.Py ROC Curves I 2023\n\n\nWeek 09\nUnsupervised Learning\nCh. 12\nslidesbook lab\n- Video: Statistical Learning: 12.1 Principal Components- Video: Statistical Learning: 12.2 Higher order principal components- Video: Statistical Learning: 12.3 k means Clustering- Video: Statistical Learning: 12.4 Hierarchical Clustering- Video: Statistical Learning: 12.5 Matrix Completion- Video: Statistical Learning: 12.6 Breast Cancer Example- Video: Statistical Learning: 12.Py Principal Components I 2023- Video: Statistical Learning: 12.Py Clustering I 2023- Video: Statistical Learning: 12.Py Application: NCI60 Data I 2023\n\n\nWeek 10\nElements of Data Communication\n.\nslides\nPoster TemplateRubricVideo: Creating a Professional Poster\n\n\nWeek 11\nFinal Project\n.\n.\n.\n\n\nWeek 12\nFinal Project\n.\n.\n.\n\n\nWeek 13\nDeep Learning\nCh. 10\nslidesbook labcourse lab\n- Video: Statistical Learning: 10.1 Introduction to Neural Networks- Video: Statistical Learning: 10.2 Convolutional Neural Networks- Video: Statistical Learning: 10.3 Document Classification- Video: Statistical Learning: 10.4 Recurrent Neural Networks- Video: Statistical Learning: 10.6 Fitting Neural Networks- Video: Statistical Learning: 10.7 Interpolation and Double Descent- Video: Statistical Learning: 10.Py Single Layer Model: Hitters Data I 2023- Video: Statistical Learning: 10.Py Multilayer Model: MNIST Digit Data I 2023- Video: Statistical Learning: 10.Py Convolutional Neural Network: CIFAR Image Data I 2023- Video: Statistical Learning: 10.Py Document Classification and Recurrent Neural Networks I 2023\n\n\nWeek 14\nDeep Learning\nCh. 10\n.\n.\n\n\nWeek 15\nDeep Learning\nCh. 10\n.\n.\n\n\n\n* The course slides and labs are based on the ISLP book, “An Introduction to Statistical Learning with Applications in Python” by James, G., Witten, D., Hastie, T., and Tibshirani, R., and have been adapted to suit the specific needs of our course.",
    "crumbs": [
      "Schedule and Material"
    ]
  },
  {
    "objectID": "index.html#course-description-and-objectives",
    "href": "index.html#course-description-and-objectives",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, support vector machines, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 414\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required): Google Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "MGMT 47400: Predictive Analytics",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#overview",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#overview",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nStructured Data\nUnstructured Data\nDatabases\nRelational Databases\n\n\n\nNon Relational Databases\nMeta Data and Dictionary (code book)"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#what-is-data-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#what-is-data-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "What is Data?",
    "text": "What is Data?\n\n\n\n\nData refers to raw, unprocessed facts, figures, and symbols that represent information about the world around us. Data can take many forms, such as numbers, text, images, audio, and video, and it can be quantitative (numerical) or qualitative (categorical).\n\n\n\nData (Wiki)"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#types-of-data-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#types-of-data-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Types of Data",
    "text": "Types of Data\n\n\nStructured Data: Data that is organized in a defined format, such as rows and columns in a database (e.g., an Excel spreadsheet).\nUnstructured Data: Data that does not have a predefined structure, such as text, emails, social media posts, videos, and images.\nSemi-Structured Data: Data that does not conform to a strict structure but contains tags or markers to separate elements (e.g., XML or JSON files)."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#structured-business-data",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#structured-business-data",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Structured Business Data",
    "text": "Structured Business Data\n\n\nBusiness data refers to the information gathered by an organization, such as customer data, financial data, sales data, employee data, and more. Business data can come from a wide variety of sources - from customers’ purchase transactions and social media activities to market research and financial reports.\n\nBecause structured data is typically organized in a specific format that can be easily searched and analyzed, most business analytics are designed and applied to structured data. This course will focus solely on using and analyzing structured data."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#introduction-to-databases-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#introduction-to-databases-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Introduction to Databases",
    "text": "Introduction to Databases\n\n\n\n\nA Database is a structured collection of data, typically managed by a Database Management System (DBMS) to efficiently store, retrieve, and manage data for various applications."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-relational-model-1970s",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-relational-model-1970s",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Relational Model: 1970s",
    "text": "The Relational Model: 1970s\n\n\n\n\n\n\n\n\n\n\n\n\n\nEdgar F. Codd\n\n\n\n\nEdgar F. Codd: Proposed the relational model in 1970, which became the foundation for modern databases.\nRDBMS: A Relational Database Management System (RDBMS) is used to maintain relational databases.\nSQL: Structured Query Language (SQL) was developed to interact (query and update) with relational databases.\nAdoption: The relational model became dominant in the 1980s, with systems like Oracle, IBM DB2, and Microsoft SQL Server emerging. Nowadays, open-source systems like MySQL are used by big companies to handle their relational data."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-rise-of-nosql-2000s",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#the-rise-of-nosql-2000s",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "The Rise of NoSQL: 2000s",
    "text": "The Rise of NoSQL: 2000s\n\nLimitations of RDBMS: Traditional relational databases struggled with the scale and complexity of modern web applications.\n\nExample: A social media platform with millions of users posting, commenting, and liking content simultaneously.\nLimitation: RDBMS typically scale vertically (adding more power to a single server), which becomes increasingly expensive and challenging as the database grows. In contrast, NoSQL databases like Cassandra or MongoDB are designed to scale horizontally (adding more servers), making them better suited for handling such large-scale data across distributed systems.\n\nNoSQL Databases: Emerged to address these challenges. They offer flexibility, scalability, and performance improvements. They are designed to scale horizontally (adding more servers), making them better suited for handling such large-scale data across distributed systems.\n\nTypes: Document (e.g., MongoDB), Key-Value (e.g., Redis), Column-Family (e.g., Cassandra), and Graph (e.g., Neo4j).\nUse Cases: Ideal for big data, handling unstructured data, real-time web applications, and distributed systems."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#modern-database-trends",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#modern-database-trends",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Modern Database Trends",
    "text": "Modern Database Trends\n\n\nNewSQL: Combines the scalability of NoSQL with the Atomicity, Consistency, Isolation, and Durability (ACID) guarantees of traditional relational databases (e.g., Google Spanner).\nCloud Databases: The adoption of cloud computing has led to the rise of managed database services (e.g., Amazon RDS, Google Cloud SQL).\nData Lakes: A storage repository that holds vast amounts of raw data in its native format (e.g., AWS S3, Azure Data Lake)."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA relational database links data tables through pre-defined and shared fields in various data tables, establishing relationships.\nThis permits more efficient organization and utilization of data across multiple tables.\nMoreover, a relational database serves as a potent tool for handling extensive data volumes and managing complex data structures."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-2",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery 1: Flights from a Specific Carrier\n\n\nSELECT flights.year, flights.month, flights.day, \n       flights.flight, airlines.names AS airline_name\nFROM flights\nJOIN airlines ON flights.carrier = airlines.carrier\nWHERE airlines.names = 'American Airlines';  \n-- Replace 'American Airlines' with the desired carrier name"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-3",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-3",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery 2: Weather Conditions at the Time of a Specific Flight\n\n\nSELECT flights.flight, flights.origin, \n        flights.dest, weather.*\nFROM flights\nJOIN weather \nON flights.year = weather.year AND\n   flights.month = weather.month AND\n   flights.day = weather.day AND\n   flights.hour = weather.hour AND\n   flights.origin = weather.origin\nWHERE flights.flight = 'AA123';  \n-- Replace 'AA123' with the desired flight number"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-4",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#relational-database-4",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Relational Database",
    "text": "Relational Database\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuery 3: Count of Flights Per Airport\n\n\n\nSELECT airports.faa, \n        COUNT(flights.flight) AS flight_count\nFROM flights\nJOIN airports ON flights.origin = airports.faa\nGROUP BY airports.faa\nORDER BY flight_count DESC;"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Warehouse",
    "text": "Data Warehouse\n\n\nA Data Warehouse is a large and comprehensive storage system that consolidates data from various sources, including relational databases, into a centralized repository, much like a university campus that encompasses buildings of various functions.\n\nThe primary purpose of a data warehouse is to facilitate data storage, reporting, and analysis for business intelligence and decision-making purposes."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Information Management System",
    "text": "Information Management System\n\n\nThe primary goal of an Information Management System (IMS) is to ensure that accurate, timely, and relevant information is generated and available to the right people at the right time, enabling efficient and informed decision-making processes."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-2",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#information-management-system-2",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Information Management System",
    "text": "Information Management System\n\n\nKey Components: Data sources, ETL (Extract, Transform, Load), Data Warehouse, OLAP Engine, and Analytic Reporting."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-sources",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-sources",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Sources",
    "text": "Data Sources\n\n\nFinance Data: Information related to financial transactions, budgeting, and accounting.\nCRM Data: Customer Relationship Management data, including customer interactions, sales, and service records.\nOperations Data: Data concerning the day-to-day operations of a business, such as supply chain, inventory, and production.\nMore Data: Any additional data sources that contribute to the organization’s information ecosystem."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#etl-process-extract-transform-load",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#etl-process-extract-transform-load",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "ETL Process (Extract, Transform, Load)",
    "text": "ETL Process (Extract, Transform, Load)\n\n\nExtract: Data is collected from various sources, such as finance systems, CRM systems, and operations databases.\nTransform: The extracted data is cleaned, aggregated, and formatted to fit the data warehouse schema.\nLoad: The transformed data is loaded into the Data Warehouse, where it is stored and made available for analysis."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse-and-olap-engine",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-warehouse-and-olap-engine",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Warehouse and OLAP Engine",
    "text": "Data Warehouse and OLAP Engine\n\n\nData Warehouse: A centralized repository that stores integrated data from multiple sources, optimized for query and analysis.\nOLAP Engine (Online Analytical Processing): Tools that allow for complex analytical queries and multi-dimensional data analysis.\n\nExample: Analyzing sales trends over time, across different regions, or by product categories."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#analytic-reporting-and-advanced-analytics",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#analytic-reporting-and-advanced-analytics",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Analytic Reporting and Advanced Analytics",
    "text": "Analytic Reporting and Advanced Analytics\n\n\nAnalytic Reporting Engine: Produces reports and dashboards for users to visualize and understand the data.\n\nAd Hoc Reporting: Enables users to create custom reports on-demand.\nDashboards: Provides a visual summary of key performance indicators (KPIs) and metrics.\n\nAdvanced Analytics: Includes data mining, predictive modeling, and other sophisticated analytical techniques to uncover hidden patterns and insights."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#users-and-decision-making",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#users-and-decision-making",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Users and Decision-Making",
    "text": "Users and Decision-Making\n\n\nUsers: Business analysts, managers, and executives who use the IMS to make informed decisions.\nOutcome: The IMS enables data-driven decision-making, improving efficiency, reducing risks, and enhancing overall business performance."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#meta-data-and-data-dictionary-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#meta-data-and-data-dictionary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Meta Data and Data Dictionary",
    "text": "Meta Data and Data Dictionary\n\n\n\nMetadata is essentially information about structured data. It can include details like the date and time a database or file was created, who created it, and what types of information it contains.\nA data dictionary is a more specific type of metadata that describes the structure, content, and format of a dataset. It’s like a guidebook and a codebook that provides a comprehensive list of all the variables or columns in a dataset, along with their definitions, data types, and other attributes.\n\nWithout it, you might get lost in a sea of information and struggle to make sense of it all."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#mtcars",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#mtcars",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "mtcars",
    "text": "mtcars\n\n\n\nThe mtcars data file provides information on various features of different brands of cars, including their engine size, horsepower, and fuel efficiency. The dataset is structured as a table, where each row represents a different car, and each column represents a different variable or feature of the cars."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-dictionary-for-mtcars",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#data-dictionary-for-mtcars",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Data Dictionary for mtcars",
    "text": "Data Dictionary for mtcars\n\n\n?mtcars"
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#learning-path-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#learning-path-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Learning Path",
    "text": "Learning Path\nThere are plenty of college courses to choose (course titles may vary by schools):\n\nDatabase Management Systems: This course focuses on the design, implementation, and management of databases, teaching students how to organize and manage data effectively.\nInformation Security and Privacy: This course covers the principles and practices of securing information and ensuring data privacy, preparing students to handle data security challenges.\nData Governance and Management: This course explores the governance and management of data assets, including data quality, data integration, and data lifecycle management.\nInformation Systems Analysis and Design: This course teaches students how to analyze business requirements and design information systems to meet organizational needs."
  },
  {
    "objectID": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#summary-1",
    "href": "lecture_slides/02_data_organization_decision/02_data_organization_decision.html#summary-1",
    "title": " MGMT 17300: Data Mining Lab ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nStructured Data: Highly organized and formatted data that is easily searchable (e.g., tables with rows and columns).\nDatabases: Used to store and manage structured data efficiently.\n\nTypes: Include relational databases (RDBMS) like MySQL and NoSQL databases like MongoDB.\nFunctionality: Provides tools for querying, updating, and managing large datasets.\n\nRelational Databases: Organizes data into tables that can be linked by shared keys.\n\nBenefits: Ensures data integrity and supports complex queries and transactions.\nKey Components: Tables, primary and foreign keys, SQL for data manipulation.\n\n\n\n\n\n\nNon-Relational Databases: NoSQL databases designed for unstructured data and scalability.\n- Advantages: Handle large-scale data across distributed systems more effectively than traditional RDBMS.\nMeta Data: Information describing other data, providing context and making it easier to understand.\nData Dictionary: Detailed description of dataset variables, ensuring consistent data usage."
  },
  {
    "objectID": "syllabus.html#course-description-and-objectives",
    "href": "syllabus.html#course-description-and-objectives",
    "title": "Syllabus",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, support vector machines, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 414\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required): Google Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\n\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\nAs part of a university-wide initiative, the Business School has adopted an Official Grading Policy that caps the overall class GPA at 3.3. Final letter grades are determined by curving final percentages, subject to any extra-credit exceptions discussed in this syllabus. While you will see your final percentage in Brightspace, individual grade thresholds will not be disclosed before official submissions.\n\n\n\nAssessment\nWeight\n\n\n\n\nAttendance/Participation\n10%\n\n\nQuizzes\n20%\n\n\nHomework\n30%\n\n\nFinal Project\n40%\n\n\n\n\nAttendance and Participation\nAttend class, participate in activities, and complete any participatory exercises. Random attendance checks will be used to measure involvement. According to Purdue regulations, students are expected to attend every class/lab meeting for which they are registered.\n\n\nQuizzes\nRegular quizzes based on lecture material will be administered, with no drops. Due dates and details will be on Brightspace. Quizzes help reinforce content and maintain steady engagement.\n\n\nHomework\nHomework assignments offer practical, hands-on exposure to data mining tasks. Expect multiple-choice questions requiring analysis of provided results. Deadlines will be posted in Brightspace. These assignments are crucial for building technical and analytical skills.\n\n\nFinal Project\nIn groups, students will complete a practical predictive analytics project culminating in a poster presentation at the Undergraduate Research Conference. A comprehensive set of project guidelines will be provided, and the assessment structure will adhere to the following criteria:\n\nMilestone Deliverables (40%): Students will submit incremental project components on specific due dates. These deliverables allow for early feedback and ensure steady progress throughout the semester. Grades will reflect each milestone’s clarity, completeness, and timely submission.\nPeer Evaluation (20%): To encourage accountability and productive teamwork, students will evaluate their peers’ contributions. These assessments help ensure balanced participation and measure collaborative effectiveness.\nPoster Presentation at the Purdue Undergraduate Research Conference (40%): A poster template and assessment rubric will be shared, and you are encouraged to review previous award-winning student posters for inspiration. Your final posters must be submitted by the due date indicated in the syllabus, after which they will be printed and distributed during a dedicated Poster Presentation Preparation class. Additional details on the conference can be found at https://www.purdue.edu/undergrad-research/conferences/index.php. As the event may not coincide with our regular class time, please communicate with your other course instructors in advance regarding potential scheduling conflicts. If any issues arise, please let me know. We will not hold our usual class immediately following the Poster Presentation, allowing you time to rest and catch up on other coursework. Consult the course schedule for further details.\n\n\n\nGrade Challenges\nGrades and solutions will be posted soon after each assignment deadline. Students have 7 calendar days from the grade posting to submit any challenge (3 days for the final two quizzes and homework assignments). Challenges must be based on legitimate discrepancies regarding data mining principles or grading accuracy.\n\nReview posted solutions thoroughly.\n\nIf you suspect an error, email Dr. Moreira with:\n\nCourse name, section, and lecture day/time\n\nYour name and Student ID\n\nAssignment/Exam Title or Number\n\nSpecific deduction questioned\n\nClear rationale referencing solutions or rubrics\n\n\n\nNo grades will be discussed in-class. Please use office hours for clarifications. After the 7-day (or 3-day) window, grades are final.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies-and-additional-details",
    "href": "syllabus.html#course-policies-and-additional-details",
    "title": "Syllabus",
    "section": "Course Policies and Additional Details",
    "text": "Course Policies and Additional Details\n\nExtra Credit Opportunities\n\nCheck the Course Syllabus document on Brightspace for details.\n\n\n\nAI Policy\n\nYou may use AI tools to support your learning (e.g., clarifying concepts, generating examples), but:\n\nDo not use AI for requesting solutions or exams.\n\nPractice refining prompts to get better AI outputs.\n\nVerify all AI-generated content for accuracy.\n\nCite any AI usage in your documents.\n\n\n\n\nAdditional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.\n\n\nSubject to Change Policy\nWhile we will endeavor to maintain the course schedule, the syllabus may be adjusted to accommodate the learning pace and needs of the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overview",
    "href": "lecture_slides/01_introduction/01_introduction.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroductions\nCourse Overview and Logistics\nMotivation\nCourse Objectives\n\n\n\nSupervised Learning\nUnsupervised Learning\nStatistical Learning Overview\n\nWhat is Statistical Learning?\nParametric and Structured Models\nAssessing Model Accuracy\nClassification Problems\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor",
    "text": "Instructor\n\n\n\n\n\n\n\n\n\n\n\ndmoreira@purdue.edu\nhttps://davi-moreira.github.io/\n\n\nClinical Assistant Professor in the Management Department at Purdue University;\n\n\n\nMy academic work addresses Political Communication, Data Science, Text as Data, Artificial Intelligence, and Comparative Politics.\n\n\n\nM&E Specialist consultant - World Bank (Brazil, Mozambique, Angola, and DRC)"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Most Exciting Game in History - Video"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#instructors-passions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Instructor’s Passions",
    "text": "Instructor’s Passions\n\n\nNYT - How John Travolta Became the Star of Carnival-Video."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#students",
    "href": "lecture_slides/01_introduction/01_introduction.html#students",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Students",
    "text": "Students\n\n\nIt is your turn! - 5 minutes\n\n\n\nPresent yourself to your left/right colleague and tell her/him what are the current two main passions in your life."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#course-overview-and-logistics-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Course Overview and Logistics",
    "text": "Course Overview and Logistics\n\nMaterials:\n\nBrightspace\nCourse Webpage\n\nSyllabus\n\nClass Times & Location: check the course syllabus.\nOffice Hours: check the course syllabus for group and individual appointments.\n\nSchedule"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "href": "lecture_slides/01_introduction/01_introduction.html#spam-detection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Spam Detection",
    "text": "Spam Detection\n\n\n\nData from 4601 emails sent to an individual (named George, at HP Labs, before 2000). Each is labeled as spam or email.\nGoal: build a customized spam filter.\nInput features: relative frequencies of 57 of the most commonly occurring words and punctuation marks in these email messages.\n\n\n\n\nWord\nSpam\nEmail\n\n\n\n\ngeorge\n0.00\n1.27\n\n\nyou\n2.26\n1.27\n\n\nhp\n0.02\n0.90\n\n\nfree\n0.52\n0.07\n\n\n!\n0.51\n0.11\n\n\nedu\n0.01\n0.29\n\n\nremove\n0.28\n0.01\n\n\n\nAverage percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest difference between spam and email."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "href": "lecture_slides/01_introduction/01_introduction.html#zip-code",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Zip Code",
    "text": "Zip Code\n\n\nIdentify the numbers in a handwritten zip code."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "href": "lecture_slides/01_introduction/01_introduction.html#netflix-prize",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Netflix Prize",
    "text": "Netflix Prize\n\n\n\n\n\n\n\n\n\n\n\nVideo: Winning the Netflix Prize\nNetflix Prize - Wiki"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "href": "lecture_slides/01_introduction/01_introduction.html#starting-point",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Starting point",
    "text": "Starting point\n\n\n\n\nOutcome measurement \\(Y\\) (also called dependent variable, response, target).\nVector of \\(p\\) predictor measurements \\(X\\) (also called inputs, regressors, covariates, features, independent variables).\nIn the regression problem, \\(Y\\) is quantitative (e.g., price, blood pressure).\nIn the classification problem, \\(Y\\) takes values in a finite, unordered set (e.g., survived/died, digit 0–9, cancer class of tissue sample).\nWe have training data \\((x_1, y_1), \\ldots, (x_N, y_N)\\). These are observations (examples, instances) of these measurements."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "href": "lecture_slides/01_introduction/01_introduction.html#objectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Objectives",
    "text": "Objectives\nOn the basis of the training data, we would like to:\n\nAccurately predict unseen test cases.\nUnderstand which inputs affect the outcome, and how.\nAssess the quality of our predictions and inferences."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "href": "lecture_slides/01_introduction/01_introduction.html#philosophy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Philosophy",
    "text": "Philosophy\n\n\n\nIt is important to understand the ideas behind the various techniques, in order to know how and when to use them.\nWe wil understand the simpler methods first to grasp the more sophisticated ones later.\nIt is important to accurately assess the performance of a method, to know how well or how badly it is working."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#unsupervised-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\n\n\nNo outcome variable, just a set of predictors (features) measured on a set of samples.\nObjective is more fuzzy:\n\nFind groups of samples that behave similarly.\nFind features that behave similarly.\nFind linear combinations of features with the most variation.\n\nDifficult to know how well we are doing.\nDifferent from supervised learning, but can be useful as a pre-processing step for supervised learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-statistical-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\n\n\n\n\n\n\n\n\n\nShown are Sales vs TV, Radio, and Newspaper, with a blue linear-regression line fit separately to each.\nCan we predict Sales using these three?\n\nPerhaps we can do better using a model:\n\\[\n\\text{Sales} \\approx f(\\text{TV}, \\text{Radio}, \\text{Newspaper})\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#notation",
    "href": "lecture_slides/01_introduction/01_introduction.html#notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Notation",
    "text": "Notation\n\n\n\n\nSales is a response or target that we wish to predict. We generically refer to the response as \\(Y\\).\nTV is a feature, or input, or predictor; we name it \\(X_1\\).\nLikewise, name Radio as \\(X_2\\), and so on.\nThe input vector collectively is referred to as:\n\n\\[\nX = \\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{pmatrix}\n\\]\nWe write our model as:\n\\[\nY = f(X) + \\epsilon\n\\]\nwhere \\(\\epsilon\\) captures measurement errors and other discrepancies."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "href": "lecture_slides/01_introduction/01_introduction.html#what-is-fx-good-for",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is \\(f(X)\\) Good For?",
    "text": "What is \\(f(X)\\) Good For?\n\nWith a good \\(f\\), we can make predictions of \\(Y\\) at new points \\(X = x\\).\nUnderstand which components of \\(X = (X_1, X_2, \\ldots, X_p)\\) are important in explaining \\(Y\\), and which are irrelevant.\n\nExample: Seniority and Years of Education have a big impact on Income, but Marital Status typically does not.\n\nDepending on the complexity of \\(f\\), understand how each component \\(X_j\\) affects \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#is-there-an-ideal-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is There an Ideal \\(f(X)\\)?",
    "text": "Is There an Ideal \\(f(X)\\)?\n\n\nIn particular, what is a good value for \\(f(X)\\) at a selected value of \\(X\\), say \\(X = 4\\)?\n\n\n\n\n\n\n\n\n\n\nThere can be many \\(Y\\) values at \\(X=4\\). A good value is:\n\\[\nf(4) = E(Y|X=4)\n\\]\nwhere \\(E(Y|X=4)\\) means the expected value (average) of \\(Y\\) given \\(X=4\\).\nThis ideal \\(f(x) = E(Y|X=x)\\) is called the regression function."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-regression-function-fx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Regression Function \\(f(x)\\)",
    "text": "The Regression Function \\(f(x)\\)\n\n\n\nIs also defined for a vector \\(\\mathbf{X}\\).\n\n\\[\nf(\\mathbf{x}) = f(x_1, x_2, x_3) = \\mathbb{E}[\\,Y \\mid X_1 = x_1,\\, X_2 = x_2,\\, X_3 = x_3\\,].\n\\]\n\n\nIs the ideal or optimal predictor of \\(Y\\) in terms of mean-squared prediction error:\n\n\\[\n  f(x) = \\mathbb{E}[Y \\mid X = x]\n  \\quad\\text{is the function that minimizes}\\quad\n  \\mathbb{E}[(Y - g(X))^2 \\mid X = x]\n  \\text{ over all } g \\text{ and for all points } X = x.\n\\]\n\n\n\n\\(\\varepsilon = Y - f(x)\\) is the irreducible error.\n\nEven if we knew \\(f(x)\\), we would still make prediction errors because at each \\(X = x\\) there is a distribution of possible \\(Y\\) values.\n\n\n\n\n\nFor any estimate \\(\\hat{f}(x)\\) of \\(f(x)\\),\n\n\\[\n    \\mathbb{E}\\bigl[(Y - \\hat{f}(X))^2 \\mid X = x\\bigr]\n    = \\underbrace{[\\,f(x) - \\hat{f}(x)\\,]^2}_{\\text{Reducible}}\n      \\;+\\; \\underbrace{\\mathrm{Var}(\\varepsilon)}_{\\text{Irreducible}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "href": "lecture_slides/01_introduction/01_introduction.html#how-to-estimate-f",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Estimate \\(f\\)",
    "text": "How to Estimate \\(f\\)\n\n\nOften, we lack sufficient data points for exact computation of \\(E(Y|X=x)\\).\nSo, we relax the definition:\n\n\\[\n\\hat{f}(x) = \\text{Ave}(Y|X \\in \\mathcal{N}(x))\n\\]\nwhere \\(\\mathcal{N}(x)\\) is a neighborhood of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest Neighbor Observations",
    "text": "Nearest Neighbor Observations\n\nNearest neighbor averaging can be pretty good for small \\(p\\) — i.e., \\(p \\le 4\\) — and large-ish \\(N\\).\nWe will discuss smoother versions, such as kernel and spline smoothing, later in the course.\nNearest neighbor methods can be lousy when \\(p\\) is large.\n\nReason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions.\nWe need to get a reasonable fraction of the \\(N\\) values of \\(y_i\\) to average in order to bring the variance down (e.g., 10%).\nA 10% neighborhood in high dimensions is no longer truly local, so we lose the spirit of estimating \\(\\mathbb{E}[Y \\mid X = x]\\) via local averaging."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "href": "lecture_slides/01_introduction/01_introduction.html#the-curse-of-dimensionality",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The curse of dimensionality",
    "text": "The curse of dimensionality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop panel: \\(X_1\\) and \\(X_2\\) are uniformly distributed with edges minus one to plus one.\n\n1-Dimensional Neighborhood\n\nFocuses only on \\(X_1\\), ignoring \\(X_2\\).\nNeighborhood is defined by vertical red dotted lines.\nCentered on the target point \\((0, 0)\\).\nExtends symmetrically along \\(X_1\\) until it captures 10% of the data points.\n\n2-Dimensional Neighborhood\n\nNow, Considers both \\(X_1\\) and \\(X_2\\).\nNeighborhood is a circular region centered on the same target point \\((0, 0)\\).\nRadius of the circle expands until it encloses 10% of the total data points.\nThe radius in 2D is much larger than the 1D width due to the need to account for more dimensions.\n\n\n\nBotton panel: We see how far we have to go out in one, two, three, five, and ten dimensions in order to capture a certain fraction of the points.\n\nKey Takeaway: As dimensionality increases, neighborhoods must expand significantly to capture the same fraction of data points, illustrating the curse of dimensionality."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#parametric-and-structured-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Parametric and Structured Models",
    "text": "Parametric and Structured Models\nThe linear model is a key example of a parametric model to deal with the curse of dimensionality:\n\\[\nf_L(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p\n\\]\n\nA linear model is specified in terms of \\(p+1\\) parameters (\\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)).\nWe estimate the parameters by fitting the model to training data.\nAlthough it is almost never correct, it serves as a good and interpretable approximation to the unknown true function \\(f(X)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "href": "lecture_slides/01_introduction/01_introduction.html#comparison-of-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Models",
    "text": "Comparison of Models\n\n\n\n\nLinear model\n\n\\[\n\\hat{f}_L(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X\n\\]\n\n\n\n\n\n\n\n\n\nThe linear model gives a reasonable fit here.\n\n\n\nQuadratic model:\n\n\\[\n\\hat{f}_Q(X) = \\hat{\\beta}_0 + \\hat{\\beta}_1X + \\hat{\\beta}_2X^2\n\\]\n\n\n\n\n\n\n\n\n\nQuadratic models may fit slightly better than linear models in some cases."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "href": "lecture_slides/01_introduction/01_introduction.html#simulated-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Example",
    "text": "Simulated Example\nRed points are simulated values for income from the model:\n\n\\[\n\\text{income} = f(\\text{education}, \\text{seniority}) + \\epsilon\n\\]\n\\(f\\) is the blue surface."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#linear-regression-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression Fit",
    "text": "Linear Regression Fit\nLinear regression model fit to the simulated data:\n\n\\[\n\\hat{f}_L(\\text{education}, \\text{seniority}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times \\text{education} + \\hat{\\beta}_2 \\times \\text{seniority}\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexible-regression-model-fit",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexible Regression Model Fit",
    "text": "Flexible Regression Model Fit\nMore flexible regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data.\n\nHere we use a technique called a thin-plate spline to fit a flexible surface. We control the roughness of the fit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "href": "lecture_slides/01_introduction/01_introduction.html#overfitting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overfitting",
    "text": "Overfitting\nEven more flexible spline regression model \\(\\hat{f}_S(\\text{education}, \\text{seniority})\\) fit to the simulated data. We tunned the parameter all the way down to zero and this surface actually goes through every single data point.\n\nThe fitted model makes no errors on the training data! This is known as overfitting."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "href": "lecture_slides/01_introduction/01_introduction.html#some-trade-offs",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Trade-offs",
    "text": "Some Trade-offs\n\nPrediction accuracy versus interpretability:\n\nLinear models are easy to interpret; thin-plate splines are not.\n\nGood fit versus over-fit or under-fit:\n\nHow do we know when the fit is just right?\n\nParsimony versus black-box:\n\nPrefer simpler models involving fewer variables over black-box predictors."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/01_introduction/01_introduction.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\n\nHigh interpretability: Subset selection, Lasso.\n\nIntermediate: Least squares, Generalized Additive Models, Trees.\n\nHigh flexibility: Support Vector Machines, Deep Learning."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#assessing-model-accuracy-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing Model Accuracy",
    "text": "Assessing Model Accuracy\n\n\nSuppose we fit a model \\(\\hat{f}(x)\\) to some training data \\(Tr = \\{x_i, y_i\\}_{i=1}^N\\), and we wish to evaluate its performance:\n\nCompute the average squared prediction error over the training set \\(Tr\\), the Mean Squared Error (MSE):\n\n\\[\n\\text{MSE}_{Tr} = \\text{Ave}_{i \\in Tr}[(y_i - \\hat{f}(x_i))^2]\n\\]\nHowever, this may be biased toward more overfit models.\n\n\nInstead, use fresh test data \\(Te = \\{x_i, y_i\\}_{i=1}^M\\):\n\n\\[\n\\text{MSE}_{Te} = \\text{Ave}_{i \\in Te}[(y_i - \\hat{f}(x_i))^2]\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop Panel: Model Fits\n\nBlack Curve: The true generating function, representing the underlying relationship we want to estimate.\nData Points: Observations generated from the black curve, with added noise (error).\nFitted Models:\n\nOrange Line: A simple linear model (low flexibility).\nBlue Line: A moderately flexible model, likely a spline or thin plate spline.\nGreen Line: A highly flexible model that closely fits the data points but may overfit.\n\n\nKey Insight:\nThe green model captures the data points well but risks overfitting, while the orange model is too rigid and misses the underlying structure. The blue model strikes a balance.\n\nBotton Panel: Mean Squared Error (MSE)\n\nGray Curve: Training data MSE.\n\nDecreases consistently as flexibility increases.\nFlexible models fit the training data well, but this does not generalize to test data.\n\nRed Curve: Test data MSE across models of increasing flexibility.\n\nStarts high for rigid models (orange line).\nDecreases to a minimum (optimal model complexity, blue line).\nIncreases again for overly flexible models (green line), due to overfitting.\n\n\nKey Takeaway:\nThere is an optimal model complexity (the “magic point”) where test data MSE is minimized. Beyond this point, models become overly complex and generalization performance deteriorates."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-other-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off: Other Examples",
    "text": "Bias-Variance Trade-off: Other Examples\n\n\n\n\n\nHere, the truth is smoother, so smoother fits and linear models perform well.\n\n\n\n\n\n\n\n\n\n\n\n\nHere, the truth is wiggly and the noise is low. More flexible fits perform the best."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off",
    "text": "Bias-Variance Trade-off\n\n\nSuppose we have fit a model \\(\\hat{f}(x)\\) to some training data \\(\\text{Tr}\\), and let \\((x_0, y_0)\\) be a test observation drawn from the population.\nIf the true model is\n\\[\n    Y = f(X) + \\varepsilon\n    \\quad \\text{(with } f(x) = \\mathbb{E}[Y \\mid X = x]\\text{)},\n\\]\nthen\n\\[\n\\mathbb{E}\\Bigl[\\bigl(y_0 - \\hat{f}(x_0)\\bigr)^2\\Bigr]\n    = \\mathrm{Var}\\bigl(\\hat{f}(x_0)\\bigr)\n    + \\bigl[\\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\\bigr]^2\n    + \\mathrm{Var}(\\varepsilon).\n\\]\nThe expectation averages over the variability of \\(y_0\\) as well as the variability in \\(\\text{Tr}\\). Note that\n\\[\n    \\mathrm{Bias}\\bigl(\\hat{f}(x_0)\\bigr)\n    = \\mathbb{E}[\\hat{f}(x_0)] - f(x_0).\n\\]\nTypically, as the flexibility of \\(\\hat{f}\\) increases, its variance increases and its bias decreases. Hence, choosing the flexibility based on average test error amounts to a bias-variance trade-off."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "href": "lecture_slides/01_introduction/01_introduction.html#bias-variance-trade-off-of-the-examples",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bias-Variance Trade-off of the Examples",
    "text": "Bias-Variance Trade-off of the Examples\n\n\nBelow is a schematic illustration of the mean squared error (MSE), bias, and variance curves as a function of the model’s flexibility.\n\n\n\n\n\n\n\n\n\n\n\nMSE (red curve) goes down initially (as the model becomes more flexible) but eventually goes up (as overfitting sets in).\nBias (blue/teal curve) decreases with increasing flexibility.\nVariance (orange curve) increases with increasing flexibility.\n\n\nThe vertical dotted line in each panel suggests a model flexibility that balances both bias and variance in an “optimal” region for minimizing MSE."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Problems",
    "text": "Classification Problems\n\n\nHere the response variable \\(Y\\) is qualitative. For example:\n\nEmail could be classified as spam or ham (good email).\nDigit classification could be one of \\(\\{0, 1, 2, \\dots, 9\\}\\).\n\n\nOur goals are to:\n\nBuild a classifier \\(C(X)\\) that assigns a class label from the set \\(C\\) to a future unlabeled observation \\(X\\).\nAssess the uncertainty in each classification.\nUnderstand the roles of the different predictors among \\(X = (X_1, X_2, \\dots, X_p)\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "href": "lecture_slides/01_introduction/01_introduction.html#ideal-classifier-and-bayes-decision-rule",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ideal Classifier and Bayes Decision Rule",
    "text": "Ideal Classifier and Bayes Decision Rule\n\n\n\n\n\n\n\n\n\n\n\nConsider a classification problem with \\(K\\) possible classes, numbered \\(1, 2, \\ldots, K\\). Define\n\\[\n  p_k(x) = \\Pr(Y = k \\mid X = x),\n  \\quad k = 1, 2, \\ldots, K.\n\\]\nThese are the conditional class probabilities at \\(x\\); e.g. see little barplot at \\(x=5\\).\nThe Bayes optimal classifier at \\(x\\) is\n\\[\n  C(x) \\;=\\; j \\quad \\text{if} \\quad p_j(x) =\n      \\max \\{\\,p_1(x),\\, p_2(x),\\, \\dots,\\, p_K(x)\\}.\n\\]"
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "href": "lecture_slides/01_introduction/01_introduction.html#nearest-neighbor-averaging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nearest-Neighbor Averaging",
    "text": "Nearest-Neighbor Averaging\n\n\n\n\n\n\n\n\n\n\n\nNearest-neighbor averaging can be used as before.\nAlso breaks down as dimension grows. However, the impact on \\(\\hat{C}(x)\\)is less than on \\(\\hat{p}_k(x)\\), for \\(k = 1,\\ldots,K\\)."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "href": "lecture_slides/01_introduction/01_introduction.html#classification-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification: Some Details",
    "text": "Classification: Some Details\n\n\nTypically we measure the performance of \\(\\hat{C}(x)\\) using the misclassification error rate:\n\\[\n    \\mathrm{Err}_{\\mathrm{Te}}\n      = \\mathrm{Ave}_{i\\in \\mathrm{Te}}\n        \\bigl[I(y_i \\neq \\hat{C}(x_i))\\bigr].\n\\]\n\nThe Bayes classifier (using the true \\(p_k(x)\\)) has the smallest error in the population.\nSupport-vector machines build structured models for \\(\\hat{C}(x)\\).\nWe also build structured models for representing \\(p_k(x)\\). For example, logistic regression or generalized additive models."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "href": "lecture_slides/01_introduction/01_introduction.html#example-k-nearest-neighbors-in-two-dimensions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: K-Nearest Neighbors in Two Dimensions",
    "text": "Example: K-Nearest Neighbors in Two Dimensions\nBelow is an example data set in two dimensions \\((X_1, X_2)\\). Points shown in blue might represent one class, and points in orange the other. The dashed boundary suggests a decision boundary formed by a classifier."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-10",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 10",
    "text": "KNN: K = 10\nHere is the same data set classified by k-nearest neighbors with \\(k = 10\\). The black boundary line encloses the region of the feature space predicted as orange vs. blue, showing how the decision boundary has become smoother."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-k-1-vs.-k-100",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN: K = 1 vs. K = 100",
    "text": "KNN: K = 1 vs. K = 100\n\nComparisons of a very low value of \\(k\\) (left, \\(k=1\\)) versus a very high value (right, \\(k=100\\)).\n\n\\(k=1\\): Overly flexible boundary that can overfit.\n\\(k=100\\): Very smooth boundary that can underfit."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "href": "lecture_slides/01_introduction/01_introduction.html#knn-error-rates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "KNN Error Rates",
    "text": "KNN Error Rates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure illustrates how training errors (blue curve) and test errors (orange curve) change for a K-nearest neighbors (KNN) classifier as \\(\\frac{1}{K}\\) varies.\n\nFor small \\(K\\) (i.e., large \\(\\frac{1}{K}\\)), the model can become very flexible, often driving down training error but increasing overfitting and thus test error.\nFor large \\(K\\) (i.e., small \\(\\frac{1}{K}\\)), the model becomes smoother, which can help avoid overfitting but sometimes leads to underfitting.\n\nThe dashed horizontal line is the bayes error, used as reference for comparison."
  },
  {
    "objectID": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "href": "lecture_slides/01_introduction/01_introduction.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nStatistical Learning and Predictive Analytics\n\nGoal: Build models to predict outcomes and understand relationships between inputs (predictors) and responses.\nSupervised Learning: Focuses on predicting \\(Y\\) (response) using \\(X\\) (predictors) via models like regression and classification.\nUnsupervised Learning: Focuses on finding patterns in data without predefined responses (e.g., clustering).\n\nBias-Variance Trade-off\n\nKey Trade-off: Model flexibility affects bias and variance:\n\nHigh flexibility → Low bias but high variance (overfitting).\nLow flexibility → High bias but low variance (underfitting).\n\nGoal: Find the optimal flexibility that minimizes test error.\n\n\nTechniques and Applications\n\nParametric Models:\n\nSimpler and interpretable (e.g., linear regression).\nOften used as approximations.\n\nFlexible Models:\n\nHandle complex patterns (e.g., splines, SVMs, deep learning).\nRequire careful tuning to avoid overfitting.\n\n\nPractical Considerations\n\nAssessing Model Accuracy:\n\nUse test data to calculate MSE.\nBalance between training performance and generalizability.\n\n\nKey Challenges\n\nCurse of Dimensionality:\n\nHigh-dimensional data affects distance-based methods like KNN.\nLarger neighborhoods needed, losing “locality.”"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#overview",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nLinear Regression\nSimple Linear Regression\nMultiple Linear Regression\nConsiderations in the Regression Model\n\nQualitative Predictors\n\n\n\n\nExtensions of the Linear Model\n\nInteractions\nHierarchy\n\nNon-linear effects of predictors\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n\n\nLinear regression is a simple approach to supervised learning. It assumes that the dependence of \\(Y\\) on \\(X_1, X_2, \\ldots, X_p\\) is linear.\nTrue regression functions are never linear!\n\n\n\n\n\n\n\n\n\n\n\nAlthough it may seem overly simplistic, linear regression is extremely useful both conceptually and practically."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-for-the-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#linear-regression-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression for the Advertising Data",
    "text": "Linear Regression for the Advertising Data\n\n\nConsider the advertising data shown:\n\n\n\n\n\n\n\n\n\nQuestions we might ask:\n\n\n\n\nIs there a relationship between advertising budget and sales?\nHow strong is the relationship between advertising budget and sales?\nWhich media contribute to sales?\nHow accurately can we predict future sales?\nIs the relationship linear?\nIs there synergy among the advertising media?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#simple-linear-regression-using-a-single-predictor-x",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Linear Regression using a single predictor \\(X\\)",
    "text": "Simple Linear Regression using a single predictor \\(X\\)\n\n\n\nWe assume a model:\n\n\\[\n  Y = \\beta_0 + \\beta_1X + \\epsilon,\n\\]\nwhere \\(\\beta_0\\) and \\(\\beta_1\\) are two unknown constants that represent the intercept and slope, also known as coefficients or parameters, and \\(\\epsilon\\) is the error term.\n\n\nGiven some estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) for the model coefficients, we predict future sales using:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x,\n\\]\nwhere \\(\\hat{y}\\) indicates a prediction of \\(Y\\) on the basis of \\(X = x\\). The hat symbol denotes an estimated value."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-of-the-parameters-by-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation of the parameters by least squares",
    "text": "Estimation of the parameters by least squares\n\n\n\nLet \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) be the prediction for \\(Y\\) based on the \\(i\\)th value of \\(X\\). Then \\(e_i = y_i - \\hat{y}_i\\) represents the \\(i\\)th residual.\nWe define the residual sum of squares (RSS) as:\n\n\\[\n    RSS = e_1^2 + e_2^2 + \\cdots + e_n^2,\n\\]\nor equivalently as:\n\\[\n    RSS = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_2)^2 + \\cdots + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_n)^2.\n\\]\n\n\nThe least squares approach selects the estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to minimize the RSS. The minimizing values can be shown to be:\n\n\\[\n    \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad\n    \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\n\\]\nwhere \\(\\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^n y_i\\) and \\(\\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^n x_i\\) are the sample means."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#example-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#example-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Advertising Data",
    "text": "Example: Advertising Data\n\n\n\n\n\n\n\n\n\n\n\nThe least squares fit for the regression of sales onto TV is shown. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-accuracy-of-the-coefficient-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Accuracy of the Coefficient Estimates",
    "text": "Assessing the Accuracy of the Coefficient Estimates\n\n\n\nThe standard error of an estimator reflects how it varies under repeated sampling:\n\n\\[\n  SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}, \\quad SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right].\n\\]\nwhere \\(\\sigma^2 = Var(\\epsilon)\\)\n\n\nThese standard errors can be used to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. It has the form:\n\n\\[\n  \\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1).\n\\]\n\n\n\nThere is approximately a 95% chance that the interval:\n\n\\[\n  \\left[ \\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1 + 2 \\cdot SE(\\hat{\\beta}_1) \\right]\n\\]\nwill contain the true value of \\(\\beta_1\\) (under a scenario where we obtained repeated samples like the present sample).\n\nFor the advertising data, the 95% confidence interval for \\(\\beta_1\\) is:\n\n\\[\n  [0.042, 0.053].\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nStandard errors can be used to perform hypothesis tests on coefficients. The most common hypothesis test involves testing the null hypothesis:\n\n\\[\n  H_0: \\text{There is no relationship between } X \\text{ and } Y\n\\] versus the alternative hypothesis:\n\\[\n  H_A: \\text{There is some relationship between } X \\text{ and } Y.\n\\]\n\n\nMathematically, this corresponds to testing:\n\n\\[\n  H_0: \\beta_1 = 0\n\\] versus:\n\\[\n  H_A: \\beta_1 \\neq 0,\n\\]\nsince if \\(\\beta_1 = 0\\), then the model reduces to \\(Y = \\beta_0 + \\epsilon\\), and \\(X\\) is not associated with \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hypothesis-testing-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\nTo test the null hypothesis (\\(H_0\\)), compute a \\(t\\)-statistic as follows:\n\n\\[\n  t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}.\n\\]\n\nThe \\(t\\)-statistic follows a \\(t\\)-distribution with \\(n - 2\\) degrees of freedom under the null hypothesis (\\(\\beta_1 = 0\\)).\nUsing statistical software, we can compute the \\(p\\)-value to determine the likelihood of observing a \\(t\\)-statistic as extreme as the one calculated."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-the-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-the-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for the Advertising Data",
    "text": "Results for the Advertising Data\n\n\n\n\nVariable\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n7.0325\n0.4578\n15.36\n&lt; 0.0001\n\n\nTV\n0.0475\n0.0027\n17.67\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#assessing-the-overall-accuracy-of-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assessing the Overall Accuracy of the Model",
    "text": "Assessing the Overall Accuracy of the Model\n\n\n\nResidual Standard Error (RSE):\n\n\\[\n  RSE = \\sqrt{\\frac{1}{n-2} RSS} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n\\] where the Residual Sum of Square (RSS) is \\(\\sum_{i=1}^n (y_i - \\hat{y})^2\\).\n\n\n\\(R^2\\), the fraction of variance explained:\n\n\\[\n  R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}, \\quad TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2\n\\] where TSS is the Total Sums of Squares.\n\n\n\nIt can be shown that in this Simple Linear Regression setting that \\(R^2 = r^2\\), where \\(r\\) is the correlation between X and Y:\n\n\\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data-results",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#advertising-data-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advertising Data Results",
    "text": "Advertising Data Results\n\nKey metrics for model accuracy:\n\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n3.26\n\n\nR²\n0.612\n\n\nF-statistic\n312.1"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#multiple-linear-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\n\n\nHere our model is\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon,\n\\]\n\nWe interpret \\(\\beta_j\\) as the average effect on \\(Y\\) of a one-unit increase in \\(X_j\\), holding all other predictors fixed.\n\n\n\nIn the advertising example, the model becomes\n\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper} + \\epsilon.\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interpreting-regression-coefficients",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interpreting-regression-coefficients",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpreting Regression Coefficients",
    "text": "Interpreting Regression Coefficients\n\nThe ideal scenario is when the predictors are uncorrelated — a balanced design:\n\nEach coefficient can be estimated and tested separately.\nInterpretations such as “a unit change in \\(X_j\\) is associated with a \\(\\beta_j\\) change in \\(Y\\), while all the other variables stay fixed” are possible.\n\nCorrelations amongst predictors cause problems:\n\nThe variance of all coefficients tends to increase, sometimes dramatically.\nInterpretations become hazardous — when \\(X_j\\) changes, everything else changes.\n\nClaims of causality should be avoided for observational data."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#estimation-and-prediction-for-multiple-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimation and Prediction for Multiple Regression",
    "text": "Estimation and Prediction for Multiple Regression\n\n\n\nGiven estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\), we can make predictions using the formula:\n\n\\[\n  \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 + \\cdots + \\hat{\\beta}_px_p.\n\\]\n\n\nWe estimate \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) as the values that minimize the sum of squared residuals:\n\n\\[\n  \\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2\n             = \\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_{i1} - \\hat{\\beta}_2x_{i2} - \\cdots - \\hat{\\beta}_px_{ip} \\right)^2.\n\\]\n\nThis is done using standard statistical software. The values \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimize RSS are the multiple least squares regression coefficient estimates."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Advertising Data",
    "text": "Results for Advertising Data\n\n\n\nRegression Coefficients\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n2.939\n0.3119\n9.42\n&lt; 0.0001\n\n\nTV\n0.046\n0.0014\n32.81\n&lt; 0.0001\n\n\nradio\n0.189\n0.0086\n21.89\n&lt; 0.0001\n\n\nnewspaper\n-0.001\n0.0059\n-0.18\n0.8599\n\n\n\n\n\n\nCorrelations\n\n\n\n\nPredictor\nTV\nradio\nnewspaper\nsales\n\n\n\n\nTV\n1.0000\n0.0548\n0.0567\n0.7822\n\n\nradio\n\n1.0000\n0.3541\n0.5762\n\n\nnewspaper\n\n\n1.0000\n0.2283\n\n\nsales\n\n\n\n1.0000"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#some-important-questions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#some-important-questions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Important Questions",
    "text": "Some Important Questions\n\n\nIs at least one of the predictors \\(X_1, X_2, \\dots, X_p\\) useful in predicting the response?\nDo all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful?\nHow well does the model fit the data?\nGiven a set of predictor values, what response value should we predict, and how accurate is our prediction?"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#is-at-least-one-predictor-useful",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#is-at-least-one-predictor-useful",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Is at Least One Predictor Useful?",
    "text": "Is at Least One Predictor Useful?\nFor the first question, we can use the F-statistic:\n\\[\nF = \\frac{(TSS - RSS) / p}{RSS / (n - p - 1)} \\sim F_{p, n-p-1}\n\\]\n\n\n\nQuantity\nValue\n\n\n\n\nResidual Standard Error\n1.69\n\n\n\\(R^2\\)\n0.897\n\n\nF-statistic\n570\n\n\n\n\nThe F-statistic is huge and it’s p-value is less than \\(.0001\\). This says that there’s a strong association of the predictors on the outcome variable."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#deciding-on-the-important-variables",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#deciding-on-the-important-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deciding on the Important Variables",
    "text": "Deciding on the Important Variables\n\nThe most direct approach is called all subsets or best subsets regression:\n\nCompute the least squares fit for all possible subsets.\nChoose between them based on some criterion that balances training error with model size.\n\n\n\n\nHowever, we often can’t examine all possible models since there are (\\(2^p\\)) of them.\n\nFor example, when (p = 40), there are over a billion models!\n\nInstead, we need an automated approach that searches through a subset of them."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#forward-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#forward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Selection",
    "text": "Forward Selection\n\nBegin with the null model — a model that contains an intercept but no predictors.\nFit \\(p\\) Simple Linear Regressions and add to the null model the variable that results in the lowest RSS.\nAdd to that model the variable that results in the lowest RSS amongst all two-variable models.\nContinue until some stopping rule is satisfied:\n\nFor example, when all remaining variables have a p-value above some threshold."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#backward-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#backward-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Selection",
    "text": "Backward Selection\n\nStart with all variables in the model.\nRemove the variable with the largest p-value — that is, the variable that is the least statistically significant.\nThe new (\\(p - 1\\))-variable model is fit, and the variable with the largest p-value is removed.\nContinue until a stopping rule is reached:\n\nFor instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#model-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Model Selection",
    "text": "Model Selection\n\nWe will discuss other criterias for choosing an “optimal” member in the path of models produced by forward or backward stepwise selection, including:\n\nMallow’s \\(C_p\\)\nAkaike information criterion (AIC)\nBayesian information criterion (BIC)\nAdjusted \\(R^2\\)\nCross-validation (CV)"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nSome predictors are qualitative, taking discrete values (e.g., gender, ethnicity).\nCategorical predictors can be represented using factor variables.\nQualitative variables: Gender, Student (Student Status), Status (Marital Status), Ethnicity."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\nSuppose we investigate differences in credit card balance between males and females, ignoring the other variables. We create a new variable:\n\\[\nx_i =\n\\begin{cases}\n1 & \\text{if } i\\text{th person is female} \\\\\n0 & \\text{if } i\\text{th person is male}\n\\end{cases}\n\\]\nResulting model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i =\n\\begin{cases}\n\\beta_0 + \\beta_1 + \\epsilon_i & \\text{if } i\\text{th person is female} \\\\\n\\beta_0 + \\epsilon_i & \\text{if } i\\text{th person is male.}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-2",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#credit-card-data-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit Card Data",
    "text": "Credit Card Data\n\nResults for gender model:\n\n\n\n\nPredictor\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n509.80\n33.13\n15.389\n&lt; 0.0001\n\n\nGender \\(Female\\)\n19.73\n46.05\n0.429\n0.6690\n\n\n\n\nWe see the coefficient is 19.73, but it’s not significant. The p value is 0.66 which is not significant (&gt; 0.05). So, contrary to popular wisdom, females don’t generally have a higher credit card balance than males."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-with-more-than-two-levels",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors with More Than Two Levels",
    "text": "Qualitative Predictors with More Than Two Levels\n\nWith more than two levels, we create additional dummy variables.\nFor example, for the ethnicity variable, we create two dummy variables:\n\\[\nx_{i1} =\n\\begin{cases}\n      1 & \\text{if i-th person is Asian} \\\\\n      0 & \\text{if i-th person is not Asian}\n    \\end{cases}\n\\]\n\\[\nx_{i2} = \\begin{cases}\n      1 & \\text{if i-th person is Caucasian} \\\\\n      0 & \\text{if i-th person is not Caucasian}\n    \\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#qualitative-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\n\nBoth variables can be used in the regression equation to obtain the model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i =\n\\begin{cases}\n      \\beta_0 + \\beta_1 + \\epsilon_i & \\text{if i-th person is Asian} \\\\\n      \\beta_0 + \\beta_2 + \\epsilon_i & \\text{if i-th person is Caucasian}\\\\\n      \\beta_0 + \\epsilon_i & \\text{if i-th person is African American (baseline)}\n    \\end{cases}\n\\] \nNote: There will always be one fewer dummy variable than the number of levels. The level with no dummy variable — African American (AA) in this example — is known as the baseline."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-ethnicity",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#results-for-ethnicity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results for Ethnicity",
    "text": "Results for Ethnicity\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n531.00\n46.32\n11.464\n&lt; 0.0001\n\n\nethnicity \\(Asian\\)\n-18.69\n65.02\n-0.287\n0.7740\n\n\nethnicity \\(Caucasian\\)\n-12.50\n56.68\n-0.221\n0.8260\n\n\n\n\nThe coefficient -18.69 compares Asian to African American and that’s not significant. Likewise, the Caucasian to African-American is also not significant.\n\nNote: the choice of the baseline does not affect the fit of the model. The residual sum of sum of squares will be the same no matter which category we chose as the baseline. At its turn, the p-values will potentially change as we change the baseline category."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\nIn our previous analysis of the Advertising data, we assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media.\n\nFor example, the linear model\n\\[\n\\widehat{\\text{sales}} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times \\text{newspaper}\n\\]\nstates that the average effect on sales of a one-unit increase in TV is always \\(\\beta_1\\), regardless of the amount spent on radio."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions",
    "text": "Interactions\n\nBut suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases.\nIn this situation, given a fixed budget of $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or radio.\nIn marketing, this is known as a synergy effect, and in statistics, it is referred to as an interaction effect."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interaction-in-advertising-data",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interaction-in-advertising-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interaction in Advertising Data",
    "text": "Interaction in Advertising Data\n\n\nWhen levels of TV or radio are low, true sales are lower than predicted.\nSplitting advertising between TV and radio underestimates sales."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#modeling-interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#modeling-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Modeling Interactions",
    "text": "Modeling Interactions\nModel takes the form:\n\\[\n\\text{sales} = \\beta_0 + \\beta_1 \\times \\text{TV} + \\beta_2 \\times \\text{radio} + \\beta_3 \\times (\\text{radio} \\times \\text{TV}) + \\epsilon\n\\]\n\n\n\n\nTerm\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n6.7502\n0.248\n27.23\n&lt; 0.0001\n\n\nTV\n0.0191\n0.002\n12.70\n&lt; 0.0001\n\n\nradio\n0.0289\n0.009\n3.24\n0.0014\n\n\nTV × radio\n0.0011\n0.000\n20.73\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interpretation",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interpretation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe results in this table suggest that interactions are important.The p-value for the interaction term TV \\(\\times\\) radio is extremely low, indicating that there is strong evidence for ( H_A : \\(\\beta_3 \\neq 0\\)).\nThe ( \\(R^2\\) ) for the interaction model is 96.8%, compared to only 89.7% for the model that predicts sales using TV and radio without an interaction term.\nThis means that (\\(\\frac{96.8 - 89.7}{100 - 89.7}\\)) = 69% of the variability in sales that remains after fitting the additive model has been explained by the interaction term.\nThe coefficient estimates in the table suggest that an increase in TV advertising of $1,000 is associated with increased sales of (\\(\\hat{\\beta}_1 + \\hat{\\beta}_3 \\times \\text{radio}\\)) \\(\\times 1000 = 19 + 1.1 \\times \\text{radio} \\text{ units}.\\)\nAn increase in radio advertising of $1,000 will be associated with an increase in sales of (\\(\\hat{\\beta}_2 + \\hat{\\beta}_3 \\times \\text{TV}\\)) \\(\\times 1000 = 29 + 1.1 \\times \\text{TV} \\text{ units}.\\)"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchy",
    "text": "Hierarchy\n\nSometimes it is the case that an interaction term has a very small p-value, but the associated main effects (in this case, TV and radio) do not.\nThe hierarchy principle:\n\nIf we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant.\n\nThe rationale for this principle is that interactions are hard to interpret in a model without main effects.\nSpecifically, the interaction terms also contain main effects, if the model has no main effect terms."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#interactions-between-qualitative-and-quantitative-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interactions Between Qualitative and Quantitative Variables",
    "text": "Interactions Between Qualitative and Quantitative Variables\nConsider the Credit data set, and suppose that we wish to predict balance using income (quantitative) and student (qualitative).\nWithout an interaction term, the model takes the form:\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]\n\\[\n= \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_0 + \\beta_2 & \\text{if } i^\\text{th} \\text{ person is a student} \\\\\n\\beta_0 & \\text{if } i^\\text{th} \\text{ person is not a student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#with-interactions-it-takes-the-form",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#with-interactions-it-takes-the-form",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "With Interactions, It Takes the Form",
    "text": "With Interactions, It Takes the Form\n\\[\n\\text{balance}_i \\approx \\beta_0 + \\beta_1 \\times \\text{income}_i +\n\\begin{cases}\n\\beta_2 + \\beta_3 \\times \\text{income}_i & \\text{if student} \\\\\n0 & \\text{if not student}\n\\end{cases}\n\\]\n\\[\n=\n\\begin{cases}\n(\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) \\times \\text{income}_i & \\text{if student} \\\\\n\\beta_0 + \\beta_1 \\times \\text{income}_i & \\text{if not student}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#visualizing-interactions",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#visualizing-interactions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Visualizing Interactions",
    "text": "Visualizing Interactions\n\n\nLeft: no interaction between income and student.\nRight: with an interaction term between income and student."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-effects-of-predictors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear effects of predictors",
    "text": "Non-linear effects of predictors\n\nPolynomial regression on Auto data"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-regression-results",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#non-linear-regression-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-linear regression results",
    "text": "Non-linear regression results\nThe figure suggests that the following model\n\\[\nmpg = \\beta_0 + \\beta_1 \\times horsepower + \\beta_2 \\times horsepower^2 + \\epsilon\n\\]\nmay provide a better fit.\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nt-statistic\np-value\n\n\n\n\nIntercept\n56.9001\n1.8004\n31.6\n&lt; 0.0001\n\n\nhorsepower\n-0.4662\n0.0311\n-15.0\n&lt; 0.0001\n\n\n\\(\\text{horsepower}^2\\)\n0.0012\n0.0001\n10.1\n&lt; 0.0001"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#what-we-did-not-cover",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#what-we-did-not-cover",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What we did not cover",
    "text": "What we did not cover\n\n\nOutliers\nNon-constant variance of error terms\nHigh leverage points\nCollinearity"
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#generalizations-of-the-linear-model",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#generalizations-of-the-linear-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalizations of the Linear Model",
    "text": "Generalizations of the Linear Model\n\n\nIn much of the rest of the course we discuss methods that expand the scope of linear models and how they are fit:\n\nClassification problems: logistic regression, support vector machines.\nNon-linearity: kernel smoothing, splines, generalized additive models; nearest neighbor methods.\nInteractions: Tree-based methods, bagging, random forests, boosting (these also capture non-linearities).\nRegularized fitting: Ridge regression and lasso."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html#summary-1",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nLinear Regression:\n\nA foundational supervised learning method.\nAssumes a linear relationship between predictors (\\(X\\)) and the response (\\(Y\\)).\nUseful for both prediction and understanding relationships.\n\nSimple vs. Multiple Regression:\n\nSimple regression: one predictor.\nMultiple regression: multiple predictors.\n\nKey Metrics:\n\nResidual Standard Error (RSE), \\(R^2\\), and F-statistic.\nConfidence intervals and hypothesis testing for coefficients.\n\n\n\n\nQualitative Predictors:\n\nUse dummy variables for categorical predictors.\nInterpret results based on chosen baselines.\n\nInteractions:\n\nModels with interaction terms (e.g., \\(X_1 \\times X_2\\)) capture synergistic effects.\n\nNon-linear Effects:\n\nPolynomial regression accounts for curvature in data.\n\nChallenges:\n\nMulticollinearity, outliers, high leverage points.\nOverfitting vs. underfitting: balance flexibility and interpretability."
  },
  {
    "objectID": "lecture_slides/03_linear_regression/03_linear_regression.html",
    "href": "lecture_slides/03_linear_regression/03_linear_regression.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Linear Regression\nSimple Linear Regression\nMultiple Linear Regression\nConsiderations in the Regression Model\n\nQualitative Predictors\n\n\n\n\nExtensions of the Linear Model\n\nInteractions\nHierarchy\n\nNon-linear effects of predictors\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#overview",
    "href": "lecture_slides/04_classification/04_classification.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nIntroduction to Classification\nLinear versus Logistic Regression\nMaking Predictions\nMultinomial Logistic Regression\n\n\n\nDiscriminant Analysis\nLinear Discriminant Analysis when \\(p &gt; 1\\)\nTypes of errors\nOther Forms of Discriminant Analysis\nNaive Bayes\nGeneralized Linear Models\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#what-is-a-classification-problem",
    "href": "lecture_slides/04_classification/04_classification.html#what-is-a-classification-problem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is a classification problem?",
    "text": "What is a classification problem?\n\n\n\n\nClassification involves categorizing data into predefined classes or groups based on their features."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#classification",
    "href": "lecture_slides/04_classification/04_classification.html#classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification",
    "text": "Classification\n\nQualitative variables take values in an unordered set \\(C\\), such as:\n\n\\(\\text{eye color} \\in \\{\\text{brown}, \\text{blue}, \\text{green}\\}\\)\n\\(\\text{email} \\in \\{\\text{spam}, \\text{ham}\\}\\)\n\nGiven a feature vector \\(X\\) and a qualitative response \\(Y\\) taking values in the set \\(C\\), the classification task is to build a function \\(C(X)\\) that takes as input the feature vector \\(X\\) and predicts its value for \\(Y\\); i.e. \\(C(X) \\in C\\).\nOften, we are more interested in estimating the probabilities that \\(X\\) belongs to each category in \\(C\\).\n\nFor example, it is more valuable to have an estimate of the probability that an insurance claim is fraudulent, than a classification as fraudulent or not."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-credit-card-default",
    "href": "lecture_slides/04_classification/04_classification.html#example-credit-card-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Card Default",
    "text": "Example: Credit Card Default\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot of income vs. balance with markers indicating whether a person defaulted (e.g., “+” for defaulted, “o” for not defaulted).\n\n\n\n\n\n\n\n\n\n\n\nBoxplots comparing balance and income for default (“Yes”) vs. no default (“No”)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#can-we-use-linear-regression",
    "href": "lecture_slides/04_classification/04_classification.html#can-we-use-linear-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can we use Linear Regression?",
    "text": "Can we use Linear Regression?\nSuppose for the Default classification task that we code:\n\\[\nY =\n\\begin{cases}\n0 & \\text{if No} \\\\\n1 & \\text{if Yes.}\n\\end{cases}\n\\]\nCan we simply perform a linear regression of \\(Y\\) on \\(X\\) and classify as Yes if \\(\\hat{Y} &gt; 0.5\\)?\n\nIn this case of a binary outcome, linear regression does a good job as a classifier and is equivalent to linear discriminant analysis, which we discuss later.\nSince in the population \\(E(Y|X = x) = \\Pr(Y = 1|X = x)\\), we might think that regression is perfect for this task.\nHowever, linear regression might produce probabilities less than zero or greater than one. Logistic regression is more appropriate."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-probability-of-default",
    "href": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-probability-of-default",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression: Probability of Default",
    "text": "Linear versus Logistic Regression: Probability of Default\n\n\n\nThe orange marks indicate the response \\(Y\\), either 0 or 1.\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression does not estimate \\(\\Pr(Y = 1|X)\\) well.\n\n\n\nLogistic regression seems well-suited to the task."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-regression-continued",
    "href": "lecture_slides/04_classification/04_classification.html#linear-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Regression continued",
    "text": "Linear Regression continued\n\n\n\nNow suppose we have a response variable with three possible values. A patient presents at the emergency room, and we must classify them according to their symptoms.\n\\[\nY =\n\\begin{cases}\n1 & \\text{if stroke;} \\\\\n2 & \\text{if drug overdose;} \\\\\n3 & \\text{if epileptic seizure.}\n\\end{cases}\n\\]\nThis coding suggests an ordering, and in fact implies that the difference between stroke and drug overdose is the same as between drug overdose and epileptic seizure.\nLinear regression is not appropriate here. Multiclass Logistic Regression or Discriminant Analysis are more appropriate."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLet’s write \\(p(X) = \\Pr(Y = 1|X)\\) for short and consider using balance to predict default. Logistic regression uses the form:\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}.\n\\]\n\\((e \\approx 2.71828)\\) is a mathematical constant Euler’s number.\nIt is easy to see that no matter what values \\(\\beta_0\\), \\(\\beta_1\\), or \\(X\\) take, \\(p(X)\\) will have values between 0 and 1.\n\nA bit of rearrangement gives:\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X.\n\\]\nThis monotone transformation is called the log odds or logit transformation of \\(p(X)\\). (By log, we mean natural log: \\(\\ln\\).)"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 1: Express \\(1 - p(X)\\)\n\nSince \\(p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\\), we can write:\n\\[\n1 - p(X) = 1 - \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]\nSimplify:\n\\[\n1 - p(X) = \\frac{1 + e^{\\beta_0 + \\beta_1 X} - e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} = \\frac{1}{1 + e^{\\beta_0 + \\beta_1 X}}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-1",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 2: Compute the Odds\n\nThe odds are defined as:\n\\[\n\\frac{p(X)}{1 - p(X)}\n\\]\nSubstitute \\(p(X)\\) and \\(1 - p(X)\\):\n\\[\n\\frac{p(X)}{1 - p(X)} =\n\\frac{\\dfrac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}}}\n{\\dfrac{1}{1 + e^{\\beta_0 + \\beta_1 X}}}\n\\]\nSimplify:\n\\[\n\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1 X}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-2",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nStep 3: Take the Log of the Odds\n\nTaking the natural logarithm:\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\log\\!\\Bigl(e^{\\beta_0 + \\beta_1 X}\\Bigr)\n\\]\nSimplify using the log property \\(\\log(e^x) = x\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-3",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-transformation-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression Transformation",
    "text": "Logistic Regression Transformation\n\n\nConclusion\n\nThe final transformation shows that the log-odds (logit) of \\(p(X)\\) is a linear function of \\(X\\):\n\\[\n\\log\\!\\Bigl(\\frac{p(X)}{1 - p(X)}\\Bigr) = \\beta_0 + \\beta_1 X\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-1",
    "href": "lecture_slides/04_classification/04_classification.html#linear-versus-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear versus Logistic Regression",
    "text": "Linear versus Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nLogistic regression ensures that our estimate for \\(p(X)\\) lies between 0 and 1."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\n\nWe use maximum likelihood to estimate the parameters.\n\\[\n\\ell(\\beta_0, \\beta) = \\prod_{i:y_i=1} p(x_i) \\prod_{i:y_i=0} (1 - p(x_i)).\n\\]\n\nThe Maximum Likelihood Estimation (MLE) is a method used to estimate the parameters of a model by maximizing the likelihood function, which measures how likely the observed data is given the parameters.\n\nThe likelihood function is based on the probability distribution of the data. If you assume that the data points are independent, the likelihood function is the product of the probabilities of each observation.\n\nConsidering a data series of observed zeros and ones, and a model for the probabilities involving parameters (e.g., \\(\\beta_0\\) and \\(\\beta_1\\)), for any specific parameter values, we can compute the probability of observing the data.\nSince the observations are assumed to be independent, the joint probability of the observed sequence is the product of the probabilities for each observation. For each “1,” we use the model’s predicted probability, \\(p(x_i)\\), and for each “0,” we use \\(1 - p(x_i)\\).\nThe goal of MLE is to find the parameter values that maximize this joint probability, as they make the observed data most likely to have occurred."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSuppose you are flipping a coin, and you observe 5 heads out of 10 flips. The coin’s bias (the probability of heads) is \\(p\\), and you want to estimate \\(p\\).\nThe probability of observing a single outcome (heads or tails) follows the Bernoulli distribution:\n\\[\nP(\\text{Heads or Tails}) = p^x (1-p)^{1-x}, \\quad \\text{where } x = 1 \\text{ for heads, } x = 0 \\text{ for tails.}\n\\]\nFor 10 independent flips, the likelihood function is:\n\\[\nL(p) = P(\\text{data} \\mid p) = \\prod_{i=1}^{10} p^{x_i}(1-p)^{1-x_i}.\n\\]\nIf there are 5 heads (\\(x=1\\)) and 5 tails (\\(x=0\\)):\n\\[\nL(p) = p^5 (1-p)^5.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nSimplify with the Log-Likelihood\nSince multiplying probabilities can result in very small numbers, we take the logarithm of the likelihood (log-likelihood). The logarithm simplifies the product into a sum:\n\\[\n\\ell(p) = \\log L(p) = \\log \\left(p^5 (1-p)^5\\right) = 5\\log(p) + 5\\log(1-p).\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nMaximize the Log-Likelihood\nTo find the value of \\(p\\) that maximizes \\(\\ell(p)\\), take the derivative of the log-likelihood with respect to \\(p\\) and set it to zero:\n\\[\n\\frac{\\partial\\ell(p)}{\\partial p} = \\frac{5}{p} - \\frac{5}{1-p} = 0.\n\\]\nSimplify:\n\\[\n\\frac{5}{p} = \\frac{5}{1-p}.\n\\]\nSolve for \\(p\\):\n\\[\n1 - p = p \\quad \\Rightarrow \\quad 1 = 2p \\quad \\Rightarrow \\quad p = 0.5.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-coin-flipping-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping",
    "text": "Maximum Likelihood Estimation (MLE) Example: Coin Flipping\nTo confirm that \\(p = 0.5\\) is the maximum, you can check the second derivative of the log-likelihood (concavity) or use numerical methods.\nIn our example, \\(p = 0.5\\) makes sense intuitively because the data (5 heads out of 10 flips) suggests the coin is unbiased.\nThe maximum likelihood estimate of \\(p\\) is \\(0.5\\). The MLE method finds the parameter values that make the observed data most likely, given the assumed probability model."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\n\nAssumptions:\n\nData \\(x_1, x_2, \\dots, x_n\\) are drawn from a normal distribution with:\n\n\\[\n  f(x | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n\\]\n\nAssume \\(\\sigma\\) is known (say, \\(\\sigma = 1\\)) and we want to estimate \\(\\mu\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\nThe likelihood for \\(n\\) independent observations is:\n\\[\nL(\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x_i - \\mu)^2}{2}}\n\\]\nTaking the natural log:\n\\[\n\\ell(\\mu) = \\log L(\\mu) = \\sum_{i=1}^n \\left[ -\\frac{1}{2} \\log(2\\pi) - \\frac{(x_i - \\mu)^2}{2} \\right]\n\\]\nSimplify (since \\(-\\frac{1}{2} \\log(2\\pi)\\) is constant):\n\\[\n\\ell(\\mu) = -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "href": "lecture_slides/04_classification/04_classification.html#maximum-likelihood-estimation-mle-example-estimating-the-mean-of-a-normal-distribution-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) Example: Estimating the Mean of a Normal Distribution\n\n\nDifferentiate with respect to \\(\\mu\\):\n\\[\n\\frac{\\partial \\ell(\\mu)}{\\partial \\mu} = -\\sum_{i=1}^n (x_i - \\mu)\n\\] Set this to zero:\n\\[\n\\sum_{i=1}^n (x_i - \\mu) = 0\n\\]\nSolve for \\(\\mu\\):\n\\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nThe MLE for the mean \\(\\mu\\) is simply the sample mean:\n\\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#making-predictions-1",
    "href": "lecture_slides/04_classification/04_classification.html#making-predictions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Making Predictions",
    "text": "Making Predictions\nMost statistical packages can fit linear logistic regression models by maximum likelihood.\nLogistic Regression Coefficients\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.6513\n0.3612\n-29.5\n&lt; 0.0001\n\n\nbalance\n0.0055\n0.0002\n24.9\n&lt; 0.0001\n\n\n\n\nWhat is our estimated probability of default for someone with a credit card balance of $1000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 1000}}{1 + e^{-10.6513 + 0.0055 \\times 1000}} = 0.006\n\\]\nWith a a credit card balance of $2000?\n\\[\n\\hat{p}(X) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}}{1 + e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 X}} = \\frac{e^{-10.6513 + 0.0055 \\times 2000}}{1 + e^{-10.6513 + 0.0055 \\times 2000}} = 0.586\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-student-predictor",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-student-predictor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Student Predictor",
    "text": "Logistic Regression with Student Predictor\nLet’s do it again, using student as the predictor.\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-3.5041\n0.0707\n-49.55\n&lt; 0.0001\n\n\nstudent \\(Yes\\)\n0.4049\n0.1150\n3.52\n0.0004\n\n\n\n\nPredicted Probabilities\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{Yes}) = \\frac{e^{-3.5041 + 0.4049 \\times 1}}{1 + e^{-3.5041 + 0.4049 \\times 1}} = 0.0431,\n\\]\n\\[\n\\hat{\\Pr}(\\text{default} = \\text{Yes} \\mid \\text{student} = \\text{No}) = \\frac{e^{-3.5041 + 0.4049 \\times 0}}{1 + e^{-3.5041 + 0.4049 \\times 0}} = 0.0292.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-several-variables",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-with-several-variables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression with Several Variables",
    "text": "Logistic Regression with Several Variables\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\n\\[\np(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}\n\\]\n\nLogistic Regression Coefficients\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nZ-statistic\nP-value\n\n\n\n\nIntercept\n-10.8690\n0.4923\n-22.08\n&lt; 0.0001\n\n\nbalance\n0.0057\n0.0002\n24.74\n&lt; 0.0001\n\n\nincome\n0.0030\n0.0082\n0.37\n0.7115\n\n\nstudent Yes\n-0.6468\n0.2362\n-2.74\n0.0062\n\n\n\nWhy is the coefficient for student negative, while it was positive before?"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#confounding",
    "href": "lecture_slides/04_classification/04_classification.html#confounding",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\n\n\nRelationship between Y and X controlled for W\n\n\n\nSource: Causal Inference Animated Plots"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#confounding-1",
    "href": "lecture_slides/04_classification/04_classification.html#confounding-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Confounding",
    "text": "Confounding\n\n\nStudents tend to have higher balances than non-students, so their marginal default rate is higher than for non-students.\nBut for each level of balance, students default less than non-students.\nMultiple logistic regression can tease this out."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#multinomial-logistic-regression-1",
    "href": "lecture_slides/04_classification/04_classification.html#multinomial-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\nLogistic regression is frequently used when the response is binary, or \\(K = 2\\) classes. We need a modification when there are \\(K &gt; 2\\) classes. E.g. stroke, drug overdose, and epileptic seizure for the emergency room example.\nThe simplest representation uses different linear functions for each class, combined with the softmax function to form probabilities:\n\\[\n\\Pr(Y = k | X = x) = \\text{Softmax}(z_k) = \\frac{e^{\\beta_{k0} + \\beta_{k1}x_1 + \\cdots + \\beta_{kp}x_p}}{\\sum_{l=1}^{K} e^{\\beta_{l0} + \\beta_{l1}x_1 + \\cdots + \\beta_{lp}x_p}}.\n\\]\n\nWe really only need \\(K - 1\\) functions (see the book for details).\nWe fit by maximizing the multinomial log-likelihood (cross-entropy) — a generalization of the binomial.\nAn example will given later in the course, when we fit the 10-class model to the MNIST digit dataset."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#what-is-the-softmax-function",
    "href": "lecture_slides/04_classification/04_classification.html#what-is-the-softmax-function",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is the Softmax Function?",
    "text": "What is the Softmax Function?\n\n\nThe softmax function is used in multinomial logistic regression to convert raw scores (logits) into probabilities for multiple classes.\n\n\nLogits are the raw, untransformed output of the linear component in logistic regression. For a given class \\(k\\), the logit is defined as:\n\\[\nz_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\n\\]\nWhere:\n\n\\(z_k\\): The logit for class \\(k\\).\n\\(\\beta_{k0}\\): Intercept term.\n\\(\\beta_{kj}\\): Coefficients for predictor \\(x_j\\).\n\n\n\nSoftmax Definition:\nFor \\(K\\) classes and input \\(x\\), the softmax function is defined as:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{\\sum_{l=1}^K e^{z_l}}\n\\]\nWhere:\n\n\\(z_k = \\beta_{k0} + \\beta_{k1}x_1 + \\beta_{k2}x_2 + \\cdots + \\beta_{kp}x_p\\): The linear score (logit) for class \\(k\\).\n\\(\\beta_{k0}, \\beta_{k1}, \\dots, \\beta_{kp}\\): Coefficients for class \\(k\\).\n\\(e^{z_k}\\): Exponentiated score for class \\(k\\), ensuring all values are positive.\n\n\n\n\nKey Features of the Softmax Function\n\nProbability Distribution: Outputs probabilities that sum to 1 across all \\(K\\) classes. \\(\\text{Pr}(Y = k \\mid X = x) = \\text{Softmax}(z_k)\\).\nNormalization: Normalizes logits by dividing each exponentiated logit by the sum of all exponentiated logits.\nHandles Multiclass Classification: Extends binary logistic regression to \\(K &gt; 2\\) classes."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-of-softmax-in-action",
    "href": "lecture_slides/04_classification/04_classification.html#example-of-softmax-in-action",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example of Softmax in Action",
    "text": "Example of Softmax in Action\n\n\n\n\nImagine classifying three emergency room conditions: Stroke, Drug Overdose, and Epileptic Seizure.\nSuppose the logits are: \\(z_{\\text{stroke}} = 2.5, \\quad z_{\\text{drug overdose}} = 1.0, \\quad z_{\\text{epileptic seizure}} = 0.5\\)\nThe probabilities are:\n\\[\n\\text{Softmax}(z_k) = \\frac{e^{z_k}}{e^{2.5} + e^{1.0} + e^{0.5}}\n\\]\n\nStep 1: Exponentiate the Logits\n\\(e^{z_{\\text{stroke}}} = e^{2.5} \\approx 12.182\\)\n\\(e^{z_{\\text{drug overdose}}} = e^{1.0} \\approx 2.718\\)\n\\(e^{z_{\\text{epileptic seizure}}} = e^{0.5} \\approx 1.649\\)\n\n\nStep 2: Compute the Denominator\n\\(\\sum_{l=1}^K e^{z_l} = e^{2.5} + e^{1.0} + e^{0.5}\\)\n\\(\\sum_{l=1}^K e^{z_l} \\approx 12.182 + 2.718 + 1.649 = 16.549\\)\n\n\n\nStep 3: Calculate the Probabilities\n\\(\\text{Pr}(\\text{stroke}) = \\frac{e^{z_{\\text{stroke}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{12.182}{16.549} \\approx 0.7366\\)\n\\(\\text{Pr}(\\text{drug overdose}) = \\frac{e^{z_{\\text{drug overdose}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{2.718}{16.549} \\approx 0.1642\\)\n\\(\\text{Pr}(\\text{epileptic seizure}) = \\frac{e^{z_{\\text{epileptic seizure}}}}{\\sum_{l=1}^K e^{z_l}} = \\frac{1.649}{16.549} \\approx 0.0996\\)\nThe output probabilities represent the likelihood of each condition, ensuring:\n\\[\n\\sum_{k=1}^3 \\text{Pr}(Y = k) = 1\n\\]\nWe have:\n\\[\n   0.7366 + 0.1642 + 0.0996 \\approx 1.000\n\\]\n\n\nConclusion\n\nThe softmax function translates raw scores into probabilities, making it essential for multiclass classification.\nIt ensures a probabilistic interpretation while maintaining normalization across all classes."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#discriminant-analysis-1",
    "href": "lecture_slides/04_classification/04_classification.html#discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nHere the approach is to model the distribution of \\(X\\) in each of the classes separately, and then use Bayes theorem to flip things around and obtain \\(\\Pr(Y \\mid X)\\).\nWhen we use normal (Gaussian) distributions for each class, this leads to linear or quadratic discriminant analysis.\nHowever, this approach is quite general, and other distributions can be used as well. We will focus on normal distributions as input for \\(f_k(x)\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#bayes-theorem-for-classification",
    "href": "lecture_slides/04_classification/04_classification.html#bayes-theorem-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes Theorem for Classification",
    "text": "Bayes Theorem for Classification\nThomas Bayes was a famous mathematician whose name represents a big subfield of statistical and probabilistic modeling. Here we focus on a simple result, known as Bayes theorem:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\Pr(X = x \\mid Y = k) \\cdot \\Pr(Y = k)}{\\Pr(X = x)}\n\\]\nOne writes this slightly differently for discriminant analysis:\n\\[\n\\Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{\\ell=1}^K \\pi_\\ell f_\\ell(x)}, \\quad \\text{where}\n\\]\n\n\\(f_k(x) = \\Pr(X = x \\mid Y = k)\\) is the density for \\(X\\) in class \\(k\\). Here we will use normal densities for these, separately in each class.\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal or prior probability for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#bayes-theorem-explanation",
    "href": "lecture_slides/04_classification/04_classification.html#bayes-theorem-explanation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayes’ Theorem: Explanation",
    "text": "Bayes’ Theorem: Explanation\nIt describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\n\n\\(P(A|B)\\): Posterior probability - Probability of event \\(A\\) occurring given that \\(B\\) is true — updated probability after the evidence is considered.\n\\(P(A)\\): Prior probability - Initial probability of event \\(A\\) — the probability before the evidence is considered.\n\\(P(B|A)\\): Likelihood - Probability of observing event \\(B\\) given that \\(A\\) is true.\n\\(P(B)\\): Marginal probability - Total probability of the evidence, event \\(B\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#understanding-conditional-probability",
    "href": "lecture_slides/04_classification/04_classification.html#understanding-conditional-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Understanding Conditional Probability",
    "text": "Understanding Conditional Probability\nConditional probability is the probability of an event occurring given that another event has already occurred.\nDefinition:\n\\[\n  P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nis the probability of event \\(A\\) occurring given that \\(B\\) is true.\n\nInterpretation: How likely is \\(A\\) if we know that \\(B\\) happens?"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#what-is-joint-probability",
    "href": "lecture_slides/04_classification/04_classification.html#what-is-joint-probability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is Joint Probability?",
    "text": "What is Joint Probability?\nJoint probability refers to the probability of two events occurring together.\nDefinition: \\(P(A \\cap B)\\) is the probability that both \\(A\\) and \\(B\\) occur.\n\nConnection to Conditional Probability:\n\\[\n  P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n  P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\nThis formula is crucial for understanding Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#symmetry-in-joint-events",
    "href": "lecture_slides/04_classification/04_classification.html#symmetry-in-joint-events",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Symmetry in Joint Events",
    "text": "Symmetry in Joint Events\nJoint probability is symmetric, meaning:\n\\[\nP(A \\cap B) = P(B \\cap A)\n\\]\nThus, we can also express it as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\nThis symmetry is the key to deriving Bayes’ Theorem."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#deriving-bayes-theorem",
    "href": "lecture_slides/04_classification/04_classification.html#deriving-bayes-theorem",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deriving Bayes’ Theorem",
    "text": "Deriving Bayes’ Theorem\n\n\nGiven that the definition of Conditional Probability is:\n\\[\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\n\n\nUsing the Definition of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(A|B) \\cdot P(B)\n\\]\n\\[\n   P(B \\cap A) = P(B|A) \\cdot P(A)\n\\]\n\n\n\nSymmetry of Joint Probability:\n\n\\[\n   P(A \\cap B) = P(B \\cap A)\n\\]\n\n\nThus, we can express the joint probability as:\n\\[\nP(A \\cap B) = P(B|A) \\cdot P(A)\n\\]\n\nThe Bayes’ Theorem!\n\nSubstitute this back into the conditional probability definition:\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#why-bayes-theorem-matters",
    "href": "lecture_slides/04_classification/04_classification.html#why-bayes-theorem-matters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Bayes’ Theorem Matters?",
    "text": "Why Bayes’ Theorem Matters?\n\n\nBayes’ Theorem is a foundational principle in probability theory and statistics, enabling:\n\nIncorporation of Prior Knowledge:\nIt allows for the integration of prior knowledge or beliefs when making statistical inferences.\nBeliefs Update:\nIt provides a systematic way to update the probability estimates as new evidence or data becomes available.\nProbabilistic Thinking:\nEncourages a probabilistic approach to decision-making, quantifying uncertainty, and reasoning under uncertainty.\nVersatility in Applications:\nFrom medical diagnosis to spam filtering, Bayes’ Theorem is pivotal in areas requiring probabilistic assessment.\n\nBayes’ Theorem is a paradigm that shapes the way we interpret and interact with data, offering a powerful tool for learning from information and making decisions in an uncertain world."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#classify-to-the-highest-density",
    "href": "lecture_slides/04_classification/04_classification.html#classify-to-the-highest-density",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classify to the Highest Density",
    "text": "Classify to the Highest Density\n\n\nLeft-hand plot: single variable X and \\(\\pi_k f_k(x)\\) in the vertical axis for both classes \\(k\\) equals 1 and \\(k\\) equals 2. In this case the the pies are the same for both, so anything to the left of zero we classify as as green and anything to the right we classify as as purple.\nRight-hand plot: here we have different priors. The probability of \\(k = 2\\) is 0.7 and and of of \\(k= 1\\) is 0.3. The decision boundary moved slightly to the left. On the right, we favor the pink class."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#why-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#why-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Discriminant Analysis?",
    "text": "Why Discriminant Analysis?\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf \\(n\\) is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes, because it also provides low-dimensional views of the data."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p = 1\\)",
    "text": "Linear Discriminant Analysis when \\(p = 1\\)\nThe Gaussian density has the form:\n\\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_k}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma_k} \\right)^2}\n\\]\nHere \\(\\mu_k\\) is the mean, and \\(\\sigma_k^2\\) the variance (in class \\(k\\)). We will assume that all the \\(\\sigma_k = \\sigma\\) are the same.\n\nPlugging this into Bayes formula, we get a rather complex expression for \\(p_k(x) = \\Pr(Y = k \\mid X = x)\\):\n\\[\np_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_k}{\\sigma} \\right)^2}}{\\sum_{\\ell=1}^K \\pi_\\ell \\frac{1}{\\sqrt{2\\pi\\sigma}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_\\ell}{\\sigma} \\right)^2}}\n\\]\nHappily, there are simplifications and cancellations."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#discriminant-functions",
    "href": "lecture_slides/04_classification/04_classification.html#discriminant-functions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Discriminant Functions",
    "text": "Discriminant Functions\nTo classify one observation at the value \\(X = x\\) into a class, we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning \\(x\\) to the class with the largest discriminant score:\n\\[\n\\delta_k(x) = x \\cdot \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\]\nNote that \\(\\delta_k(x)\\) is a linear function of \\(x\\).\n\nIf there are \\(K = 2\\) classes and \\(\\pi_1 = \\pi_2 = 0.5\\), then one can see that the decision boundary is at:\n\\[\nx = \\frac{\\mu_1 + \\mu_2}{2}.\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#example-estimating-parameters-for-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Estimating Parameters for Discriminant Analysis",
    "text": "Example: Estimating Parameters for Discriminant Analysis\n\n\nLeft-Panel: Synthetic population data with \\(\\mu_1 = -1.5\\), \\(\\mu_2 = 1.5\\), \\(\\pi_1 = \\pi_2 = 0.5\\), and \\(\\sigma^2 = 1\\).\nTypically, we don’t know these parameters; we just have the training data. In that case, we simply estimate the parameters and plug them into the rule.\nRight-Panel: histograms of the sample. We see that the estimation provided a decision boundary (black solid line) pretty close to the correct one, the one of the population."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#estimating-the-parameters",
    "href": "lecture_slides/04_classification/04_classification.html#estimating-the-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating the Parameters",
    "text": "Estimating the Parameters\n\n\nThe prior is the number in each class divided by the total number:\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n}\n\\]\nThe means in each class is the sample mean:\n\\[\n\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i = k} x_i\n\\]\nWe assume that the variance is the same in each of the classes and so we assume a pooled variance estimate:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\n\\]\n\\[\n= \\sum_{k=1}^K \\frac{n_k - 1}{n - K} \\cdot \\hat{\\sigma}_k^2\n\\]\nwhere \\(\\hat{\\sigma}_k^2 = \\frac{1}{n_k - 1} \\sum_{i: y_i = k} (x_i - \\hat{\\mu}_k)^2\\) is the usual formula for the estimated variance in the \\(k\\)-th class."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-2",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nGaussian density in two Dimensions, two variables \\(x_1\\) and \\(x_2\\). On the Left-panel, we have a bell function and this is the case when the two variables are uncorrelated. On the Right-panel, there is correlation between the two predictors and it is like a stretched bell.\nDensity:\n\\[f(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} e^{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\\] where \\(\\Sigma\\) is the covariance matrix."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#covariance-matrix",
    "href": "lecture_slides/04_classification/04_classification.html#covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\n\n\nThe covariance matrix is a square matrix that summarizes the covariance (a measure of how much two random variables vary together) between multiple variables in a dataset.\nDefinition:\nFor a random vector \\(X = [X_1, X_2, \\dots, X_p]^\\top\\) with \\(p\\) variables, the covariance matrix \\(\\Sigma\\) is defined as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots & \\text{Cov}(X_1, X_p) \\\\\n\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots & \\text{Cov}(X_2, X_p) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\text{Cov}(X_p, X_1) & \\text{Cov}(X_p, X_2) & \\cdots & \\text{Var}(X_p)\n\\end{bmatrix}\n\\]\n\nKey Properties:\n\n\\(\\text{Var}(X_i)\\): Variance of variable \\(X_i\\).\n\\(\\text{Cov}(X_i, X_j)\\): Covariance between variables \\(X_i\\) and \\(X_j\\).\n\\(\\Sigma\\) is symmetric: \\(\\text{Cov}(X_i, X_j) = \\text{Cov}(X_j, X_i)\\).\nDiagonal elements represent variances, and off-diagonal elements represent covariances."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-3",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nDiscriminant function: after simplifying the density function we can find\n\\[\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\]\nNote that it is a linear function where the first component, \\(x^T \\Sigma^{-1} \\mu_k\\), has the \\(x\\) variable multiplied by a coefficient vector and, the second component, \\(\\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k\\), is a constant."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-4",
    "href": "lecture_slides/04_classification/04_classification.html#linear-discriminant-analysis-when-p-1-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Discriminant Analysis when \\(p > 1\\)",
    "text": "Linear Discriminant Analysis when \\(p &gt; 1\\)\n\n\n\n\n\n\n\n\n\n\n\nThe Discriminant function can be written as\n\\[\\delta_k(x) = c_{k0} + c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\]\na linear function. That is a function for class \\(k\\), where \\(c_{k0}\\) represents the constant we find in the second component of the Discriminant function and \\(c_{k1}x_1 + c_{k2}x_2 + \\cdots + c_{kp}x_p\\) come from the first component of the Discriminant function. We compute \\(\\delta_k(x)\\) for each of the classes and then you classify to the class for which it is largest."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#illustration-p-2-and-k-3-classes",
    "href": "lecture_slides/04_classification/04_classification.html#illustration-p-2-and-k-3-classes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes",
    "text": "Illustration: \\(p = 2\\) and \\(K = 3\\) classes\n\n\nLeft-panel: The circle presents the countor of the density of a particular level of probability for the blue, green, and the orange class. Here \\(\\pi_1 = \\pi_2 = \\pi_3 = \\frac{1}{3}\\). The dashed lines are known as the Bayes decision boundaries. They are the “True” decision boundaries, were they known, they would yield the fewest misclassification errors, among all possible classifiers.\nRight-panel: We compute the mean for \\(x_1\\) and \\(x_2\\) for the each blue, green, and orange class. After plugging them into the formula, instead of getting the the dotted lines we get the solid black lines."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-fishers-iris-data-1",
    "href": "lecture_slides/04_classification/04_classification.html#example-fishers-iris-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Fisher’s Iris Data",
    "text": "Example: Fisher’s Iris Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 variables\n3 species\n50 samples/class\n\n🟦 Setosa\n🟧 Versicolor\n🟩 Virginica\n\nLDA classifies all but 3 of the 150 training samples correctly."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-fishers-discriminant-plot",
    "href": "lecture_slides/04_classification/04_classification.html#example-fishers-discriminant-plot",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Fisher’s Discriminant Plot",
    "text": "Example: Fisher’s Discriminant Plot\n\n\nDiscriminant variables 1 and 2 are linear combinations of the original variables.\nLDA classifies points based on their proximity to centroids in discriminant space.\nThe centroids lie in a subspace of the multi-dimensional space (e.g., a plane within 4D space).\nFor \\(K\\) classes:\n\nLDA can be visualized in \\(K - 1\\)-dimensional space.\nFor \\(K &gt; 3\\), the “best” 2D plane can be chosen for visualization."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#from-delta_kx-to-probabilities",
    "href": "lecture_slides/04_classification/04_classification.html#from-delta_kx-to-probabilities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "From \\(\\delta_k(x)\\) to Probabilities",
    "text": "From \\(\\delta_k(x)\\) to Probabilities\n\n\n\nOnce we have estimates of the Discriminat Functions, \\(\\hat{\\delta}_k(x)\\), we can turn these into estimates for class probabilities:\n\n\\[\n\\hat{\\Pr}(Y = k | X = x) = \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{l=1}^K e^{\\hat{\\delta}_l(x)}}.\n\\]\n\nSo classifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{\\Pr}(Y = k | X = x)\\) is largest.\nWhen \\(K = 2\\), we classify to class 2 if \\(\\hat{\\Pr}(Y = 2 | X = x) \\geq 0.5\\), else to class 1."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#lda-on-credit-data",
    "href": "lecture_slides/04_classification/04_classification.html#lda-on-credit-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "LDA on Credit Data",
    "text": "LDA on Credit Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrue Default Status\n\n\n\nPredicted Default Status\nNo\nYes\nTotal\n\n\n\n\nNo\n9644\n252\n9896\n\n\nYes\n23\n81\n104\n\n\nTotal\n9667\n333\n10000\n\n\n\n\n\n\n\n\n\\(\\frac{23 + 252}{10000}\\) errors — a 2.75% misclassification rate!\n\n\nSome caveats:\n\nThis is training error, and we may be overfitting.\nIf we classified to the prior, the proportion of cases in the classes (e.g. always assuming the class No default). We would make \\(\\frac{333}{10000}\\) errors, or only 3.33%. This is what we call the null rate.\nWe can break the errors into different kinds: of the true No’s, we make \\(\\frac{23}{9667} = 0.2\\%\\) errors; of the true Yes’s, we make \\(\\frac{252}{333} = 75.7\\%\\) errors!"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#types-of-errors-1",
    "href": "lecture_slides/04_classification/04_classification.html#types-of-errors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Types of errors",
    "text": "Types of errors\n\n\nFalse positive rate: The fraction of negative examples that are classified as positive — 0.2% in example.\nFalse negative rate: The fraction of positive examples that are classified as negative — 75.7% in example.\nWe produced this table by classifying to class Yes if:\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq 0.5\n\\]\nWe can change the two error rates by changing the threshold from \\(0.5\\) to some other value in \\([0, 1]\\):\n\\[\n\\hat{P}(\\text{Default} = \\text{Yes} \\mid \\text{Balance}, \\text{Student}) \\geq \\text{threshold},\n\\]\nand vary \\(\\text{threshold}\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#varying-the-threshold",
    "href": "lecture_slides/04_classification/04_classification.html#varying-the-threshold",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Varying the threshold",
    "text": "Varying the threshold\n\nIn order to reduce the false negative rate, we may want to reduce the threshold to 0.1 or less."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#roc-curve",
    "href": "lecture_slides/04_classification/04_classification.html#roc-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nThe ROC plot displays both simultaneously.\nSometimes we use the AUC or area under the curve to summarize the overall performance. Higher AUC is good."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#other-forms-of-discriminant-analysis-1",
    "href": "lecture_slides/04_classification/04_classification.html#other-forms-of-discriminant-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Forms of Discriminant Analysis",
    "text": "Other Forms of Discriminant Analysis\n\nWhen \\(f_k(x)\\) are Gaussian densities, with the same covariance matrix \\(\\Sigma\\) in each class, this leads to linear discriminant analysis.\n\\[\n\\Pr(Y = k|X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}\n\\]\nBy altering the forms for \\(f_k(x)\\), we get different classifiers:\n\nWith Gaussians but different \\(\\Sigma_k\\) in each class, we get quadratic discriminant analysis.\nWith \\(f_k(x) = \\prod_{j=1}^{p} f_{jk}(x_j)\\) (conditional independence model) in each class, we get naive Bayes. For Gaussians, this means \\(\\Sigma_k\\) are diagonal.\nMany other forms, by proposing specific density models for \\(f_k(x)\\), including nonparametric approaches."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#quadratic-discriminant-analysis",
    "href": "lecture_slides/04_classification/04_classification.html#quadratic-discriminant-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Quadratic Discriminant Analysis",
    "text": "Quadratic Discriminant Analysis\n\n\n\n\n\n\n\n\n\n\n\\[\n\\delta_k(x) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma_k^{-1}(x - \\mu_k) + \\log \\pi_k - \\frac{1}{2} \\log |\\Sigma_k|\n\\]\nIn the Left-plot we see a case when the true boundary should be linear. In the Right-plot, covariances were different in the true data. It is possible to see that the bayes decision boundary is curved and the quadratic discriminant analysis is also curved whereas the linear discriminant analysis gives a different boundary.\nWhether each class has the same or different covariance matrices significantly impacts how boundaries between the classes are defined. The covariance matrix describes the spread or variability of data points within each class and how the features in that class relate to each other.\n\nKey Insight: If \\(\\Sigma_k\\) are different for each class, the quadratic terms matter significantly.\nQDA allows for non-linear decision boundaries due to unique covariance matrices for each class.\nExample: Suppose we are classifying plants based on two features (e.g., height and leaf width). If one type of plant has a tall and narrow spread of data, while another type has a short and wide spread, QDA can handle these differences and draw curved boundaries to separate the groups."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#assess-the-covariance-matrices",
    "href": "lecture_slides/04_classification/04_classification.html#assess-the-covariance-matrices",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Assess the Covariance Matrices",
    "text": "Assess the Covariance Matrices\n\nLDA assumes the covariance matrices of all classes are the same, while QDA allows each class to have its own. To determine which assumption is better:\n\nHypothesys test: we can perform a Test for Equality of Covariance Matrices (e.g. Box’s M Test). If the covariance matrices are similar (test is not significant): LDA is appropriate. If the covariance matrices differ (test is significant): QDA may be better.\nVisual Inspection: Plot the data in two dimensions (e.g., using scatterplots). Check if the spread, shape, or orientation of data points differs significantly between classes. If they are similar, LDA might work well. If they are visibly different, QDA is likely better.\nCompare Model Performance: run both models and choose the model that performs better on unseen data (test set).\nConsider the Number of Features and Data Size: LDA performs well with smaller datasets because it estimates a single covariance matrix across all classes (fewer parameters). QDA requires a larger dataset because it estimates a separate covariance matrix for each class (more parameters).\nDomain Knowledge: Use your understanding of the data to decide."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#logistic-regression-versus-lda",
    "href": "lecture_slides/04_classification/04_classification.html#logistic-regression-versus-lda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Logistic Regression versus LDA",
    "text": "Logistic Regression versus LDA\n\nFor a two-class problem, one can show that for LDA:\n\\[\n\\log \\left( \\frac{p_1(x)}{1 - p_1(x)} \\right) = \\log \\left( \\frac{p_1(x)}{p_2(x)} \\right) = c_0 + c_1 x_1 + \\dots + c_p x_p\n\\]\nif we take the log odds, \\(\\log \\left( \\frac{p_1(x)}{1 - p_1(x)}\\right)\\), which is the log of the probability for class 1 versus the probability for class two, we endup with a linear function of \\(x\\), \\(c_0 + c_1 x_1 + \\dots + c_p x_p\\). So it has the same form as logistic regression.\nThe difference lies in how the parameters are estimated.\n\nLogistic regression uses the conditional likelihood based on \\(\\text{Pr}(Y|X)\\). In Machine Learning, it is known as discriminative learning.\nLDA uses the full likelihood based on the joint distributions of \\(x's\\) and \\(y's\\), \\(\\text{Pr}(X, Y)\\), whereas logistic regression was only using the distribution of \\(y's\\). It is known as generative learning.\nDespite these differences, in practice, the results are often very similar.\n\nLogistic regression can also fit quadratic boundaries like QDA by explicitly including quadratic terms in the model."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naive-bayes-1",
    "href": "lecture_slides/04_classification/04_classification.html#naive-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\n\n\nAssumes features are independent in each class.\nUseful when \\(p\\) is large, and so multivariate methods like QDA and even LDA break down.\n\nGaussian Naive Bayes assumes each \\(\\Sigma_k\\) is diagonal:\n\\[\n\\begin{aligned}\n\\delta_k(x) &\\propto \\log \\left[ \\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\right] \\\\\n            &= -\\frac{1}{2} \\sum_{j=1}^p \\left[ \\frac{(x_j - \\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\sigma_{kj}^2 \\right] + \\log \\pi_k\n\\end{aligned}\n\\]\n\nCan be used for mixed feature vectors (qualitative and quantitative). If \\(X_j\\) is qualitative, replace \\(f_{kj}(x_j)\\) with the probability mass function (histogram) over discrete categories.\nKey Point: Despite strong assumptions, naive Bayes often produces good classification results.\n\nExplanation:\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_{kj}(x_j)\\): Density function for feature \\(j\\) in class \\(k\\).\n\\(\\mu_{kj}\\): Mean of feature \\(j\\) in class \\(k\\).\n\\(\\sigma_{kj}^2\\): Variance of feature \\(j\\) in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#diagonal-covariance-matrix",
    "href": "lecture_slides/04_classification/04_classification.html#diagonal-covariance-matrix",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Diagonal Covariance Matrix",
    "text": "Diagonal Covariance Matrix\n\n\nA diagonal covariance matrix is a special case of the covariance matrix where all off-diagonal elements are zero. This implies that the variables are uncorrelated.\nGeneral Form:\nFor \\(p\\) variables, a diagonal covariance matrix \\(\\Sigma\\) is represented as:\n\\[\n\\Sigma =\n\\begin{bmatrix}\n\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_p^2\n\\end{bmatrix}\n\\]\nProperties:\n\nDiagonal Elements (\\(\\sigma_i^2\\)): Represent the variance of each variable \\(X_i\\).\nOff-Diagonal Elements: All equal to zero (\\(\\text{Cov}(X_i, X_j) = 0\\) for \\(i \\neq j\\)), indicating no linear relationship between variables.\nA diagonal covariance matrix assumes independence between variables. Each variable varies independently without influencing the others.\nCommonly used in simpler models, such as Naive Bayes, where independence is assumed."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes",
    "href": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\n\nLogistic regression models \\(\\Pr(Y = k | X = x)\\) directly, via the logistic function. Similarly, the multinomial logistic regression uses the softmax function. These all model the conditional distribution of \\(Y\\) given \\(X\\).\nBy contrast, generative models start with the conditional distribution of \\(X\\) given \\(Y\\), and then use Bayes formula to turn things around:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^{K} \\pi_l f_l(x)}.\n\\]\n\n\\(f_k(x)\\) is the density of \\(X\\) given \\(Y = k\\);\n\\(\\pi_k = \\Pr(Y = k)\\) is the marginal probability that \\(Y\\) is in class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes-1",
    "href": "lecture_slides/04_classification/04_classification.html#generative-models-and-naïve-bayes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generative Models and Naïve Bayes",
    "text": "Generative Models and Naïve Bayes\n\n\n\nLinear and quadratic discriminant analysis derive from generative models, where \\(f_k(x)\\) are Gaussian.\nUseful if some classes are well separated. A situation where logistic regression is unstable.\nNaïve Bayes assumes that the densities \\(f_k(x)\\) in each class factor:\n\n\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)\n\\]\n\nEquivalently, this assumes that the features are independent within each class.\nThen using Bayes formula:\n\n\\[\n\\Pr(Y = k | X = x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)}{\\sum_{l=1}^{K} \\pi_l \\times f_{l1}(x_1) \\times f_{l2}(x_2) \\times \\cdots \\times f_{lp}(x_p)}\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-details",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Details",
    "text": "Naïve Bayes — Details\nWhy the independence assumption?\n\nDifficult to specify and model high-dimensional densities.\nMuch easier to specify one-dimensional densities.\nCan handle mixed features:\n\nIf feature \\(j\\) is quantitative, can model as univariate Gaussian, for example: \\(X_j \\mid Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2).\\) We estimate \\(\\mu_{jk}\\) and \\(\\sigma_{jk}^2\\) from the data, and then plug into Gaussian density formula for \\(f_{jk}(x_j)\\).\nAlternatively, can use a histogram estimate of the density, and directly estimate \\(f_{jk}(x_j)\\) by the proportion of observations in the bin into which \\(x_j\\) falls.\nIf feature \\(j\\) is qualitative, can simply model the proportion in each category.\n\nSomewhat unrealistic but extremely useful in many cases.\nDespite its simplicity, often shows good classification performance due to reduced variance."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis toy example demonstrates the working of the Naïve Bayes classifier for two classes (\\(k = 1\\) and \\(k = 2\\)) and three features (\\(X_1, X_2, X_3\\)). The goal is to compute the posterior probabilities \\(\\Pr(Y = 1 \\mid X = x^*)\\) and \\(\\Pr(Y = 2 \\mid X = x^*)\\) for a given observation \\(x^* = (0.4, 1.5, 1)\\).\n\nThe prior probabilities for each class are:\n\\(\\hat{\\pi}_1 = \\hat{\\pi}_2 = 0.5\\)\n\n\nFor each feature (\\(X_1, X_2, X_3\\)), we estimate the class-conditional density functions:\n\n\\(\\hat{f}_{11}, \\hat{f}_{12}, \\hat{f}_{13}\\): Densities for \\(k = 1\\) (class 1).\n\n\\(\\hat{f}_{11}(0.4) = 0.368 \\\\\\)\n\\(\\hat{f}_{12}(1.5) = 0.484 \\\\\\)\n\\(\\hat{f}_{13}(1) = 0.226 \\\\\\)\n\n\\(\\hat{f}_{21}, \\hat{f}_{22}, \\hat{f}_{23}\\): Densities for \\(k = 2\\) (class 2).\n\n\\(\\hat{f}_{21}(0.4) = 0.030 \\\\\\)\n\\(\\hat{f}_{22}(1.5) = 0.130 \\\\\\)\n\\(\\hat{f}_{23}(1) = 0.616 \\\\\\)"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example-1",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-toy-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes — Toy Example",
    "text": "Naïve Bayes — Toy Example\n\n\n\n\n\nCompute Class-Conditional Likelihoods for each class \\(k\\), the likelihood is computed as the product of the conditional densities for each feature:\n\n\\[\n   \\hat{f}_k(x^*) = \\prod_{j=1}^3 \\hat{f}_{kj}(x_j^*)\n\\]\n\nFor \\(k = 1\\):\n\n\\[\n     \\hat{f}_{11}(0.4) = 0.368, \\quad \\hat{f}_{12}(1.5) = 0.484, \\quad \\hat{f}_{13}(1) = 0.226\n\\]\n\\[\n     \\hat{f}_1(x^*) = 0.368 \\times 0.484 \\times 0.226 \\approx 0.0402\n\\]\n\nFor \\(k = 2\\):\n\n\\[\n     \\hat{f}_{21}(0.4) = 0.030, \\quad \\hat{f}_{22}(1.5) = 0.130, \\quad \\hat{f}_{23}(1) = 0.616\n\\]\n\\[\n     \\hat{f}_2(x^*) = 0.030 \\times 0.130 \\times 0.616 \\approx 0.0024\n\\]\n\n\n\nCompute Posterior Probabilities using Bayes’ theorem:\n\n\\[\n   \\Pr(Y = k \\mid X = x^*) = \\frac{\\hat{\\pi}_k \\hat{f}_k(x^*)}{\\sum_{k=1}^2 \\hat{\\pi}_k \\hat{f}_k(x^*)}\n\\]\n\nFor \\(k = 1\\): \\[\n\\Pr(Y = 1 \\mid X = x^*) = \\frac{0.5 \\times 0.0402}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.944\n\\]\nFor \\(k = 2\\): \\[\n\\Pr(Y = 2 \\mid X = x^*) = \\frac{0.5 \\times 0.0024}{(0.5 \\times 0.0402) + (0.5 \\times 0.0024)} \\approx 0.056\n\\]\n\n\n\nKey Takeaways:\n\nNaïve Bayes Assumption: The assumption of feature independence simplifies computation by allowing the class-conditional densities to be computed separately for each feature.\nPosterior Probabilities: The posterior probability combines the prior (\\(\\pi_k\\)) and the likelihood (\\(\\hat{f}_k(x^*)\\)).\nClassification: The observation \\(x^*\\) is classified as the class with the highest posterior probability (\\(Y = 1\\))."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs",
    "text": "Naïve Bayes and GAMs\n\n\nNaïve Bayes classifier can be understood as a special case of a GAM.\n\\[\n\\begin{aligned}\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n&= \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right) \\\\\n&= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right) \\\\\n&= a_k + \\sum_{j=1}^p g_{kj}(x_j),\n\\end{aligned}\n\\]\nwhere \\(a_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\) and \\(g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\).\nHence, the Naïve Bayes model is a Generalized Additive Model (GAM):\n\nThe log-odds are expressed as a sum of additive terms.\n\\(a_k\\): Represents prior influence.\n\\(g_{kj}(x_j)\\): Represents feature contributions."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nLog-Odds of Posterior Probabilities\nThe Naïve Bayes classifier starts with the log-odds of the posterior probabilities:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right)\n\\]\nThis is the log of the ratio of the probabilities of class \\(k\\) and a reference class \\(K\\), given the feature vector \\(X = x\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-1",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nBayes’ Theorem\nUsing Bayes’ theorem, the posterior probabilities can be expressed as:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = \\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right)\n\\]\n\n\\(\\pi_k\\): Prior probability of class \\(k\\).\n\\(f_k(x)\\): Class-conditional density for class \\(k\\)."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-2",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nNaïve Bayes Assumption\nThe Naïve Bayes assumption states that features are conditionally independent given the class:\n\\[\nf_k(x) = \\prod_{j=1}^p f_{kj}(x_j)\n\\]\nSubstituting this into the equation:\n\\[\n\\log \\left( \\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)} \\right) = \\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-3",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nSeparate the Terms\nThe terms can now be separated:\n\\[\n\\log \\left( \\frac{\\pi_k \\prod_{j=1}^p f_{kj}(x_j)}{\\pi_K \\prod_{j=1}^p f_{Kj}(x_j)} \\right)\n= \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right) + \\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\n\n\\(\\log \\left( \\frac{\\pi_k}{\\pi_K} \\right)\\): Influence of prior probabilities.\n\\(\\sum_{j=1}^p \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\\): Contribution from each feature."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-4",
    "href": "lecture_slides/04_classification/04_classification.html#naïve-bayes-and-gams-details-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Naïve Bayes and GAMs: details",
    "text": "Naïve Bayes and GAMs: details\nAdditive Form\nDefine:\n\\[\na_k = \\log \\left( \\frac{\\pi_k}{\\pi_K} \\right), \\quad g_{kj}(x_j) = \\log \\left( \\frac{f_{kj}(x_j)}{f_{Kj}(x_j)} \\right)\n\\]\nThe equation becomes:\n\\[\n\\log \\left( \\frac{\\Pr(Y = k \\mid X = x)}{\\Pr(Y = K \\mid X = x)} \\right) = a_k + \\sum_{j=1}^p g_{kj}(x_j)\n\\]"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-1",
    "href": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nLinear regression is used for quantitative responses.\nLinear logistic regression is the counterpart for a binary response and models the logit of the probability as a linear model.\nOther response types exist, such as non-negative responses, skewed distributions, and more.\nGeneralized linear models provide a unified framework for dealing with many different response types."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-bikeshare-data",
    "href": "lecture_slides/04_classification/04_classification.html#example-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Bikeshare Data",
    "text": "Example: Bikeshare Data\n\n\nLinear regression with response bikers: number of hourly users in the bikeshare program in Washington, DC.\n\n\n\n\n\n\n\n\n\n\nPredictor\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n73.60\n5.13\n14.34\n0.00\n\n\nworkingday\n1.27\n1.78\n0.71\n0.48\n\n\ntemp\n157.21\n10.26\n15.32\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-12.89\n1.96\n-6.56\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-66.49\n2.97\n-22.43\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-109.75\n76.67\n-1.43\n0.15"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-meanvariance-relationship",
    "href": "lecture_slides/04_classification/04_classification.html#example-meanvariance-relationship",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Mean/Variance Relationship",
    "text": "Example: Mean/Variance Relationship\n\n\nLeft plot: we see that the variance mostly increases with the mean.\n10% of a linear model predictions are negative! (not shown here.). However, we know that the response variable, bikers, is always positive.\nTaking log(bikers) alleviates this, but is not a good solution. It has its own problems: e.g. predictions are on the wrong scale, and some counts are zero!"
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#poisson-regression-model",
    "href": "lecture_slides/04_classification/04_classification.html#poisson-regression-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\nPoisson distribution is useful for modeling counts:\n\\[\n  Pr(Y = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}, \\, \\text{for } k = 0, 1, 2, \\ldots\n\\]\nMean/variance relationship: \\(\\lambda = \\mathbb{E}(Y) = \\text{Var}(Y)\\) i.e., there is a mean/variance dependence. When the mean is higher, the variance is higher.\nModel with Covariates:\n\\[\n  \\log(\\lambda(X_1, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\nOr equivalently:\n\\[\n  \\lambda(X_1, \\ldots, X_p) = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\n\\]\nAutomatic positivity: The model ensures that predictions are non-negative by construction."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#example-poisson-regression-on-bikeshare-data",
    "href": "lecture_slides/04_classification/04_classification.html#example-poisson-regression-on-bikeshare-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Poisson Regression on Bikeshare Data",
    "text": "Example: Poisson Regression on Bikeshare Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. error\nz-statistic\np-value\n\n\n\n\nIntercept\n4.12\n0.01\n683.96\n0.00\n\n\nworkingday\n0.01\n0.00\n7.50\n0.00\n\n\ntemp\n0.79\n0.01\n68.43\n0.00\n\n\nweathersit \\(cloudy/misty\\)\n-0.08\n0.00\n-34.53\n0.00\n\n\nweathersit \\(light rain/snow\\)\n-0.58\n0.00\n-141.91\n0.00\n\n\nweathersit \\(heavy rain/snow\\)\n-0.93\n0.17\n-5.55\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\nNote: in this case, the variance is somewhat larger than the mean — a situation known as overdispersion. As a result, the p-values may be misleadingly small."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-2",
    "href": "lecture_slides/04_classification/04_classification.html#generalized-linear-models-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\n\n\nWe have covered three GLMs: Gaussian, binomial, and Poisson.\nThey each have a characteristic link function. This is the transformation of the mean represented by a linear model:\n\n\\[\n\\eta(\\mathbb{E}(Y|X_1, X_2, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n\\]\n\nThe link functions for linear, logistic, and Poisson regression are \\(\\eta(\\mu) = \\mu\\), \\(\\eta(\\mu) = \\log(\\mu / (1 - \\mu))\\), \\(\\eta(\\mu) = \\log(\\mu)\\), respectively.\nEach GLM has a characteristic variance function.\nThe models are fit by maximum likelihood, and model summaries are produced using glm() in R.\nOther GLMs include Gamma, Negative-binomial, Inverse Gaussian, and more."
  },
  {
    "objectID": "lecture_slides/04_classification/04_classification.html#summary-1",
    "href": "lecture_slides/04_classification/04_classification.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nKey Concepts:\n\nClassification involves predicting categorical outcomes based on input features.\nPopular approaches include:\n\nLogistic Regression: Directly models probabilities; suitable for \\(K=2\\) and extendable to \\(K &gt; 2\\).\nDiscriminant Analysis: Assumes Gaussian distributions; suitable for small datasets or when classes are well separated.\nNaïve Bayes: Assumes feature independence; works well with large \\(p\\) or mixed data types.\n\nThresholds and ROC Curves allow fine-tuning between false positive and false negative rates.\n\n\n\n\nPractical Insights\n\nLinear vs Logistic Regression: Logistic regression avoids issues with probabilities outside [0, 1].\nDiscriminant Analysis: Use Linear Discriminant Analysis (LDA) for shared covariance matrices or Quadratic Discriminant Analysis (QDA) when covariance matrices differ.\nNaïve Bayes: Despite its simplicity, it often performs well due to reduced variance and works for both qualitative and quantitative data.\nGeneralized Linear Models (GLMs): Extend regression to different types of responses with appropriate link and variance functions."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#overview",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nMotivation\nTraining Error versus Test Error\nValidation-Set Approach\nCross-Validation\nCross-Validation for Classification Problems\n\n\n\nBootstrap\nMore on Bootstrap\nCan the Bootstrap Estimate Prediction Error?\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#xxxx",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#xxxx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "XXXX",
    "text": "XXXX"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-and-the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-and-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-validation and the Bootstrap",
    "text": "Cross-validation and the Bootstrap\n\nIn this section we discuss two resampling methods: cross-validation and the bootstrap.\nThese methods refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model.\nFor example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training Error versus Test Error",
    "text": "Training Error versus Test Error\n\nRecall the distinction between the test error and the training error:\nThe test error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.\nIn contrast, the training error can be easily calculated by applying the statistical learning method to the observations used in its training.\nBut the training error rate often is quite different from the test error rate, and in particular, the former can dramatically underestimate the latter."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training--versus-test-set-performance",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training--versus-test-set-performance",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training- versus Test-Set Performance",
    "text": "Training- versus Test-Set Performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHorizontal Axis: Represents model complexity (low to high).\n\nLow complexity: Simpler models with fewer parameters (e.g., fitting a straight line or using a few features).\nHigh complexity: More complex models with many parameters (e.g., higher-degree polynomials or many features).\n\nVertical Axis: Represents prediction error.\n\nLower values indicate better predictive performance.\n\n\n\n\nTraining Error (Blue Curve):\n\nStarts high at low complexity because simple models underfit the training data.\nDecreases steadily as the model becomes more complex, fitting the training data better.\nContinues to decline even as the model becomes overly complex.\n\nTest Error (Red Curve):\n\nStarts high at low complexity due to underfitting (failure to generalize).\nDecreases as complexity increases and the model starts capturing relevant patterns.\nReaches a minimum at the optimal complexity (sweet spot).\nIncreases again at high complexity due to overfitting (model captures noise instead of general patterns).\n\n\n\nKey Concepts\nBias-Variance Tradeoff:\n\nHigh Bias (Left Side): Simple models fail to capture the true structure of the data.\nHigh Variance (Right Side): Complex models become overly tailored to the training data and fail to generalize.\n\nOptimal Complexity:\n\nLocated where the test error is minimized.\nBalances bias and variance for the best generalization performance.\n\nThe Goal is to select a model complexity that minimizes test error to ensure good predictive performance on unseen data."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#more-on-prediction-error-estimates",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#more-on-prediction-error-estimates",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Prediction-Error Estimates",
    "text": "More on Prediction-Error Estimates\n\nBest solution: test the model with a large test set.\n\nHowever, it is not very often available.\n\nIn the absence of a large test set, some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate.\n\nThese include the Cp statistic, AIC, and BIC.\n\nIn this lecture we consider a class of methods that estimate the test error by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held-out observations."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation-Set Approach",
    "text": "Validation-Set Approach\n\nHere we randomly divide the available set of samples into two parts: a training set and a validation or hold-out set.\nThe model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.\nThe resulting validation-set error provides an estimate of the test error.\n\nThis is typically assessed using the Mean Squared Error (MSE) in the case of a quantitative response and Misclassification Rate in the case of a qualitative (discrete) response."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Validation Process",
    "text": "The Validation Process\n\n\nA random splitting of the original dataset into two halves (two-fold validation):\n\nLeft part is the training set\nRight part is the validation set"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-validation-process-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Validation Process",
    "text": "The Validation Process\n\nA random splitting into two halves:\nLeft part is the training set, and the right part is the validation set."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-automobile-data",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-automobile-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Automobile Data",
    "text": "Example: Automobile Data\n\n\n\nWant to compare linear vs higher-order polynomial terms in a linear regression.\nWe randomly split the 392 observations into two sets:\n\nA training set containing 196 of the data points.\nA validation set containing the remaining 196 observations.\n\n\n\n\n\n\n\n\n\n\n\nLeft panel shows single split; right panel shows multiple splits."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#drawbacks-of-validation-set-approach",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#drawbacks-of-validation-set-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Drawbacks of Validation Set Approach",
    "text": "Drawbacks of Validation Set Approach\n\nThe validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\nIn the validation approach, only a subset of the observations — those that are included in the training set rather than in the validation set — are used to fit the model.\nThis suggests that the validation set error may tend to overestimate the test error for the model fit on the entire data set. Why?\n\nHaving more data generally leads to lower error because it provides more information for training the model.\nFor example, training on 200 observations is typically preferable to 100 observations, as larger datasets improve accuracy.\nHowever, when the training set is reduced (e.g., during validation), error estimates can be higher since smaller datasets may fail to capture all patterns in the data.\nThis limitation highlights the drawbacks of simple validation.\nCross-validation addresses this issue by efficiently using the data to produce more accurate and reliable error estimates."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\nWidely used approach for estimating test error.\nEstimates can be used to select the best model and to give an idea of the test error of the final chosen model.\nThe idea is to randomly divide the data into \\(K\\) equal-sized parts. We leave out part \\(k\\), fit the model to the other \\(K-1\\) parts (combined), and then obtain predictions for the left-out \\(k\\)-th part.\nThis is done in turn for each part \\(k = 1, 2, \\ldots, K\\), and then the results are combined."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation in Detail",
    "text": "K-Fold Cross-Validation in Detail\nDivide data into \\(K\\) roughly equal-sized parts (\\(K = 5\\) here)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-details",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Details",
    "text": "The Details\n\nLet the \\(K\\) parts be \\(C_1, C_2, \\ldots, C_K\\), where \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(N\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\nCompute:\n\n\\[\n  \\text{CV}_{(K)} = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{MSE}_k\n\\]\nwhere \\(\\text{MSE}_k = \\frac{\\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2}{n_k}\\), and \\(\\hat{y}_i\\) is the fit for observation \\(i\\), obtained from the data with part \\(k\\) removed.\n\nSetting \\(K = n\\) yields \\(n\\)-fold or leave-one-out cross-validation (LOOCV)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-nice-special-case",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-nice-special-case",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A Nice Special Case!",
    "text": "A Nice Special Case!\nWith least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:\n\\[\n  \\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2,\n\\]\nwhere \\(\\hat{y}_i\\) is the \\(i\\)-th fitted value from the original least-squares fit, and \\(h_i\\) is the leverage (diagonal of the “hat” matrix; see book for details). This is like the ordinary MSE, except the \\(i\\)-th residual is divided by \\(1 - h_i\\).\n\nLOOCV is sometimes useful, but typically doesn’t shake up the data enough. The estimates from each fold are highly correlated, and hence their average can have high variance.\nA better choice is \\(K = 5\\) or \\(K = 10\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#auto-data-revisited",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#auto-data-revisited",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Auto Data Revisited",
    "text": "Auto Data Revisited"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#true-and-estimated-test-mse-for-the-simulated-data",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#true-and-estimated-test-mse-for-the-simulated-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "True and Estimated Test MSE for the Simulated Data",
    "text": "True and Estimated Test MSE for the Simulated Data\n\n\n\n\n\n\n\n\n\n\n\nThe plot presents the cross-validation estimates and true test error rates that result from applying smoothing splines to the simulated data sets illustrated in Figures 2.9–2.11 of Chapter 2 of the book.\nThe true test MSE is displayed in blue.\nThe black dashed and orange solid lines respectively show the estimated LOOCV and 10-fold CV estimates.\nIn all three plots, the two cross-validation estimates are very similar.\nRight-hand panel: the true test MSE and the cross-validation curves are almost identical.\nCenter panel: the two sets of curves are similar at the lower degrees of flexibility, while the CV curves overestimate the test set MSE for higher degrees of flexibility.\nLeft-hand panel: the CV curves have the correct general shape, but they underestimate the true test MSE."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-issues-with-cross-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-issues-with-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Issues with Cross-Validation",
    "text": "Other Issues with Cross-Validation\n\nSince each training set is only \\(\\frac{K - 1}{K}\\) as big as the original training set, the estimates of prediction error will typically be biased upward. Why?\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has high variance, as noted earlier.\n\\(K = 5\\) or \\(10\\) provides a good compromise for this bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-Validation for Classification Problems",
    "text": "Cross-Validation for Classification Problems\n\nWe divide the data into \\(K\\) roughly equal-sized parts \\(C_1, C_2, \\ldots, C_K\\). \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(n\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\nCompute:\n\n\\[\n  \\text{CV}_K = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{Err}_k\n\\]\nwhere \\(\\text{Err}_k = \\frac{\\sum_{i \\in C_k} I(y_i \\neq \\hat{y}_i)}{n_k}\\).\n\nThe estimated standard deviation of \\(\\text{CV}_K\\) is:\n\n\\[\n  \\widehat{\\text{SE}}(\\text{CV}_K) = \\sqrt{\\frac{1}{K} \\sum_{k=1}^{K} \\frac{(\\text{Err}_k - \\overline{\\text{Err}_k})^2}{K - 1}}\n\\]\n\nThis is a useful estimate, but strictly speaking, not quite valid. Why not?"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-right-and-wrong",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-right-and-wrong",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-Validation: Right and Wrong",
    "text": "Cross-Validation: Right and Wrong\n\nConsider a simple classifier applied to some two-class data:\n\nStarting with 5000 predictors and 50 samples, find the 100 predictors having the largest correlation with the class labels.\nWe then apply a classifier such as logistic regression, using only these 100 predictors.\n\n\nHow do we estimate the test set performance of this classifier?\nCan we apply cross-validation in step 2, forgetting about step 1?"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#no",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#no",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "NO!",
    "text": "NO!\n\nThis would ignore the fact that in Step 1, the procedure has already seen the labels of the training data, and made use of them. This is a form of training and must be included in the validation process.\nIt is easy to simulate realistic data with the class labels independent of the outcome, so that true test error = 50%, but the CV error estimate that ignores Step 1 is zero! Try to do this yourself.\nWe have seen this error made in many high-profile genomics papers."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-wrong-and-right-way",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-wrong-and-right-way",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Wrong and Right Way",
    "text": "The Wrong and Right Way\n\nWrong: Apply cross-validation in step 2.\nRight: Apply cross-validation to steps 1 and 2."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#wrong-way",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#wrong-way",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Wrong Way",
    "text": "Wrong Way"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#right-way",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#right-way",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Right Way",
    "text": "Right Way"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap",
    "text": "The Bootstrap\n\nThe bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.\nFor example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#where-does-the-name-come-from",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#where-does-the-name-come-from",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Where Does the Name Come From?",
    "text": "Where Does the Name Come From?\n\nThe use of the term bootstrap derives from the phrase to pull oneself up by one’s bootstraps, widely thought to be based on one of the eighteenth-century The Surprising Adventures of Baron Munchausen by Rudolph Erich Raspe:\n\nThe Baron had fallen to the bottom of a deep lake. Just when it looked like all was lost, he thought to pick himself up by his own bootstraps.\n\nIt is not the same as the term bootstrap used in computer science, meaning to “boot” a computer from a set of core instructions, though the derivation is similar."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-simple-example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-simple-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A Simple Example",
    "text": "A Simple Example\n\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y\\), respectively, where \\(X\\) and \\(Y\\) are random quantities.\nWe will invest a fraction \\(\\alpha\\) of our money in \\(X\\), and will invest the remaining \\(1 - \\alpha\\) in \\(Y\\).\nWe wish to choose \\(\\alpha\\) to minimize the total risk, or variance, of our investment. In other words, we want to minimize \\(\\text{Var}(\\alpha X + (1 - \\alpha) Y).\\)\nOne can show that the value that minimizes the risk is given by:\n\n\\[\n  \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}},\n\\] where \\(\\sigma_X^2 = \\text{Var}(X)\\), \\(\\sigma_Y^2 = \\text{Var}(Y)\\), and \\(\\sigma_{XY} = \\text{Cov}(X, Y)\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\nBut the values of \\(\\sigma_X^2\\), \\(\\sigma_Y^2\\), and \\(\\sigma_{XY}\\) are unknown.\nWe can compute estimates for these quantities, \\(\\hat{\\sigma}_X^2\\), \\(\\hat{\\sigma}_Y^2\\), and \\(\\hat{\\sigma}_{XY}\\), using a data set that contains measurements for \\(X\\) and \\(Y\\).\nWe can then estimate the value of \\(\\alpha\\) that minimizes the variance of our investment using:\n\n\n\\[\n  \\hat{\\alpha} = \\frac{\\hat{\\sigma}_Y^2 - \\hat{\\sigma}_{XY}}{\\hat{\\sigma}_X^2 + \\hat{\\sigma}_Y^2 - 2\\hat{\\sigma}_{XY}}.\n\\]"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\n\n\n\n\n\n\n\n\n\nEach panel displays 100 simulated returns for investments X and Y. From left to right and top to bottom, the resulting estimates for \\(\\alpha\\), the fraction to minimize the total risk, are 0.576, 0.532, 0.657, and 0.651."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-2",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\n\n\nTo estimate the standard deviation of \\(\\hat{\\alpha}\\), we repeated the process of simulating 100 paired observations of \\(X\\) and \\(Y\\), and estimating \\(\\alpha\\) 1,000 times.\nWe thereby obtained 1,000 estimates for \\(\\alpha\\), which we can call \\(\\hat{\\alpha}_1, \\hat{\\alpha}_2, \\ldots, \\hat{\\alpha}_{1000}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe left-hand panel of the Figure displays a histogram of the resulting estimates.\nFor these simulations, the parameters were set to \\(\\sigma_X^2 = 1, \\, \\sigma_Y^2 = 1.25, \\, \\sigma_{XY} = 0.5,\\) and so we know that the true value of \\(\\alpha\\) is 0.6 (indicated by the red line)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-3",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-continued-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued",
    "text": "Example Continued\nThe mean over all 1,000 estimates for \\(\\alpha\\) is:\n\\[\n  \\bar{\\alpha} = \\frac{1}{1000} \\sum_{r=1}^{1000} \\hat{\\alpha}_r = 0.5996,\n\\]\nvery close to \\(\\alpha = 0.6\\), and the standard deviation of the estimates is:\n\n\\[\n  \\sqrt{\\frac{1}{1000 - 1} \\sum_{r=1}^{1000} (\\hat{\\alpha}_r - \\bar{\\alpha})^2} = 0.083.\n\\]\n\nThis gives us a very good idea of the accuracy of \\(\\hat{\\alpha}\\): \\(\\text{SE}(\\hat{\\alpha}) \\approx 0.083\\).\nSo roughly speaking, for a random sample from the population, we would expect \\(\\hat{\\alpha}\\) to differ from \\(\\alpha\\) by approximately 0.08, on average."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\nComparison of the microarray predictor with some clinical predictors, using logistic regression with outcome prognosis:\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nRe-use\n\n\n\n\n\n\nmicroarray\n4.096\n1.092\n3.753\n0.000\n\n\nangio\n1.208\n0.816\n1.482\n0.069\n\n\ner\n-0.554\n1.044\n-0.530\n0.298\n\n\ngrade\n-0.697\n1.003\n-0.695\n0.243\n\n\npr\n1.214\n1.057\n1.149\n0.125\n\n\nage\n-1.593\n0.911\n-1.748\n0.040\n\n\nsize\n1.483\n0.732\n2.026\n0.021\n\n\n\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nPre-validated\n\n\n\n\n\n\nmicroarray\n1.549\n0.675\n2.296\n0.011\n\n\nangio\n1.589\n0.682\n2.329\n0.010\n\n\ner\n-0.617\n0.894\n-0.690\n0.245\n\n\ngrade\n0.719\n0.720\n0.999\n0.159\n\n\npr\n0.537\n0.863\n0.622\n0.267\n\n\nage\n-1.471\n0.701\n-2.099\n0.018\n\n\nsize\n0.998\n0.594\n1.681\n0.046"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#now-back-to-the-real-world",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#now-back-to-the-real-world",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Now Back to the Real World",
    "text": "Now Back to the Real World\n\nThe procedure outlined above cannot be applied, because for real data we cannot generate new samples from the original population.\nHowever, the bootstrap approach allows us to use a computer to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.\nRather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set with replacement.\nEach of these “bootstrap data sets” is created by sampling with replacement, and is the same size as our original dataset. As a result, some observations may appear more than once in a given bootstrap data set and some not at all."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-with-just-3-observations",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-with-just-3-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example with Just 3 Observations",
    "text": "Example with Just 3 Observations\n\n\nA graphical illustration of the bootstrap approach on a small sample containing \\(n = 3\\) observations.\nEach bootstrap data set contains \\(n\\) observations, sampled with replacement from the original data set.\nEach bootstrap data set is used to obtain an estimate of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#bootstrap-standard-error",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#bootstrap-standard-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bootstrap Standard Error",
    "text": "Bootstrap Standard Error\n\nDenoting the first bootstrap data set by \\(Z^{*1}\\), we use \\(Z^{*1}\\) to produce a new bootstrap estimate for \\(\\alpha\\), which we call \\(\\hat{\\alpha}^{*1}\\).\nThis procedure is repeated \\(B\\) times for some large value of \\(B\\) (say 100 or 1000), in order to produce \\(B\\) different bootstrap data sets, \\(Z^{*1}, Z^{*2}, \\ldots, Z^{*B}\\), and \\(B\\) corresponding \\(\\alpha\\) estimates, \\(\\hat{\\alpha}^{*1}, \\hat{\\alpha}^{*2}, \\ldots, \\hat{\\alpha}^{*B}\\).\nWe estimate the standard error of these bootstrap estimates using the formula:\n\n\n\\[\nSE_B(\\hat{\\alpha}) = \\sqrt{\\frac{1}{B - 1} \\sum_{r=1}^B (\\hat{\\alpha}^{*r} - \\bar{\\alpha}^{*})^2}.\n\\]\n\nThis serves as an estimate of the standard error of \\(\\hat{\\alpha}\\) estimated from the original data set. See center and right panels of Figure on slide 29. Bootstrap results are in blue.\nFor this example \\(SE_B(\\hat{\\alpha}) = 0.087\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-general-picture-for-the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#a-general-picture-for-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "A General Picture for the Bootstrap",
    "text": "A General Picture for the Bootstrap\n\n\n\n\n\nReal World\n\nPopulation \\(P\\)\n\nWe imagine there is a true, unknown population (or data‐generating process).\n\nIn practice, we typically do not have direct access to all of \\(P\\).\n\nRandom Sampling\n\nWe draw a finite sample \\(Z = (z_1, z_2, \\dots, z_n)\\) from the population \\(P\\).\n\nThis sample \\(Z\\) is our observed dataset (often called the “training data” in applied work).\n\nEstimate \\(f(Z)\\)\n\nFrom this observed data \\(Z\\), we compute a statistic or estimate, denoted \\(f(Z)\\).\n\nExamples might include a mean, a regression coefficient, or (in the investment example) an optimal allocation parameter \\(\\alpha\\).\n\n\nIn short, the Real World side shows how our single dataset \\(Z\\) arrives by randomly sampling from the true population \\(P\\).\n\nBootstrap World\n\nEstimated Population \\(\\hat{P}\\)\n\nBecause we usually cannot sample repeatedly from the real population \\(P\\), the bootstrap creates a stand‐in population \\(\\hat{P}\\). We ‘replace’ the population by our sample.\n\\(\\hat{P}\\) is the empirical distribution function of the observed data \\(Z\\). Informally, it assigns probability \\(\\tfrac{1}{n}\\) to each observed point in \\(Z\\).\n\nRandom Sampling from \\(\\hat{P}\\)\n\nTo mimic drawing new data from the real population, we instead draw (with replacement) from \\(\\hat{P}\\).\n\nThis produces a bootstrap dataset \\(Z^* = (z_1^*, z_2^*, \\dots, z_n^*)\\). Each \\(z_i^*\\) is sampled (with replacement) from among the original observed points \\(\\{z_1, \\dots, z_n\\}\\).\n\nBootstrap Estimate \\(f(Z^*)\\)\n\nWe compute the same statistic (or estimator) on each bootstrap sample, giving \\(f(Z^*)\\).\n\nBy repeating this bootstrap sampling many times, we obtain a distribution of estimates \\(\\{f(Z^*_1), f(Z^*_2), \\dots\\}\\). This approximates how \\(f(Z)\\) would vary if we could repeatedly resample from the true population."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-in-general",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-in-general",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap in General",
    "text": "The Bootstrap in General\n\nIn more complex data situations, figuring out the appropriate way to generate bootstrap samples can require some thought.\nFor example, if the data is a time series, we can’t simply sample the observations with replacement (why not?).\n\nThe main reason we typically cannot simply resample individual points with replacement in a time series is that time‐ordered data exhibits serial dependence. That is, adjacent observations (e.g., today’s stock price and yesterday’s stock price) are correlated in ways that we lose if we treat all observations as independent units and shuffle them arbitrarily.\nA simple i.i.d. bootstrap would ignore the natural ordering of the data points (and the correlations it encodes), thereby violating a crucial assumption about the structure of time‐series data.\n\nWe can instead create blocks of consecutive observations and sample those with replacements. Then we paste together sampled blocks to obtain a bootstrap dataset."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-uses-of-the-bootstrap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#other-uses-of-the-bootstrap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Other Uses of the Bootstrap",
    "text": "Other Uses of the Bootstrap\n\nPrimarily used to obtain standard errors of an estimate.\nAlso provides approximate confidence intervals for a population parameter. For example, looking at the histogram in the middle panel of the figure on slide 29, the 5% and 95% quantiles of the 1,000 values is (0.43, 0.72).\nThis represents an approximate 90% confidence interval for the true α. How do we interpret this confidence interval?\nThe above interval is called a Bootstrap Percentile confidence interval. It is the simplest method (among many approaches) for obtaining a confidence interval from the bootstrap."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can the Bootstrap Estimate Prediction Error?",
    "text": "Can the Bootstrap Estimate Prediction Error?\n\nIn cross-validation, each of the \\(K\\) validation folds is distinct from the other \\(K-1\\) folds used for training: there is no overlap. This is crucial for its success. Why?\nTo estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training sample, and the original sample as our validation sample.\nBut each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample. Can you prove this?\nThis will cause the bootstrap to seriously underestimate the true prediction error. Why?\nThe other way around— with the original sample as the training sample, and the bootstrap dataset as the validation sample— is worse!"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#removing-the-overlap",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#removing-the-overlap",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Removing the Overlap",
    "text": "Removing the Overlap\n\nCan partly fix this problem by only using predictions for those observations that did not (by chance) occur in the current bootstrap sample.\nBut the method gets complicated, and in the end, cross-validation provides a simpler, more attractive approach for estimating prediction error."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pre-validation",
    "text": "Pre-validation\n\nIn microarray and other genomic studies, an important problem is to compare a predictor of disease outcome derived from a large number of “biomarkers” to standard clinical predictors.\nComparing them on the same dataset that was used to derive the biomarker predictor can lead to results strongly biased in favor of the biomarker predictor.\nPre-validation can be used to make a fairer comparison between the two sets of predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#motivating-example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#motivating-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Motivating Example",
    "text": "Motivating Example\nAn example of this problem arose in the paper of van’t Veer et al. Nature (2002). Their microarray data has 4918 genes measured over 78 cases, taken from a study of breast cancer. There are 44 cases in the good prognosis group and 34 in the poor prognosis group. A “microarray” predictor was constructed as follows:\n\n70 genes were selected, having the largest absolute correlation with the 78 class labels.\nUsing these 70 genes, a nearest-centroid classifier \\(C(x)\\) was constructed.\nApplying the classifier to the 78 microarrays gave a dichotomous predictor \\(z_i = C(x_i)\\) for each case \\(i\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#results-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\nComparison of the microarray predictor with some clinical predictors, using logistic regression with outcome prognosis:\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nRe-use\n\n\n\n\n\n\nmicroarray\n4.096\n1.092\n3.753\n0.000\n\n\nangio\n1.208\n0.816\n1.482\n0.069\n\n\ner\n-0.554\n1.044\n-0.530\n0.298\n\n\ngrade\n-0.697\n1.003\n-0.695\n0.243\n\n\npr\n1.214\n1.057\n1.149\n0.125\n\n\nage\n-1.593\n0.911\n-1.748\n0.040\n\n\nsize\n1.483\n0.732\n2.026\n0.021\n\n\n\n\n\n\nModel\nCoef\nStand. Err.\nZ score\np-value\n\n\n\n\nPre-validated\n\n\n\n\n\n\nmicroarray\n1.549\n0.675\n2.296\n0.011\n\n\nangio\n1.589\n0.682\n2.329\n0.010\n\n\ner\n-0.617\n0.894\n-0.690\n0.245\n\n\ngrade\n0.719\n0.720\n0.999\n0.159\n\n\npr\n0.537\n0.863\n0.622\n0.267\n\n\nage\n-1.471\n0.701\n-2.099\n0.018\n\n\nsize\n0.998\n0.594\n1.681\n0.046"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#idea-behind-pre-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#idea-behind-pre-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Idea behind Pre-validation",
    "text": "Idea behind Pre-validation\n\nDesigned for comparison of adaptively derived predictors to fixed, pre-defined predictors.\nThe idea is to form a “pre-validated” version of the adaptive predictor: specifically, a “fairer” version that hasn’t “seen” the response \\(y\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-process",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pre-validation Process",
    "text": "Pre-validation Process"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-7",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-in-detail-for-this-example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#pre-validation-in-detail-for-this-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pre-validation in Detail for This Example",
    "text": "Pre-validation in Detail for This Example\n\nDivide the cases up into \\(K = 13\\) equal-sized parts of 6 cases each.\nSet aside one of the parts. Using only the data from the other 12 parts:\n\nSelect the features having an absolute correlation of at least 0.3 with the class labels.\nForm a nearest centroid classification rule.\n\nUse the rule to predict the class labels for the 13th part.\nRepeat steps 2 and 3 for each of the 13 parts, yielding a “pre-validated” microarray predictor \\(\\tilde{z}_i\\) for each of the 78 cases.\nFit a logistic regression model to the pre-validated microarray predictor and the 6 clinical predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-versus-permutation-tests",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-versus-permutation-tests",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap versus Permutation Tests",
    "text": "The Bootstrap versus Permutation Tests\n\nBootstrap:\n\nSamples from the estimated population and uses the results to estimate standard errors and confidence intervals.\n\nPermutation Methods:\n\nSample from an estimated null distribution for the data.\nUsed to estimate p-values and False Discovery Rates for hypothesis tests.\n\nBootstrap for Null Hypothesis Testing:\n\nCan test a null hypothesis in simple situations.\nExample: If \\(\\theta = 0\\) is the null hypothesis, check whether the confidence interval for \\(\\theta\\) contains zero.\n\nAdapting Bootstrap for Null Distribution:\n\nCan adapt bootstrap to sample from a null distribution.\nSee Efron and Tibshirani, An Introduction to the Bootstrap (1993), Chapter 16.\nHowever, there is no real advantage over permutation tests."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#prediction",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#prediction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Prediction",
    "text": "Prediction\n\nGoal: Build predictors and classifiers to make accurate predictions from data.\nChallenge: How do we evaluate our predictions?\n\n\nIdeal Scenario: New Data\n\nThe best way to test predictions is to use new, independent data from the population.\nProblem: New data isn’t always available."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#why-not-use-training-data",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#why-not-use-training-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Not Use Training Data?",
    "text": "Why Not Use Training Data?\n\nUsing training data for evaluation is not reliable.\n\nModels tend to perform better on data they’ve already seen.\nThis leads to overly optimistic results."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#solution-resampling-methods",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#solution-resampling-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Solution: Resampling methods",
    "text": "Solution: Resampling methods\n\nCross-validation and the Bootstrap are two resampling methods.\nThese methods allows us to evaluate the performance of our predictors using the available data without relying on additional samples.\nThey refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model.\nFor example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#training-error-versus-test-error-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Training Error versus Test Error",
    "text": "Training Error versus Test Error\n\nRecall the distinction between the test error and the training error:\nThe test error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.\nThe training error can be easily calculated by applying the statistical learning method to the observations used in its training.\nBut the training error rate often is quite different from the test error rate, and in particular, the former can dramatically underestimate the latter."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#precision-and-accuracy",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#precision-and-accuracy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Precision and Accuracy",
    "text": "Precision and Accuracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrecision: Refers to the consistency or reliability of the model’s predictions.\nAccuracy: Refers to how close the model’s predictions are to the true values.\n\n\nIn the context of regression:\n\nHigh Precision, Low Accuracy: Predictions are consistent but biased.\nHigh Precision, High Accuracy: Predictions are both consistent and valid.\nLow Precision, Low Accuracy: Predictions are neither consistent nor valid.\nLow Precision, High Accuracy: Predictions are valid on average but have high variability."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-6",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#validation-set-approach-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation-Set Approach",
    "text": "Validation-Set Approach\n\nHere we randomly divide the available set of samples into two parts: a training set and a validation or hold-out set.\nThe model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.\nThe resulting validation-set error provides an estimate of the test error.\n\nThis is typically assessed using the Mean Squared Error (MSE) in the case of a quantitative response and Misclassification Rate in the case of a qualitative (discrete) response."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\n\nWidely used approach for estimating test error.\nEstimates can be used to select the best model and to give an idea of the test error of the final chosen model.\nThe idea is to randomly divide the data into \\(K\\) equal-sized parts. We leave out part \\(k\\), fit the model to the other \\(K-1\\) parts (combined), and then obtain predictions for the left-out \\(k\\)-th part.\nThis is done in turn for each part \\(k = 1, 2, \\ldots, K\\), and then the results are combined."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-5",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-detail-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation in Detail",
    "text": "K-Fold Cross-Validation in Detail\nDivide data into \\(K\\) roughly equal-sized parts (\\(K = 3\\) here).\n\n\n\n\nWiki"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-algebra",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#k-fold-cross-validation-in-algebra",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Fold Cross-Validation: in algebra",
    "text": "K-Fold Cross-Validation: in algebra\n\nLet the \\(K\\) parts be \\(C_1, C_2, \\ldots, C_K\\), where \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(N\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\nCompute the cross-validations error rate:\n\n\n\\[\n  \\text{CV}_{(K)} = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{MSE}_k\n\\]\nwhere \\(\\text{MSE}_k = \\frac{\\sum_{i \\in C_k} (y_i - \\hat{y}_i)^2}{n_k}\\), and \\(\\hat{y}_i\\) is the fit for observation \\(i\\), obtained from the data with part \\(k\\) removed.\n\nSpecial case: Setting \\(K = n\\) yields \\(n\\)-fold or leave-one-out cross-validation (LOOCV)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#leave-one-out-cross-validation-loocv",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#leave-one-out-cross-validation-loocv",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Leave-One-Out Cross-Validation (LOOCV)\n\n\n\n\nWiki"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-auto-data-revisited",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-auto-data-revisited",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Auto Data Revisited",
    "text": "Example: Auto Data Revisited\n\n\nLeft plot: Similar to the two halve validation;\nRight plot: Tenfold cross validation. With 10 different partitions of the data to train and test the model we see there is not much variability. The results are consistent, in contrast to the result when we divided into two parts."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#potential-issues-with-cross-validation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#potential-issues-with-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Potential Issues with Cross-Validation",
    "text": "Potential Issues with Cross-Validation\n\nSince each training set is only \\(\\frac{K - 1}{K}\\) as big as the original training set, the estimates of prediction error will typically be biased upward. Why?\nThis bias is minimized when \\(K = n\\) (LOOCV), but this estimate has high variance, as noted earlier.\n\\(K = 5\\) or \\(10\\) provides a good compromise for this bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#cross-validation-for-classification-problems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cross-Validation for Classification Problems",
    "text": "Cross-Validation for Classification Problems\n\nWe divide the data into \\(K\\) roughly equal-sized parts \\(C_1, C_2, \\ldots, C_K\\). \\(C_k\\) denotes the indices of the observations in part \\(k\\). There are \\(n_k\\) observations in part \\(k\\): if \\(n\\) is a multiple of \\(K\\), then \\(n_k = n / K\\).\n\nCompute the cross-validation misclassification error:\n\n\n\\[\n  \\text{CV}_K = \\sum_{k=1}^{K} \\frac{n_k}{n} \\text{Err}_k\n\\]\nwhere \\(\\text{Err}_k = \\frac{\\sum_{i \\in C_k} I(y_i \\neq \\hat{y}_i)}{n_k}\\).\n\nThe estimated standard deviation of \\(\\text{CV}_K\\) is:\n\n\n\n\\[\n  \\widehat{\\text{SE}}(\\text{CV}_K) = \\sqrt{\\frac{1}{K} \\sum_{k=1}^{K} \\frac{(\\text{Err}_k - \\overline{\\text{Err}_k})^2}{K - 1}}\n\\]\n\nThis is a useful estimate, but strictly speaking, not quite valid. Why not?\n\nWe compute the standard errors assuming these were independent observations, but they are not strictly independent as they share some training samples. So there’s some correlation between them."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-3",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-setting",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-setting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Setting",
    "text": "The Setting\n\nHigh‐dimensional data: We have 50 samples (observations) but 5000 predictors (features). In many modern applications—such as genomics—it is typical to have many more predictors than observations.\nGoal: Two‐class classification\nFeature selection (Step 1): We first look at the correlation of each of the 5000 predictors with the class labels, and we pick the 100 “best” predictors—the ones that exhibit the largest correlation with the class labels.\nModel fitting (Step 2): Once those top 100 are chosen, we fit a classifier (e.g., logistic regression) using only those top 100 predictors.\nThe question is how to estimate the true test error of this two‐step procedure."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-tempting-but-wrong-approach",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-tempting-but-wrong-approach",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Tempting (but Wrong) Approach",
    "text": "The Tempting (but Wrong) Approach\nA common mistake is to ignore Step 1 when doing cross‐validation and to apply cross‐validation only to Step 2. That is, one might simply take the already‐selected 100 features and then do, say, 10‐fold cross‐validation on the logistic regression.\n\nWhy people do this: It seems natural to say, “Now that we have our 100 features, let’s cross‐validate the classifier we fit with these 100 features.”\nWhat goes wrong: By the time you pick those 100 “best” features, the data set has already “seen” all the labels in the process of ranking and filtering. This filtering step is actually part of training, because it used the outcome labels to choose features.\n\n\nSkipping Step 1 in the cross‐validation will invariably produce an overly optimistic (often wildly optimistic) estimate of test error."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#why-it-is-wrong-data-leakage",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#why-it-is-wrong-data-leakage",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why It Is Wrong: Data Leakage",
    "text": "Why It Is Wrong: Data Leakage\n\nData leakage: The crucial point is that feature selection (filtering) depends on the relationship between each feature and the class labels. Hence, it is not “just a preprocessing step”—it is using the label information. Thus, Step 1 is part of the model‐building process.\nOverfitting by cherry‐picking: With thousands of predictors, even if none is truly predictive, by sheer chance some predictors will appear correlated with the class labels in the sample. Selecting only the strongest correlations can give the illusion that the model has learned meaningful structure, when in fact it is just capturing random noise.\nAn extreme illustration: If you simulate data where the class labels are purely random (true error = 50%), but you pick the top 100 out of 5000 or 5 million random features, then do cross‐validation only after you have chosen those top 100, you can easily see cross‐validation estimates near 0% error—clearly a false, biased result."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-correct-right-way-to-apply-crossvalidation",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-correct-right-way-to-apply-crossvalidation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Correct (Right) Way to Apply Cross‐Validation",
    "text": "The Correct (Right) Way to Apply Cross‐Validation\n\nThe key principle is that any step that uses the outcome labels must occur inside the cross‐validation loop. Concretely:\n\nSplit the data into training/validation folds (e.g., 10‐fold CV).\nFor each fold:\n\nTreat that fold as a hold‐out set.\n\nOn the remaining training folds, perform the entire procedure:\n\nFeature selection (filtering to the top 100 based on correlation with the class labels in the training folds only).\n\nFit the classifier (e.g., logistic regression) to those top 100 features in those training folds.\n\n\nFinally, evaluate the trained model on the hold‐out fold—with only the 100 features selected from the training folds.\n\nRepeat for each fold, then average the error rates (or other metrics).\n\n\nBy doing this, each hold‐out fold is kept separate from both feature selection and model training. This ensures that Step 1 (feature selection) is “relearned” anew in each training subset, just as Step 2 (the classifier) is. As a result, the cross‐validation error you compute properly reflects how the entire procedure—from filtering out thousands of features down to fitting the logistic model—would perform on truly unseen data."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nWrong: Select your 100 predictors once using all the data, then cross‐validate only the final classifier. This leads to overly optimistic, biased estimates of test error because it ignores that you used the labels in selecting those 100 predictors.\nRight: Wrap the entire two‐step process (selection and model fitting) inside the cross‐validation loop. Each fold’s feature‐selection step must be done without knowledge of the hold‐out fold’s labels.\n\n\nFollowing this correct approach is essential whenever one performs early filtering, variable selection, hyperparameter tuning, or any other step that uses the outcome labels. Such steps must be regarded as part of the training process and repeated inside each cross‐validation iteration."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#the-bootstrap-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Bootstrap",
    "text": "The Bootstrap\n\nThe bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.\nFor example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-2",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nResampling Methods\n\nCross-validation and Bootstrap allow evaluation of model performance using existing data.\nThey provide estimates of:\n\nTest-set prediction error\nStandard deviation and bias of parameter estimates.\n\n\nTraining vs Test Error\n\nTraining error decreases with model complexity.\nTest error decreases, then increases due to bias-variance tradeoff:\n\nHigh Bias: Simple models underfit the data.\nHigh Variance: Complex models overfit the training data.\n\nOptimal complexity minimizes test error.\n\n\nValidation-Set Approach\n\nDivides data into training and validation sets.\nValidation error provides an estimate of test error but:\n\nCan vary based on data split.\nMay overestimate test error due to smaller training sets.\n\n\nCross-Validation\n\nK-Fold Cross-Validation:\n\nDivides data into \\(K\\) folds for iterative training and testing.\nBalances bias and variance (e.g., \\(K = 5\\) or \\(10\\)).\n\nLeave-One-Out Cross-Validation (LOOCV):\n\nUses one data point as validation in each iteration.\nLow bias but high variance.\n\n\nBootstrap\n\nEstimates variability and uncertainty of parameter estimates.\nGenerates multiple samples with replacement from the dataset.\nProvides approximate confidence intervals and standard errors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-results",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Results",
    "text": "Example Results\n\nLeft: A histogram of the estimates of \\(\\alpha\\) obtained by generating 1,000 simulated data sets from the true population.\nCenter: A histogram of the estimates of \\(\\alpha\\) obtained from 1,000 bootstrap samples from a single data set.\nRight: The estimates of \\(\\alpha\\) displayed in the left and center panels are shown as boxplots.\n\nIn each panel, the pink line indicates the true value of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#section",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "flowchart LR\n    Observations --&gt;|Predictors| A[Omitted data]\n    A --&gt;|Pre-validated Predictor| B[Response]\n    B --&gt;|Logistic Regression| C[Fixed predictors]\n\n\n\n\n\n\n\nObservations are used to create predictors, with some data omitted.\nPre-validated predictors are derived without access to the response.\nLogistic regression is applied to pre-validated predictors and fixed predictors."
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error-1",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#can-the-bootstrap-estimate-prediction-error-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Can the Bootstrap Estimate Prediction Error?",
    "text": "Can the Bootstrap Estimate Prediction Error?\n\nIn cross-validation, each of the \\(K\\) validation folds is distinct from the other \\(K-1\\) folds used for training: there is no overlap. This is crucial for its success. Why?\n\nThere is a clear separation, no overlap, between the train and the test sets.\n\nTo estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training sample, and the original sample as our validation sample.\nBut each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample.\nThis will cause the bootstrap to seriously underestimate the true prediction error.\nThe other way around— with the original sample as the training sample, and the bootstrap dataset as the validation sample— is worse!"
  },
  {
    "objectID": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example",
    "href": "lecture_slides/05_resampling_methods/05_resampling_methods.html#example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example",
    "text": "Example\n\nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of \\(X\\) and \\(Y\\), respectively, where \\(X\\) and \\(Y\\) are random quantities.\nWe will invest a fraction \\(\\alpha\\) of our money in \\(X\\), and will invest the remaining \\(1 - \\alpha\\) in \\(Y\\).\nWe wish to choose \\(\\alpha\\) to minimize the total risk, or variance, of our investment. In other words, we want to minimize \\(\\text{Var}(\\alpha X + (1 - \\alpha) Y).\\)\nOne can show that the value that minimizes the risk is given by:\n\n\n\\[\n  \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}},\n\\]\nwhere \\(\\sigma_X^2 = \\text{Var}(X)\\), \\(\\sigma_Y^2 = \\text{Var}(Y)\\), and \\(\\sigma_{XY} = \\text{Cov}(X, Y)\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#overview",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nLinear Model Selection and Regularization\nSubset Selection\nStepwise Selection\nForward Stepwise Selection\nBackward Stepwise Selection\nChoosing the Optimal Model\nIndirect Approaches\nValidation and Cross-Validation\n\n\n\nShrinkage Methods\nRidge Regression\nThe Lasso\nSelecting the Tuning Parameter for Ridge Regression and Lasso\nDimension Reduction Methods\nPrincipal Components Regression\nPartial Least Squares (PLS)\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Model Selection and Regularization",
    "text": "Linear Model Selection and Regularization\n\nRecall the linear model\n\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon.\n\\]\n\nIn the lectures that follow, we consider some approaches for extending the linear model framework. In the lectures covering Chapter 7 of the text, we generalize the linear model in order to accommodate non-linear, but still additive, relationships.\nIn the lectures covering Chapter 8, we consider even more general non-linear models."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#in-praise-of-linear-models",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#in-praise-of-linear-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "In praise of linear models!",
    "text": "In praise of linear models!\n\nDespite its simplicity, the linear model has distinct advantages in terms of its interpretability and often shows good predictive performance.\nHence we discuss in this lecture some ways in which the simple linear model can be improved, by replacing ordinary least squares fitting with some alternative fitting procedures."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#why-consider-alternatives-to-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#why-consider-alternatives-to-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why consider alternatives to least squares?",
    "text": "Why consider alternatives to least squares?\n\nPrediction Accuracy: especially when \\(p &gt; n\\), to control the variance.\nModel Interpretability: By removing irrelevant features — that is, by setting the corresponding coefficient estimates to zero — we can obtain a model that is more easily interpreted. We will present some approaches for automatically performing feature selection."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#three-classes-of-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#three-classes-of-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Three classes of methods",
    "text": "Three classes of methods\n\nSubset Selection. We identify a subset of the \\(p\\) predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.Best Subset Selection, Foward Selection, and Backwards Selection are the main techniques here.\nShrinkage. We fit a model involving all \\(p\\) predictors, but the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance and can also perform variable selection. Ridge Regression and Lasso are the main techniques here.\nDimension Reduction. We project the \\(p\\) predictors into a \\(M\\)-dimensional subspace, where \\(M &lt; p\\). This is achieved by computing \\(M\\) different linear combinations, or projections, of the variables. Then these \\(M\\) projections are used as predictors to fit a linear regression model by least squares. Principal Components Regression and Partial Least Squares are the main techniques here."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Subset Selection",
    "text": "Subset Selection\nBest subset and stepwise model selection procedures\nBest Subset Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\nFor \\(k = 1, 2, \\ldots, p\\):\n\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors.\n\n\nPick the best among these \\(\\binom{p}{k}\\) models, and call it \\(\\mathcal{M}_k\\). Here best is defined as having the smallest RSS, or equivalently the largest \\(R^2\\).\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#example---credit-data-set",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#example---credit-data-set",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example - Credit data set",
    "text": "Example - Credit data set\n\n\n\n\n\n\n\n\n\n\n\nFor each possible model containing a subset of the ten predictors in the Credit data set, the Residual Sum of Squares (RSS) and \\(R^2\\) are displayed. The red frontier tracks the best model for a given number of predictors, according to RSS and \\(R^2\\).\nThough the data set contains only ten predictors, the x-axis ranges from 1 to 11, since one of the variables is categorical and takes on three values, leading to the creation of two dummy variables.\nThe reason that there’s a lot of dots in this picture is because there’s a lot of possible sub models given 10 total predictors. We have \\(2^{p} = 2^{10}\\approx 1,000\\) subsets.\nThe number \\(2^p\\) arises because each predictor (out of \\(p\\) predictors) can either be included or excluded from a subset model. This binary decision for each predictor gives \\(2\\) choices (include or exclude). When there are \\(p\\) predictors, the total number of possible subsets (or models) is calculated as \\(2^p\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#extensions-to-other-models",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#extensions-to-other-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Extensions to other models",
    "text": "Extensions to other models\n\n\n\nThe same ideas apply to other types of models, such as logistic regression.\nWhen dealing with other type of models, instead of the RSS, we look into the deviance (D), which is commonly used in generalized linear models. The deviance is calculated as:\n\n\\[\nD = -2 \\cdot \\log L_{\\text{max}}\n\\]\nwhere:\n\n\\(D\\) is the deviance,\n\\(\\log L_{\\text{max}}\\) is the maximized log-likelihood of the model.\n\nThis formula allows the deviance to serve as a measure of goodness of fit, analogous to the residual sum of squares (RSS) in linear regression, but applicable to a broader class of models."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\n\nFor computational reasons, best subset selection cannot be applied with very large \\(p\\). Why not?\nBest subset selection may also suffer from statistical problems when \\(p\\) is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data.\nThus an enormous search space can lead to overfitting and high variance of the coefficient estimates.\nFor both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection",
    "text": "Forward Stepwise Selection\n\nForward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.\nIn particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#in-detail",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "In Detail",
    "text": "In Detail\nForward Stepwise Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors.\nFor \\(k = 0, \\ldots, p - 1\\):\n\n2.1 Consider all \\(p - k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor.\n2.2 Choose the best among these \\(p - k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#more-on-forward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#more-on-forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Forward Stepwise Selection",
    "text": "More on Forward Stepwise Selection\n\nComputational advantage over best subset selection is clear.\nIt is not guaranteed to find the best possible model out of all \\(2^p\\) models containing subsets of the \\(p\\) predictors. Why not? Give an example."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\nThe first four selected models for best subset selection and forward stepwise selection on the Credit data set.\n\n\n\n\n\n\n\n\n# Variables\nBest subset\nForward stepwise\n\n\n\n\nOne\nrating\nrating\n\n\nTwo\nrating, income\nrating, income\n\n\nThree\nrating, income, student\nrating, income, student\n\n\nFour\ncards, income, student, limit\nrating, income, student, limit\n\n\n\nThe first three models are identical but the fourth models differ.\nThis discrepancy happens because there is correlation between features."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.\nHowever, unlike forward stepwise selection, it begins with the full least squares model containing all \\(p\\) predictors, and then iteratively removes the least useful predictor, one-at-a-time."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection: details",
    "text": "Backward Stepwise Selection: details\n\nLet \\(\\mathcal{M}_p\\) denote the full model, which contains all \\(p\\) predictors.\nFor \\(k = p, p - 1, \\ldots, 1\\):\n\n2.1 Consider all \\(k\\) models that contain all but one of the predictors in \\(\\mathcal{M}_k\\), for a total of \\(k - 1\\) predictors.\n2.2 Choose the best among these \\(k\\) models, and call it \\(\\mathcal{M}_{k-1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#more-on-backward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#more-on-backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More on Backward Stepwise Selection",
    "text": "More on Backward Stepwise Selection\n\nLike forward stepwise selection, the backward selection approach searches through only \\(1 + p(p+1)/2\\) models, and so can be applied in settings where \\(p\\) is too large to apply best subset selection.\nLike forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the \\(p\\) predictors.\nBackward selection requires that the number of samples \\(n\\) is larger than the number of variables \\(p\\) (so that the full model can be fit). In contrast, forward stepwise can be used even when \\(n &lt; p\\), and so is the only viable subset method when \\(p\\) is very large."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Optimal Model",
    "text": "Choosing the Optimal Model\n\nThe model containing all of the predictors will always have the smallest RSS and the largest \\(R^2\\), since these quantities are related to the training error.\nWe wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.\nTherefore, RSS and \\(R^2\\) are not suitable for selecting the best model among a collection of models with different numbers of predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#estimating-test-error-two-approaches",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#estimating-test-error-two-approaches",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Estimating test error: two approaches",
    "text": "Estimating test error: two approaches\n\nIndirect: We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\nDirect: We can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in previous lectures."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#c_p-aic-bic-and-adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)",
    "text": "\\(C_p\\), AIC, BIC, and Adjusted \\(R^2\\)\nThese techniques adjust the training error for the model size, and can be used to select among a set of models with different numbers of variables.\n\nThe figure displays \\(C_p\\), BIC, and adjusted \\(R^2\\) for the best model of each size produced by best subset selection on the Credit data set.\n\nIt suggests that we must choose a model with 4 to 6 predictors.\nThe main recommmendation is to keep the model as simple as possible. By identifying that the values do not change too much as we increase the number of predictors, a model with 4 predictors will be recommended."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\n\n\n\n\n\n\nThe validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set.\nThe cross-validation errors were computed using \\(k = 10\\) folds. In this case, the validation and cross-validation methods both result in a six-variable model.\nHowever, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.\nIn this setting, we can select a model using the one-standard-error rule.\n\nEstimate Test Error: We compute the test error (e.g., MSE) for each model size.\nCalculate Standard Error: Compute the standard error (SE) of the test error for each model size to account for variability.\nSelect the Model: Identify the model with the lowest test error (the “best” model). Choose the simplest model whose test error is within one SE of the lowest test error."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#now-for-some-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#now-for-some-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Now for some details",
    "text": "Now for some details\n\nMallow’s \\(C_p\\): \\[\nC_p = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2),\n\\]\n\nwhere \\(d\\) is the total # of parameters used and \\(\\hat{\\sigma}^2\\) is an estimate of the variance of the error \\(\\epsilon\\) associated with each response measurement.\n\nThe AIC criterion is defined for a large class of models fit by maximum likelihood:\n\n\\[\n  \\text{AIC} = -2 \\log L + 2 \\cdot d,\n\\]\nwhere \\(L\\) is the maximized value of the likelihood function for the estimated model.\n\nIn the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and \\(C_p\\) and AIC are equivalent. Prove this."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-on-bic",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-on-bic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details on BIC",
    "text": "Details on BIC\n\\[\n\\text{BIC} = \\frac{1}{n} \\left( \\text{RSS} + \\log(n)d\\hat{\\sigma}^2 \\right).\n\\]\n\nLike \\(C_p\\), the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.\nNotice that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by \\(C_p\\) with a \\(\\log(n)d\\hat{\\sigma}^2\\) term, where \\(n\\) is the number of observations.\nSince \\(\\log n &gt; 2\\) for any \\(n &gt; 7\\), the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than \\(C_p\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#adjusted-r2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#adjusted-r2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\nFor a least squares model with \\(d\\) variables, the adjusted \\(R^2\\) statistic is calculated as\n\\[\n    \\text{Adjusted } R^2 = 1 - \\frac{\\text{RSS}/(n - d - 1)}{\\text{TSS}/(n - 1)}.\n\\]\nwhere TSS is the total sum of squares, \\(TSS = \\Sigma_i^n(y_i - \\bar{y})^2\\).\n\nUnlike \\(C_p\\), AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted \\(R^2\\) indicates a model with a small test error.\nMaximizing the adjusted \\(R^2\\) is equivalent to minimizing \\(\\frac{\\text{RSS}}{n - d - 1}\\). While RSS always decreases as the number of variables in the model increases, \\(\\frac{\\text{RSS}}{n - d - 1}\\) may increase or decrease, due to the presence of \\(d\\) in the denominator.\nUnlike the \\(R^2\\) statistic, the adjusted \\(R^2\\) statistic pays a price for the inclusion of unnecessary variables in the model."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation and Cross-Validation",
    "text": "Validation and Cross-Validation\n\nEach of the procedures returns a sequence of models \\(\\mathcal{M}_k\\) indexed by model size \\(k = 0, 1, 2, \\ldots\\). Our job here is to select \\(\\hat{k}\\). Once selected, we will return model \\(\\mathcal{M}_{\\hat{k}}\\).\nWe compute the validation set error or the cross-validation error for each model \\(\\mathcal{M}_k\\) under consideration, and then select the \\(k\\) for which the resulting estimated test error is smallest.\nThis procedure has an advantage relative to AIC, BIC, \\(C_p\\), and adjusted \\(R^2\\), in that it provides a direct estimate of the test error, and doesn’t require an estimate of the error variance \\(\\sigma^2\\).\nIt can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g., the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\n\n\n\n\n\n\nIn the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\). As \\(\\lambda\\) increases, it pushes the coefficients towards zero.\nThe right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying \\(\\lambda\\) on the \\(x\\)-axis, we now display \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\), where \\(\\hat{\\beta}\\) denotes the vector of least squares coefficient estimates.\nThe notation \\(\\|\\beta\\|_2\\) denotes the \\(\\ell_2\\) norm (pronounced “ell 2”) of a vector, and is defined as \\(\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}\\).\nIn the right-hand panel, when \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2 = 1\\), \\(\\lambda = 0\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nIn the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\).\nThe right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying \\(\\lambda\\) on the \\(x\\)-axis, we now display \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\), where \\(\\hat{\\beta}\\) denotes the vector of least squares coefficient estimates.\nThe notation \\(\\|\\beta\\|_2\\) denotes the \\(\\ell_2\\) norm (pronounced “ell 2”) of a vector, and is defined as \\(\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\nRidge regression and Lasso\n\nThe subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\nAs an alternative, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\nIt may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRecall that the least squares fitting procedure estimates \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the values that minimize \\[\n\\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2.\n\\]\nIn contrast, the ridge regression coefficient estimates \\(\\hat{\\beta}^R\\) are the values that minimize \\[\n\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n= \\text{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\] where \\(\\lambda \\geq 0\\) is a tuning parameter, to be determined separately."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression: continued",
    "text": "Ridge regression: continued\n\nAs with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.\nHowever, the second term, \\(\\lambda \\sum_j \\beta_j^2\\), called a shrinkage penalty, is small when \\(\\beta_1, \\ldots, \\beta_p\\) are close to zero, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) towards zero.\nThe tuning parameter \\(\\lambda\\) serves to control the relative impact of these two terms on the regression coefficient estimates.\nSelecting a good value for \\(\\lambda\\) is critical; cross-validation is used for this."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-3",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\nLeft: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of \\(\\lambda\\). \\(\\lambda = 0.05\\) minimizes the cross-validation error.\nRight: The coefficient estimates as a function of \\(\\lambda\\). The vertical dashed line indicates the value of \\(\\lambda\\) selected by cross-validation."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-of-previous-figure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nIn the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of \\(\\lambda\\).\nThe right-hand panel displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying \\(\\lambda\\) on the \\(x\\)-axis, we now display \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\), where \\(\\hat{\\beta}\\) denotes the vector of least squares coefficient estimates.\nThe notation \\(\\|\\beta\\|_2\\) denotes the \\(\\ell_2\\) norm (pronounced “ell 2”) of a vector, and is defined as \\(\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-scaling-of-predictors",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-scaling-of-predictors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge Regression: Scaling of Predictors",
    "text": "Ridge Regression: Scaling of Predictors\n\nThe standard least squares coefficient estimates are scale equivariant: multiplying \\(X_j\\) by a constant \\(c\\) simply leads to a scaling of the least squares coefficient estimates by a factor of \\(1/c\\). In other words, regardless of how the \\(j\\)th predictor is scaled, \\(X_j \\hat{\\beta}_j\\) will remain the same.\nIn contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty part of the ridge regression objective function.\nTherefore, it is best to apply ridge regression after standardizing the predictors, using the formula\n\n\n\\[\n\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}}\n\\]\n\n\nAfter standardizing the predictors, their standard deviations will be 1. That make the features comparable and make the coefficients comparable."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#why-does-ridge-regression-improve-over-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why Does Ridge Regression Improve Over Least Squares?",
    "text": "Why Does Ridge Regression Improve Over Least Squares?\nThe Bias-Variance Tradeoff\n\n\nSimulated data with \\(n = 50\\) observations, \\(p = 45\\) predictors, all having nonzero coefficients.\nSquared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of \\(\\lambda\\) and \\(\\|\\hat{\\beta}_\\lambda^R\\|_2 / \\|\\hat{\\beta}\\|_2\\).\nThe horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\nRidge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all \\(p\\) predictors in the final model.\nThe Lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat{\\beta}^L_\\lambda\\), minimize the quantity\n\n\\[\n  \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]\n\nIn statistical parlance, the lasso uses an \\(\\ell_1\\) (pronounced “ell 1”) penalty instead of an \\(\\ell_2\\) penalty. The \\(\\ell_1\\) norm of a coefficient vector \\(\\beta\\) is given by \\(\\|\\beta\\|_1 = \\sum |\\beta_j|\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso: Continued",
    "text": "The Lasso: Continued\n\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero.\nHowever, in the case of the lasso, the \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nHence, much like best subset selection, the lasso performs variable selection.\nWe say that the lasso yields sparse models — that is, models that involve only a subset of the variables.\nAs in ridge regression, selecting a good value of \\(\\lambda\\) for the lasso is critical; cross-validation is again the method of choice."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#example-credit-dataset",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#example-credit-dataset",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Credit Dataset",
    "text": "Example: Credit Dataset"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-variable-selection-property-of-the-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-variable-selection-property-of-the-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Variable Selection Property of the Lasso",
    "text": "The Variable Selection Property of the Lasso\nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero?\nOne can show that the lasso and ridge regression coefficient estimates solve the problems (equivalent to Lagrange formulations):\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} |\\beta_j| \\leq s\n\\]\nand\n\\[\n\\text{minimize}_{\\beta} \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^{p} \\beta_j^2 \\leq s,\n\\]\nrespectively."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-picture",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-picture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso Picture",
    "text": "The Lasso Picture"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression",
    "text": "Comparing the Lasso and Ridge Regression\n\nSimulated data with \\(n = 50\\) observations, \\(p = 45\\) predictors, all having nonzero coefficients.\nLeft: Lasso: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on simulated data set.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#comparing-the-lasso-and-ridge-regression-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparing the Lasso and Ridge Regression: continued",
    "text": "Comparing the Lasso and Ridge Regression: continued\n\nLeft: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data equals to the one used before, except that now only two predictors are related to the response.\nRight: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dashed). Both are plotted against their \\(R^2\\) on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#conclusions",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#conclusions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusions",
    "text": "Conclusions\n\nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.\nIn general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.\nHowever, the number of predictors that is related to the response is never known a priori for real data sets.\nA technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Selecting the Tuning Parameter for Ridge Regression and Lasso",
    "text": "Selecting the Tuning Parameter for Ridge Regression and Lasso\n\nAs for subset selection, for ridge regression and lasso we require a method to determine which of the models under consideration is best.\nThat is, we require a method selecting a value for the tuning parameter \\(\\lambda\\) or equivalently, the value of the constraint \\(s\\).\nCross-validation provides a simple way to tackle this problem. We choose a grid of \\(\\lambda\\) values, and compute the cross-validation error rate for each value of \\(\\lambda\\).\nWe then select the tuning parameter value for which the cross-validation error is smallest.\nFinally, the model is re-fit using all of the available observations and the selected value of the tuning parameter."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-4",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#credit-data-example-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Credit data example",
    "text": "Credit data example\n\n\n\n\n\nCross-validation errors for ridge regression\n\n\nLeft: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of \\(\\lambda\\).\n\n\n\n\nCoefficient estimates as function of lambda\n\n\nRight: The coefficient estimates as a function of \\(\\lambda\\). The vertical dashed line indicates the value of \\(\\lambda\\) selected by cross-validation."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#simulated-data-example",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#simulated-data-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simulated Data Example",
    "text": "Simulated Data Example\n\nLeft: Ten-fold cross-validation MSE for the lasso, applied to the sparse simulated data set.\nRight: The corresponding lasso coeﬃcient estimates are displayed. The vertical dashed lines indicate the lasso fit for which the cross-validation error is smallest."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nThe methods that we have discussed so far in this chapter have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1, X_2, \\ldots, X_p\\).\nWe now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods: Details",
    "text": "Dimension Reduction Methods: Details\n\n\n\nLet \\(Z_1, Z_2, \\ldots, Z_M\\) represent \\(M &lt; p\\) linear combinations of our original \\(p\\) predictors. That is,\n\n\\[\n    Z_m = \\sum_{j=1}^p \\phi_{mj} X_j \\quad \\text{(1)}\n\\]\nfor some constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\).\n\nWe can then fit the linear regression model,\n\n\\[\n    y_i = \\theta_0 + \\sum_{m=1}^M \\theta_m z_{im} + \\epsilon_i, \\quad i = 1, \\ldots, n, \\quad \\text{(2)}\n\\]\nusing ordinary least squares.\n\nNote that in model (2), the regression coefficients are given by \\(\\theta_0, \\theta_1, \\ldots, \\theta_M\\). If the constants \\(\\phi_{m1}, \\ldots, \\phi_{mp}\\) are chosen wisely, then such dimension reduction approaches can often outperform OLS regression."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods: Continued",
    "text": "Dimension Reduction Methods: Continued\n\nNotice that from definition (1),\n\n\\[\n    \\sum_{m=1}^M \\theta_m z_{im} = \\sum_{m=1}^M \\theta_m \\sum_{j=1}^p \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\sum_{m=1}^M \\theta_m \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\beta_j x_{ij},\n\\] where\n\\[\n    \\beta_j = \\sum_{m=1}^M \\theta_m \\phi_{mj}. \\quad \\text{(3)}\n\\]\n\nHence model (2) can be thought of as a special case of the original linear regression model.\nDimension reduction serves to constrain the estimated \\(\\beta_j\\) coefficients, since now they must take the form (3).\nCan win in the bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nHere we apply principal components analysis (PCA) (discussed in Chapter 10 of the text) to define the linear combinations of the predictors, for use in our regression.\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has the largest variance, subject to being uncorrelated with the first.\nAnd so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\nThe population size (pop) and ad spending (ad) for 100 different cities are shown as purple circles. The green solid line indicates the first principal component, and the blue dashed line indicates the second principal component.\nNote that these two principal components are uncorrelated!"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\nA subset of the advertising data. Left: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments. Right: The left-hand panel has been rotated so that the first principal component lies on the x-axis."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\nPlots of the first principal component scores \\(z_{i1}\\) versus pop and ad. The relationships are strong."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA: continued",
    "text": "Pictures of PCA: continued\n\nPlots of the second principal component scores \\(z_{i2}\\) versus pop and ad. The relationships are weak."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#application-to-principal-components-regression",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#application-to-principal-components-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Application to Principal Components Regression",
    "text": "Application to Principal Components Regression\n\nPCR was applied to two simulated data sets. The black, green, and purple lines correspond to squared bias, variance, and test mean squared error, respectively.\n\nLeft: Simulated data with \\(n= 50\\) observations, \\(p= 45\\) predictors. The plot shows that a model with \\(\\approx 18\\) principal components can provide a good result.\nRight: Simulated data with \\(n= 50\\) observations, \\(p= 45\\) predictors, except that now only two predictors are related to the response. The plot shows that a model with \\(\\approx 25\\) principal components can provide a good result."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-directions-m",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-directions-m",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Number of Directions \\(M\\)",
    "text": "Choosing the Number of Directions \\(M\\)\n\nLeft: PCR standardized coefficient estimates on the Credit data set for different values of \\(M\\).\nRight: The 10-fold cross-validation MSE obtained using PCR, as a function of \\(M\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares",
    "text": "Partial Least Squares\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1, \\dots, X_p\\).\nThese directions are identified in an unsupervised way, since the response \\(Y\\) is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-continued",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares: Continued",
    "text": "Partial Least Squares: Continued\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1, \\dots, Z_M\\) that are linear combinations of the original features, and then fits a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way – that is, it makes use of the response \\(Y\\) in order to identify new features that not only approximate the old features well, but also that are related to the response.\nRoughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#details-of-partial-least-squares",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#details-of-partial-least-squares",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Partial Least Squares",
    "text": "Details of Partial Least Squares\n\nAfter standardizing the \\(p\\) predictors, PLS computes the first direction \\(Z_1\\) by setting each \\(\\phi_{1j}\\) in (1) equal to the coefficient from the simple linear regression of \\(Y\\) onto \\(X_j\\).\nOne can show that this coefficient is proportional to the correlation between \\(Y\\) and \\(X_j\\).\nHence, in computing \\(Z_1 = \\sum_{j=1}^p \\phi_{1j} X_j\\), PLS places the highest weight on the variables that are most strongly related to the response.\nSubsequent directions are found by taking residuals and then repeating the above prescription."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nEstimating test error for models involves adjusting training error or using direct methods like cross-validation. Tools like CP, AIC, BIC, and adjusted R-squared help select optimal models.\nHighlights\n\nEstimating test error is crucial for model selection.\nTwo approaches: indirect adjustment of training error and direct estimation methods.\nCP, AIC, BIC, and adjusted R-squared help model comparison.\nA model should minimize CP and BIC while maximizing adjusted R-squared.\nAdjusted R-squared allows meaningful comparisons across different models.\nCross-validation is versatile and can be applied to various models.\nSimplicity is favored; fewer predictors often yield better results.\n\nKey Insights\n\nTest Error Estimation: Accurate test error estimation is vital for model evaluation. It helps choose the best model among multiple options, ensuring reliability in predictions.\nIndirect vs. Direct Methods: Understanding both indirect (adjusting training error) and direct (cross-validation) methods provides flexibility in model evaluation, catering to different scenarios in data analysis.\nModel Selection Criteria: CP, AIC, BIC, and adjusted R-squared serve as essential criteria for model selection. They help quantify model performance and complexity, aiding in decision-making.\nMinimizing CP and BIC: Aiming for lower CP and BIC values suggests a more parsimonious model, which is often preferred for its simplicity and interpretability while still capturing the necessary relationships.\nCross-Validation Versatility: Cross-validation is a powerful tool applicable to a wide range of models, including non-linear ones, making it a preferred method for estimating test error in various contexts.\nAdjusted R-squared Utility: Unlike traditional R-squared, adjusted R-squared provides a way to compare models with differing numbers of predictors, addressing the limitations of model evaluation in regression analysis.\nSimplicity Preference: Favoring simpler models with fewer predictors can lead to better generalization and reduced risk of overfitting, aligning with the principle of Occam’s Razor in statistical modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nShrinkage methods like Ridge regression and Lasso use penalties to shrink coefficients towards zero, improving model performance, especially with large datasets.\nHighlights\n\nRidge regression uses a penalty to shrink coefficients towards zero.\nLasso also shrinks coefficients but can set some to exactly zero.\nThese methods are effective for large datasets with many variables.\nFast computation has revived interest in these techniques recently.\nCross-validation is crucial for selecting the optimal tuning parameter, Lambda.\nScaling of variables is important when applying Ridge regression.\nRidge regression reduces variance while maintaining bias, leading to better mean squared error.\n\nKey Insights\n\nShrinkage Techniques: Ridge regression and Lasso are modern approaches to regularization, balancing model fit and complexity. Shrinking coefficients helps mitigate overfitting, particularly in high-dimensional data.\nTuning Parameter Lambda: The choice of Lambda is critical; it determines the strength of the penalty. Using cross-validation to optimize this parameter is essential for achieving the best model performance.\nBias-Variance Tradeoff: Ridge regression effectively controls variance without significantly increasing bias, thereby minimizing mean squared error. This tradeoff is vital for model accuracy.\nLarge Datasets: As datasets grow in size and complexity, shrinkage methods become increasingly relevant. They are designed to handle situations where the number of predictors can exceed the number of observations.\nImportance of Scaling: Unlike least squares, the performance of Ridge regression is sensitive to the scale of the predictors. Standardizing variables ensures comparability and effectiveness of the shrinkage.\nContinuous Shrinkage: Ridge regression produces coefficients that are close to zero but rarely exactly zero, which differs from Lasso. This characteristic can be advantageous for retaining all predictors in the model.\nCurrent Research Trends: Shrinkage methods are a hot topic in statistical research, with ongoing developments aimed at enhancing their effectiveness and applicability across various fields."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#linear-model-selection-and-regularization-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Model Selection and Regularization",
    "text": "Linear Model Selection and Regularization\nRecall the linear model\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon.\n\\]\n\nIn the lectures that follow, we consider some approaches for extending the linear model framework. We will generalize the linear model in order to accommodate non-linear, but still additive, relationships.\nIn the lectures covering Chapter 8, we consider even more general non-linear models."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#subset-selection-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Subset Selection",
    "text": "Subset Selection\nBest subset and stepwise model selection procedures\nBest Subset Selection\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\nFor \\(k = 1, 2, \\ldots, p\\):\n\n\nFit all \\(\\binom{p}{k}\\) models that contain exactly \\(k\\) predictors.\n\n\nPick the best among these \\(\\binom{p}{k}\\) models, and call it \\(\\mathcal{M}_k\\). Here best is defined as having the smallest RSS, or equivalently the largest \\(R^2\\).\n\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#best-subset-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#best-subset-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Best Subset Selection",
    "text": "Best Subset Selection\n\nThe core idea is to identify a simpler model that includes only a subset of the \\(P\\) available predictors, thereby improving interpretability and potentially enhancing predictive performance.\nTo implement best subset selection systematically, we consider every possible combination of predictors and evaluate each resulting model. The process begins with the null model (\\(M_0\\)), which includes no predictors and only an intercept, meaning it predicts the sample mean for all observations. From there, models are incrementally built by incorporating different subsets of predictors, ultimately selecting the model that optimally balances predictive accuracy and complexity. Here are the steps:\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.\nFor \\(k = 1, 2, \\ldots, p\\):\n\nFit all \\(\\binom{p}{k}\\) models, “\\(p\\) choose \\(k\\) models”, that contain exactly \\(k\\) predictors. \\(\\binom{p}{k} = \\frac{p!}{k!(p-k)!}\\)\nPick the best among these \\(\\binom{p}{k}\\) models, and call it \\(\\mathcal{M}_k\\). Here best is defined as having the smallest Residual Sum of Squares (RSS), or equivalently the largest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\). The goal is to choose the model with the smallest test error, not the smallest training error."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#stepwise-selection-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Stepwise Selection",
    "text": "Stepwise Selection\n\nFor computational reasons, best subset selection cannot be applied with very large \\(p\\).\nBest subset selection may also suffer from statistical problems when \\(p\\) is large: larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data.\nThus an enormous search space can lead to overfitting and high variance of the coefficient estimates. For the authors of the book, it is not recommended to use the best subset approach if you have more than 20 predictors.\nFor both of these reasons, stepwise methods, which explore a far more restricted set of models (\\(p^2\\)), are attractive alternatives to best subset selection."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection-in-detail",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection: In Detail",
    "text": "Forward Stepwise Selection: In Detail\n\n\n\nLet \\(\\mathcal{M}_0\\) denote the null model, which contains no predictors.\nFor \\(k = 0, \\ldots, p - 1\\):\n\n2.1 Consider all \\(p - k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor. This is different from what we were doing in in the best subset selection case. Here do not look at every possible model containing \\(p\\) predictors. Instead, we are just looking at every possible model that contains one more predictor than \\(M_{k-1}\\).\n2.2 Choose the best among these \\(p - k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). Here best is defined as having smallest RSS or highest \\(R^2\\).\n\nSelect a single best model from among \\(\\mathcal{M}_0, \\ldots, \\mathcal{M}_p\\) using cross-validated prediction error, \\(C_p\\) (AIC), BIC, or adjusted \\(R^2\\).\n\n\nForward Stepwise Selection presents computational advantage over best subset selection. The total number of models evaluated is the sum of the sequence:\n\n\\[\np + (p - 1) + (p - 2) + \\dots + 1 = \\frac{p(p + 1)}{2}\n\\]\n\nFor large \\(p\\), the term \\(\\frac{p(p + 1)}{2}\\) is dominated by \\(\\frac{p^2}{2}\\). Thus, the computational cost is approximately proportional to \\(p^2\\).\nIt is not guaranteed to find the best possible model out of all \\(2^p\\) models containing subsets of the \\(p\\) predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection-summary",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#forward-stepwise-selection-summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Forward Stepwise Selection: Summary",
    "text": "Forward Stepwise Selection: Summary\n\n\nStepwise selection is a computationally efficient alternative to best subset selection in model building, especially with large predictor sets.\nHighlights\n\nStepwise selection offers a practical approach for model selection.\nBest subset selection can lead to overfitting, especially with many predictors.\nForward stepwise selection considers fewer models than best subset, making it computationally efficient.\nDeviance generalizes residual sum of squares across various models.\nBest subset selection becomes impractical beyond 30-40 predictors due to computational limits.\nForward stepwise may not always find the optimal model compared to best subset.\nCorrelation between features impacts model selection outcomes between methods.\n\nKey Insights\n\nComputational Efficiency: Stepwise selection significantly reduces the number of models evaluated, making it feasible for larger datasets. This is essential in modern data analysis, where predictors can number in the thousands.\nOverfitting Risks: With best subset selection, the risk of overfitting increases as the number of predictors grows, which can lead to poor performance on unseen data. This highlights the importance of model validation techniques.\nModel Nesting: Forward stepwise selection builds models incrementally, ensuring that each new model is a superset of the previous one, which helps maintain a streamlined search process for the best predictors.\nDeviance vs. RSS: Understanding the difference in metrics like deviance and residual sum of squares is crucial for accurately assessing model fit across various types of regression analyses.\nPractical Limits: Most statistical packages struggle with subset selection beyond 30-40 predictors, indicating the need for streamlined methods like stepwise selection in high-dimensional contexts.\nModel Comparison: Forward stepwise selection may yield different models than best subset selection, emphasizing the need for careful evaluation of model performance on independent datasets.\nCorrelation Effects: The discrepancies between the two methods arise from correlations among predictors, showcasing the intricate dynamics of variable selection in regression modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection",
    "text": "Backward Stepwise Selection\n\nLike forward stepwise selection, backward stepwise selection provides an efficient alternative to best subset selection.\nHowever, unlike forward stepwise selection, it begins with the full least squares model containing all \\(p\\) predictors, and then iteratively removes the least useful predictor, one-at-a-time."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-summary",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#backward-stepwise-selection-summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backward Stepwise Selection: Summary",
    "text": "Backward Stepwise Selection: Summary\n\n\nBackward stepwise selection removes predictors from a full model to improve efficiency in model selection, contrasting with forward stepwise selection.\nHighlights\n\nBackward stepwise starts with a full model (mp) and removes predictors one at a time.\nIt evaluates the least useful predictor to minimize impact on model fit.\nThis method is computationally efficient, considering around p²/2 models.\nOnly applicable when the number of observations (n) is greater than the number of predictors.\nR-squared and RSS might mislead model selection, focusing on training error rather than test error.\nCross-validation, AIC, BIC, or adjusted R-squared should guide the final model choice.\nBackward and forward selections are not guaranteed to find the best model but can yield good test set results.\n\nKey Insights\n\nMethodology Contrast: Backward stepwise selection is an efficient alternative to forward selection, emphasizing the removal of predictors rather than their addition. This reversal highlights different strategies in model optimization.\nModel Evaluation: The approach assesses the least impactful predictors, ensuring that model performance remains stable as predictors are eliminated, which is crucial for maintaining predictive accuracy.\nComputational Efficiency: Backward stepwise selection dramatically reduces computational load compared to best subset selection, making it a suitable option for larger datasets.\nObservational Requirement: This method necessitates that the number of observations is greater than the number of predictors, ensuring that a least squares model can be appropriately fitted, which is a critical consideration in practical applications.\nTraining vs. Test Error: Relying solely on training error metrics like RSS and R-squared can lead to overfitting, indicating the need for broader evaluation methods to predict future performance.\nModel Selection Techniques: Utilizing techniques like cross-validation, AIC, or BIC for model selection can help mitigate the risks associated with simply opting for models with the best training error.\nOutcome Consistency: While backward stepwise may not find the absolute best model, it can produce models that perform well on unseen data, demonstrating its practical utility in predictive modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-optimal-model-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Optimal Model",
    "text": "Choosing the Optimal Model\n\nThe model containing all of the predictors will always have the smallest RSS and the largest \\(R^2\\), since these quantities are related to the training error.\nWe wish to choose a model with low test error, not a model with low training error. Recall that training error is usually a poor estimate of test error.\nTherefore, RSS and \\(R^2\\) are not suitable for selecting the best model among a collection of models with different numbers of predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#mallows-c_p",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#mallows-c_p",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Mallows’ \\(C_p\\)",
    "text": "Mallows’ \\(C_p\\)\n\n\nMallows’ \\(C_p\\) balances model fit and model complexity:\n\\[\nC_p = \\frac{1}{n} \\left( \\text{RSS} + 2d\\hat{\\sigma}^2 \\right)\n\\]\nwhere:\n\n\\(d\\): Total number of parameters used in the model (including the intercept).\n\n\\(\\hat{\\sigma}^2\\): Estimate of the error variance \\(\\epsilon\\), associated with each response measurement.\n\n\\(\\text{RSS}\\): Residual Sum of Squares, measuring the error between observed and predicted values.\n\n\\(n\\): Number of observations in the dataset.\n\nExplanation\n\nThe \\(\\text{RSS}\\) measures the fit.\nThe penalty term \\(2d\\hat{\\sigma}^2\\) discourages overfitting.\n\nDecision: The lowest, the better!"
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#aic",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#aic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AIC",
    "text": "AIC\nThe Akaike Information Criteria (AIC) criterion is defined for a large class of models fit by maximum likelihood:\n\\[\n  \\text{AIC} = -2 \\log L + 2 \\cdot d,\n\\]\nwhere \\(L\\) is the maximized value of the likelihood function for the estimated model.\n\nIn the case of the linear model, \\(-2 \\log L = \\frac{RSS}{\\sigma^2}\\)\nIn the case of the linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and \\(C_p\\) and AIC are equivalent.\nAIC and Mallow’s \\(C_p\\) are proportional to each other.\nAIC is a good approach for non-linear models, e.g. logistic regression."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#bic",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#bic",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BIC",
    "text": "BIC\nThis is the Bayesian Information Criterion (BIC):\n\\[\n\\text{BIC} = \\frac{1}{n} \\left( \\text{RSS} + \\log(n)d\\hat{\\sigma}^2 \\right).\n\\]\n\nLike \\(C_p\\), the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value.\nNotice that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by \\(C_p\\) with a \\(\\log(n)d\\hat{\\sigma}^2\\) term, where \\(n\\) is the number of observations.\nSince \\(\\log n &gt; 2\\) for any \\(n &gt; 7\\), the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than \\(C_p\\) or AIC."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#mallows-c_p-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#mallows-c_p-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Mallow’s \\(C_p\\):",
    "text": "Mallow’s \\(C_p\\):\n\\[\n    C_p = \\frac{1}{n} (\\text{RSS} + 2d\\hat{\\sigma}^2),\n\\]\nwhere \\(d\\) is the total # of parameters used and \\(\\hat{\\sigma}^2\\) is an estimate of the variance of the error \\(\\epsilon\\) associated with each response measurement."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#validation-and-cross-validation-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Validation and Cross-Validation",
    "text": "Validation and Cross-Validation\n\nEach of the procedures returns a sequence of models \\(\\mathcal{M}_k\\) indexed by model size \\(k = 0, 1, 2, \\ldots\\). Our job here is to select \\(\\hat{k}\\). Once selected, we will return model \\(\\mathcal{M}_{\\hat{k}}\\).\nWe compute the validation set error or the cross-validation error for each model \\(\\mathcal{M}_k\\) under consideration, and then select the \\(k\\) for which the resulting estimated test error is smallest.\nThis procedure has an advantage relative to AIC, BIC, \\(C_p\\), and adjusted \\(R^2\\), in that it provides a direct estimate of the test error, and doesn’t require an estimate of the error variance \\(\\sigma^2\\).\nIt can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g., the number of predictors in the model) or hard to estimate the error variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nKey Concepts\n\nThree classes of methods:\n\nSubset Selection: Focuses on identifying subsets of predictors for simpler models.\nShrinkage Methods: Regularizes coefficients to reduce variance and improve model performance (e.g., Ridge, Lasso).\nDimension Reduction: Reduces the number of predictors using linear combinations (e.g., PCA, PLS).\n\nModel Selection Criteria:\n\nUse metrics like $C_p $, AIC, BIC, and Adjusted $R^2 $to balance model fit and complexity.\nCross-validation is essential for estimating test error and selecting tuning parameters.\n\nBias-Variance Tradeoff:\n\nShrinkage methods improve prediction accuracy by reducing variance while maintaining bias.\n\n\n\nPractical Insights\n\nBest Subset Selection:\n\nComputationally intensive (\\(2^p\\) models).\nNot recommended for \\(p &gt; 20\\).\n\nStepwise Selection:\n\nMore efficient (\\(p^2\\) models).\nForward and backward methods balance performance and computation.\n\nRidge vs. Lasso:\n\nRidge shrinks coefficients but includes all predictors.\nLasso performs variable selection by setting coefficients to zero.\n\nPrincipal Components Regression (PCR):\n\nReduces dimensionality by finding uncorrelated components.\nWorks well when high variance directions correlate with the response.\n\nPartial Least Squares (PLS):\n\nSupervised alternative to PCR, incorporating response variable information."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#shrinkage-methods-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Shrinkage Methods",
    "text": "Shrinkage Methods\n\nThe subset selection methods use least squares to fit a linear model that contains a subset of the predictors.\nAs an alternative, we can fit a model containing all \\(p\\) predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero.\nIt may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nRecall that the least squares fitting procedure estimates \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the values that minimize\n\\[\n    \\text{RSS} = \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2.\n\\]\n\nIn contrast, the ridge regression coefficient estimates \\(\\hat{\\beta}^R\\) are the values that minimize\n\n\\[\n\\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n    = \\text{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\]\nwhere \\(\\lambda \\geq 0\\) is a tuning parameter, to be determined separately.\n\nShrinkage penalty: \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\) penalizes coefficients that get too large. The model will pay a price according to the number of non-zero coefficients."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-4",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nSelecting the tuning parameter (lambda) for ridge regression and lasso is crucial, as it significantly influences model performance. Cross-validation is an effective method for this selection.\nHighlights\n\nLambda is crucial: Affects the solution from full least squares to zero coefficients. ⚖️ Regularization importance: Zero lambda means no regularization; high lambda leads to zero solutions.\nCross-validation advantage: Ideal for tuning parameters as it doesn’t require the unknown number of parameters (d).\nRidge example: With lambda of 100, all variables appear included, but coefficients are shrunk.\nCross-validation curves: Show how errors change with varying lambda values for both ridge and lasso.\nLasso effectiveness: Properly identifies non-zero coefficients while setting others to zero.\nSimulation success: In a simulated scenario, the model accurately identifies the correct number of non-zero coefficients.\n\nKey Insights\n\nImportance of Lambda: The tuning parameter lambda significantly influences the model’s complexity and overall performance. Choosing lambda wisely is essential for achieving the desired balance between bias and variance.\nCross-validation as a solution: Cross-validation provides a robust framework for assessing model performance across different lambda values without needing to know the exact number of parameters, making it a practical choice for tuning.\nDegree of freedom confusion: In ridge regression, even when coefficients are shrunk, counting parameters can be misleading, as all variables remain included in the model. ⚖️ Regularization trade-offs: The process of regularization through ridge and lasso not only simplifies models but also introduces nuanced definitions of model complexity, changing our understanding of ‘degrees of freedom.’\nError analysis via curves: Cross-validation curves reveal how model errors fluctuate with lambda, helping visualize optimal tuning points.\nLasso’s precision: Lasso regression demonstrates its strength in feature selection, effectively pinpointing relevant variables while ignoring the irrelevant ones, enhancing interpretability."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#ridge-regression-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge regression",
    "text": "Ridge regression\n\nAs with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small.\nHowever, the second term, \\(\\lambda \\sum_j \\beta_j^2\\), called a shrinkage penalty, is small when \\(\\beta_1, \\ldots, \\beta_p\\) are close to zero, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) towards zero.\nThe tuning parameter \\(\\lambda\\) serves to control the relative impact of these two terms on the regression coefficient estimates.\nSelecting a good value for \\(\\lambda\\) is critical; cross-validation is used for this."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\n\n\nRidge regression does have one obvious disadvantage: unlike subset selection, which will generally select models that involve just a subset of the variables, ridge regression will include all \\(p\\) predictors in the final model.\nThe Lasso, first published in 1996 by Rob Tibshirani, one of the authors of the book, is an alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, \\(\\hat{\\beta}^L_\\lambda\\), minimize the quantity\n\n\\[\n  \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|.\n\\]\n\nIn statistical parlance, the lasso uses the sum of absolute values, an \\(\\ell_1\\) (pronounced “ell 1”) penalty, instead of an \\(\\ell_2\\) penalty. The \\(\\ell_1\\) norm of a coefficient vector \\(\\beta\\) is given by \\(\\|\\beta\\|_1 = \\sum |\\beta_j|\\)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso",
    "text": "The Lasso\n\nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero.\nHowever, in the case of the lasso, the \\(\\ell_1\\) penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large.\nHence, much like best subset selection, the lasso performs variable selection. It is a combination of shirinkage and selection of variables.\nWe say that the lasso yields sparse models — that is, models that involve only a subset of the variables.\nAs in ridge regression, selecting a good value of \\(\\lambda\\) for the lasso is critical; cross-validation is again the method of choice."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-and-ridge-picture",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#the-lasso-and-ridge-picture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Lasso and Ridge Picture",
    "text": "The Lasso and Ridge Picture\nThis picture helps to explain why the lasso gives sparsity:\n\n\nOn the right we have the ridge regression and on the left is the lasso regression.\nIt is possible to see where the where the red boundary touch the blue constraing.\nIn the case of the ridge regression (right plot), we see that the solution does not create a zero.\nIn the case of the lasso regression (left plot), we see that the solution does create a zero for one of the predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-3",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nThe Lasso regression technique improves upon ridge regression by both shrinking coefficients and performing variable selection, setting some coefficients to zero.\nHighlights\n\nLasso regression shrinks coefficients while allowing for variable selection.\nIt uses an L1 penalty, contrasting with ridge’s L2 penalty.\nThe concept of sparsity is central to Lasso’s effectiveness.\nIncreased computational efficiency has popularized Lasso in recent years.\nLasso is particularly useful in high-dimensional datasets with many features.\nCross-validation is essential for selecting the optimal lambda value.\nPerformance varies: Lasso excels in sparse models, while ridge may perform better in dense ones.\n\nKey Insights\n\nLasso vs. Ridge: Lasso regression not only shrinks coefficients but also sets some to zero, enabling simpler models through variable selection. This property makes it particularly valuable in high-dimensional settings where many variables may be irrelevant.\nL1 vs. L2 Penalty: The L1 penalty used in Lasso creates a constraint that promotes sparsity, while the L2 penalty in ridge regression tends to retain all variables with smaller coefficients. This difference is crucial for effective model building.\nSparsity: The concept of sparsity refers to models that only include a small subset of variables. Sparse models are easier to interpret and can enhance predictive performance when only a few predictors are relevant.\nComputational Advances: Recent improvements in computational power and techniques in convex optimization have made applying Lasso feasible even on large datasets, broadening its applicability across various fields.\nReal-World Applications: In situations like medical diagnostics, where finding a minimal number of significant predictors is vital, Lasso provides a practical solution by efficiently identifying key variables among thousands of measurements.\nChoosing Lambda: The tuning parameter lambda is critical; cross-validation is typically used to determine its optimal value, balancing model complexity and predictive accuracy.\nModel Performance: The effectiveness of Lasso and ridge regression varies based on the underlying data structure. Lasso performs better with sparse true models, while ridge regression may be more effective when many predictors are significant."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-5",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nDimension reduction transforms original predictors into fewer linear combinations, improving model fitting while maintaining low bias and variance.\nHighlights\n\nDimension Reduction simplifies models by using fewer predictors.\nNew predictors are linear combinations of original ones.\nFitting uses least squares on transformed predictors.\nAim: Reduce dimensions from P predictors to M (M &lt; P).\nBalances bias and variance effectively.\nSimilar to Ridge and Lasso, but with different coefficient constraints.\nWorks best when M &lt; P; otherwise, it results in standard least squares.\n\nKey Insights\n\nEfficiency in Modeling: Dimension reduction allows for a simpler model with fewer predictors, leading to potentially better performance without losing significant information. This method is advantageous in high-dimensional datasets.\nConstruction of New Predictors: By creating new predictors through linear combinations, we can capture essential relationships in the data while reducing complexity, which may help in enhancing interpretability.\nBias-Variance Trade-off: This approach effectively manages the bias-variance trade-off, leading to models with lower bias and variance compared to using all original features, which is crucial for better generalization to unseen data.\nUse of Least Squares: While retaining the least squares fitting method, this approach modifies the predictor space, allowing for a fresh perspective on regression problems and leading to potentially improved outcomes.\nRelation to Ridge and Lasso: Although dimension reduction shares similarities with Ridge and Lasso in terms of model fitting, it introduces unique constraints on coefficients, which can lead to different insights about the data.\nImportance of Dimensions: The effectiveness of dimension reduction hinges on the condition that M (new predictors) is less than P (original predictors). If M equals P, the method reduces to standard least squares, negating its advantages.\nInnovation in Coefficient Form: The requirement for coefficients to adopt a specific structure in dimension reduction can provide insights into the relationships among predictors, enhancing model interpretability and utility."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#conclusions-about-ridge-and-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#conclusions-about-ridge-and-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusions about Ridge and Lasso",
    "text": "Conclusions about Ridge and Lasso\n\nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other.\nIn general, one might expect the lasso to perform better when the response is a function of only a relatively small number of predictors.\nHowever, the number of predictors that is related to the response is never known a priori for real data sets.\nA technique such as cross-validation can be used in order to determine which approach is better on a particular data set."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#selecting-the-tuning-parameter-for-ridge-regression-and-lasso-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Selecting the Tuning Parameter for Ridge Regression and Lasso",
    "text": "Selecting the Tuning Parameter for Ridge Regression and Lasso\n\nAs for subset selection, for ridge regression and lasso we require a method to determine which of the models under consideration is best.\nThat is, we require a method selecting a value for the tuning parameter \\(\\lambda\\) or equivalently, the value of the constraint \\(s\\).\nCross-validation provides a simple way to tackle this problem. We choose a grid of \\(\\lambda\\) values, and compute the cross-validation error rate for each value of \\(\\lambda\\).\nWe then select the tuning parameter value for which the cross-validation error is smallest.\nFinally, the model is re-fit using all of the available observations and the selected value of the tuning parameter."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\nThe methods that we have discussed so far have involved fitting linear regression models, via least squares or a shrunken approach, using the original predictors, \\(X_1, X_2, \\ldots, X_p\\).\nWe now explore a class of approaches that transform the predictors and then fit a least squares model using the transformed variables. We will refer to these techniques as dimension reduction methods."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-6",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\nPrincipal Components Regression (PCR) reduces dimensionality by finding principal components and using them in least squares regression for efficient modeling.\nHighlights\n\nPrincipal Components Regression (PCR) uses a two-step procedure to reduce dimensionality.\nFirst, principal components with the highest variance are identified from the data.\nThe first principal component is aligned with the direction of maximum variance.\nThe second principal component is uncorrelated with the first and captures additional variance.\nUsing few principal components can effectively summarize complex datasets.\nChoosing the optimal number of components is crucial for minimizing mean squared error.\nPartial Least Squares (PLS) improves upon PCR by considering response variables in component selection.\n\nKey Insights\n\nDimensionality Reduction: PCR simplifies models by reducing the number of predictors while retaining essential information, aiding in interpretation and computation. This is particularly useful with datasets containing many variables relative to observations.\nUncorrelated Components: The process ensures that the principal components are uncorrelated, which helps in creating more robust models by minimizing multicollinearity issues common in regression analysis.\nModel Selection: The selection of the number of components directly impacts model performance. Cross-validation is recommended to find the optimal number of components for the best predictive accuracy.\nEfficiency in Prediction: PCR can significantly enhance prediction accuracy when dealing with high-dimensional data by focusing on variance rather than individual variable contributions.\nAssumption of Variance-Response Relationship: The effectiveness of PCR hinges on the assumption that high variance directions in predictors correlate with the response, which may not always hold true.\nPartial Least Squares: PLS offers a supervised alternative to PCR by incorporating response variable information, potentially leading to better predictive models, although it may not always outperform PCR.\nModern Applications: Techniques like PCR and PLS are increasingly relevant in fields with large datasets, where simpler models are needed to prevent overfitting and enhance interpretability."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-8",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-8",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nKey Concepts\n\nThree classes of methods:\n\nSubset Selection: Focuses on identifying subsets of predictors for simpler models.\nShrinkage Methods: Regularizes coefficients to reduce variance and improve model performance (e.g., Ridge, Lasso).\nDimension Reduction: Reduces the number of predictors using linear combinations (e.g., PCA, PLS).\n\nModel Selection Criteria:\n\nUse metrics like $C_p $, AIC, BIC, and Adjusted $R^2 $to balance model fit and complexity.\nCross-validation is essential for estimating test error and selecting tuning parameters.\n\nBias-Variance Tradeoff:\n\nShrinkage methods improve prediction accuracy by reducing variance while maintaining bias.\n\n\n\n\n\nPractical Insights\n\nBest Subset Selection:\n\nComputationally intensive ($2^p $models).\nNot recommended for $p &gt; 20 $.\n\nStepwise Selection:\n\nMore efficient ($p^2 $models).\nForward and backward methods balance performance and computation.\n\nRidge vs. Lasso:\n\nRidge shrinks coefficients but includes all predictors.\nLasso performs variable selection by setting coefficients to zero.\n\nPrincipal Components Regression (PCR):\n\nReduces dimensionality by finding uncorrelated components.\nWorks well when high variance directions correlate with the response.\n\nPartial Least Squares (PLS):\n\nSupervised alternative to PCR, incorporating response variable information."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#principal-components-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Regression",
    "text": "Principal Components Regression\n\nBy far the most famous dimension reduction approach. It involves a two-step procedure:\n\nStep 1: we find what are called principal components of the data matrix (linear combinations of the predictors)\nStep 2: we perform least squares regression using those principal components as predictors\n\nThe first principal component is that (normalized) linear combination of the variables with the largest variance.\nThe second principal component has the largest variance, subject to being uncorrelated with the first.\nAnd so on.\nHence with many correlated original variables, we replace them with a small set of principal components that capture their joint variation.\nThe intuition is that if you have a data set with 45 variables and compute a few principal components, those might capture most of the variation in the data."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\n\n\n\n\n\n\n\n\n\n\nA subset of the advertising data.\n\nLeft: The first principal component, chosen to minimize the sum of the squared perpendicular distances to each point, is shown in green. These distances are represented using the black dashed line segments.\nRight: The left-hand panel has been rotated so that the first principal component lies on the x-axis."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\n\nPlots of the first principal component scores \\(z_{i1}\\) versus pop and ad. The relationships are strong.\n\nWe can visualize each principal component by plotting it against the original variables, such as population and ad spending.\nWe observe that the first principal component is highly correlated with both population and ad spending. This indicates that the first principal component effectively captures the variability in these two variables, summarizing the data in a meaningful way.\nThis suggests a valuable insight: instead of using the original variables (population and ad spending) directly, we can use the first principal component as a single, simplified predictor. We have the assumption that a linear combination of the predictors that has high variance is probably going to be associated with the response.\n\nFor example, if my goal is to predict a response variable like sales, we can incorporate the first principal component as a predictor in the model. This approach reduces the dimensionality of the data while retaining much of the information, potentially improving model interpretability and efficiency."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-3",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#pictures-of-pca-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pictures of PCA",
    "text": "Pictures of PCA\n\nPlots of the second principal component scores \\(z_{i2}\\) versus pop and ad. The relationships are weak."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-principal-component-directions-m",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#choosing-the-number-of-principal-component-directions-m",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Number of Principal Component Directions \\(M\\)",
    "text": "Choosing the Number of Principal Component Directions \\(M\\)\n\nLeft: PCR standardized coefficient estimates on the Credit data set for different values of \\(M\\).\nRight: The 10-fold cross-validation MSE obtained using PCR, as a function of \\(M\\). For each of the models we can see the cross-validated mean squared error. Here we have disappointing result. If we pick a model for which the mean squared error is as small as possible, here the mean squared error is really as small as possible when we have a model with 10 or 11 components. However, in our dataset \\(M = 11\\) is going to be the regular least squares on the original data using all variables. Basically, principal components regression does not provide any gains in this case."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-1",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares (PLS)",
    "text": "Partial Least Squares (PLS)\n\nPCR identifies linear combinations, or directions, that best represent the predictors \\(X_1, \\dots, X_p\\).\nThese directions are identified in an unsupervised way, since the response \\(Y\\) is not used to help determine the principal component directions.\nThat is, the response does not supervise the identification of the principal components.\nConsequently, PCR suffers from a potentially serious drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.\nA potential solution is to use Partial Least Squares (PLS)."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares (PLS)",
    "text": "Partial Least Squares (PLS)\n\nLike PCR, PLS is a dimension reduction method, which first identifies a new set of features \\(Z_1, \\dots, Z_M\\) that are linear combinations of the original features, and then fits a linear model via OLS using these \\(M\\) new features.\nBut unlike PCR, PLS identifies these new features in a supervised way – that is, it makes use of the response \\(Y\\) in order to identify new features that not only approximate the old features well, but also that are related to the response.\nPLS approach attempts to find directions that help explain both the response and the predictors."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-details",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#partial-least-squares-pls-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Partial Least Squares (PLS): Details",
    "text": "Partial Least Squares (PLS): Details\n\nAfter standardizing the \\(p\\) predictors, PLS computes the first direction \\(Z_1\\) by setting each \\(\\phi_{1j}\\) in (1) equal to the coefficient from the simple linear regression of \\(Y\\) onto \\(X_j\\).\nOne can show that this coefficient is proportional to the correlation between \\(Y\\) and \\(X_j\\).\nHence, in computing \\(Z_1 = \\sum_{j=1}^p \\phi_{1j} X_j\\), PLS places the highest weight on the variables that are most strongly related to the response.\nSubsequent directions are found by taking residuals and then repeating the above prescription.\nThe authors of the book highlight that PLS does not bring to much gain when compared to Ridge regression approach, for example."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-forward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-forward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Forward Stepwise Selection",
    "text": "Summary: Forward Stepwise Selection\n\n\nStepwise selection is a computationally efficient alternative to best subset selection in model building, especially with large predictor sets.\n\n\nHighlights\n\nStepwise selection offers a practical approach for model selection.\nBest subset selection can lead to overfitting, especially with many predictors.\nForward stepwise selection considers fewer models than best subset, making it computationally efficient.\nDeviance generalizes residual sum of squares across various models.\nBest subset selection becomes impractical beyond 30-40 predictors due to computational limits.\nForward stepwise may not always find the optimal model compared to best subset.\nCorrelation between features impacts model selection outcomes between methods.\n\n\nKey Insights\n\nComputational Efficiency: Stepwise selection significantly reduces the number of models evaluated, making it feasible for larger datasets. This is essential in modern data analysis, where predictors can number in the thousands.\nOverfitting Risks: With best subset selection, the risk of overfitting increases as the number of predictors grows, which can lead to poor performance on unseen data. This highlights the importance of model validation techniques.\nModel Nesting: Forward stepwise selection builds models incrementally, ensuring that each new model is a superset of the previous one, which helps maintain a streamlined search process for the best predictors.\nDeviance vs. RSS: Understanding the difference in metrics like deviance and residual sum of squares is crucial for accurately assessing model fit across various types of regression analyses.\nPractical Limits: Most statistical packages struggle with subset selection beyond 30-40 predictors, indicating the need for streamlined methods like stepwise selection in high-dimensional contexts.\nModel Comparison: Forward stepwise selection may yield different models than best subset selection, emphasizing the need for careful evaluation of model performance on independent datasets.\nCorrelation Effects: The discrepancies between the two methods arise from correlations among predictors, showcasing the intricate dynamics of variable selection in regression modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-backward-stepwise-selection",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-backward-stepwise-selection",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Backward Stepwise Selection",
    "text": "Summary: Backward Stepwise Selection\n\n\nBackward stepwise selection removes predictors from a full model to improve efficiency in model selection, contrasting with forward stepwise selection.\n\n\nHighlights\n\nBackward stepwise starts with a full model (\\(M_p\\)) and removes predictors one at a time.\nIt evaluates the least useful predictor to minimize impact on model fit.\nThis method is computationally efficient, considering around \\(p^2/2\\)models.\nOnly applicable when the number of observations (\\(n\\)) is greater than the number of predictors.\n\\(R^2\\)and RSS might mislead model selection, focusing on training error rather than test error.\nCross-validation, AIC, BIC, or adjusted \\(R^2\\)should guide the final model choice.\nBackward and forward selections are not guaranteed to find the best model but can yield good test set results.\n\n\nKey Insights\n\nMethodology Contrast: Backward stepwise selection is an efficient alternative to forward selection, emphasizing the removal of predictors rather than their addition. This reversal highlights different strategies in model optimization.\nModel Evaluation: The approach assesses the least impactful predictors, ensuring that model performance remains stable as predictors are eliminated, which is crucial for maintaining predictive accuracy.\nComputational Efficiency: Backward stepwise selection dramatically reduces computational load compared to best subset selection, making it a suitable option for larger datasets.\nObservational Requirement: This method necessitates that the number of observations is greater than the number of predictors, ensuring that a least squares model can be appropriately fitted, which is a critical consideration in practical applications.\nTraining vs. Test Error: Relying solely on training error metrics like RSS and \\(R^2\\)can lead to overfitting, indicating the need for broader evaluation methods to predict future performance.\nModel Selection Techniques: Utilizing techniques like cross-validation, AIC, or BIC for model selection can help mitigate the risks associated with simply opting for models with the best training error.\nOutcome Consistency: While backward stepwise may not find the absolute best model, it can produce models that perform well on unseen data, demonstrating its practical utility in predictive modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-c_p-aic-bic-and-adjusted-r-squared",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-c_p-aic-bic-and-adjusted-r-squared",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: \\(C_p\\), AIC, BIC, and adjusted R-squared",
    "text": "Summary: \\(C_p\\), AIC, BIC, and adjusted R-squared\n\n\nEstimating test error for models involves adjusting training error or using direct methods like cross-validation. Tools like CP, AIC, BIC, and adjusted R-squared help select optimal models.\n\n\nHighlights\n\nEstimating test error is crucial for model selection.\nTwo approaches: indirect adjustment of training error and direct estimation methods.\n\\(C_p\\), AIC, BIC, and adjusted \\(R^2\\) help model comparison.\nA model should minimize \\(C_p\\) and BIC while maximizing adjusted \\(R^2\\).\nAdjusted \\(R^2\\) allows meaningful comparisons across different models.\nCross-validation is versatile and can be applied to various models.\nSimplicity is favored; fewer predictors often yield better results.\n\n\nKey Insights\n\nTest Error Estimation: Accurate test error estimation is vital for model evaluation. It helps choose the best model among multiple options, ensuring reliability in predictions.\nIndirect vs. Direct Methods: Understanding both indirect (adjusting training error) and direct (cross-validation) methods provides flexibility in model evaluation, catering to different scenarios in data analysis.\nModel Selection Criteria: \\(C_p\\), AIC, BIC, and adjusted \\(R^2\\) serve as essential criteria for model selection. They help quantify model performance and complexity, aiding in decision-making.\nMinimizing \\(C_p\\) and BIC: Aiming for lower \\(C_p\\) and BIC values suggests a more parsimonious model, which is often preferred for its simplicity and interpretability while still capturing the necessary relationships.\nCross-Validation Versatility: Cross-validation is a powerful tool applicable to a wide range of models, including non-linear ones, making it a preferred method for estimating test error in various contexts.\nAdjusted \\(R^2\\) Utility: Unlike traditional \\(R^2\\), adjusted \\(R^2\\) provides a way to compare models with differing numbers of predictors, addressing the limitations of model evaluation in regression analysis.\nSimplicity Preference: Favoring simpler models with fewer predictors can lead to better generalization and reduced risk of overfitting, aligning with the principle of Occam’s Razor in statistical modeling."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-validation-and-cross-validation",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-validation-and-cross-validation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Validation and Cross-validation",
    "text": "Summary: Validation and Cross-validation\n\n\nValidation and cross-validation help select the best model size by estimating prediction error without needing sigma squared or the number of parameters.\n\n\nHighlights\n\nValidation splits data into training and validation sets for error estimation.\nCross-validation trains on multiple parts of data to improve error estimates.\nChoosing the optimal model size minimizes validation error effectively.\nAvoiding sigma squared estimation is crucial in high-dimensional data scenarios.\nThe one standard error rule favors simpler models that perform similarly to the best.\nBIC tends to prefer smaller models compared to AIC in error estimation.\nNew data challenges continuously evolve statistical methods and research.\n\n\nKey Insights\n\nModel Selection: Validation and cross-validation provide direct methods for estimating prediction error, making them essential for model selection. This ensures the chosen model performs well on unseen data.\nError Estimation: By dividing data into training and validation sets, we can effectively estimate how well a model will generalize, leading to more robust predictions in practice.\nAvoiding Estimation Challenges: In high-dimensional settings, traditional methods for estimating sigma squared and the number of parameters (\\(d\\)) can be unreliable. Cross-validation mitigates these concerns, simplifying the model selection process.\nSimplicity Preference: The one standard error rule encourages selecting simpler models that perform nearly as well as the best, enhancing interpretability and reducing overfitting.\nIterative Evaluation: Cross-validation’s iterative nature allows for more reliable error estimates by using multiple data partitions, thus improving the stability of model evaluations.\nBIC vs. AIC: BIC’s stronger penalty for model complexity often results in smaller models compared to AIC, which can lead to different model selection outcomes.\nEvolving Challenges: The increasing complexity of data in fields like high-dimensional statistics presents ongoing challenges, propelling research and innovation in statistical methodologies."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-shrinkage-methods",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-shrinkage-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Shrinkage methods",
    "text": "Summary: Shrinkage methods\n\n\nShrinkage methods like Ridge regression and Lasso use penalties to shrink coefficients towards zero, improving model performance, especially with large datasets.\n\n\nHighlights\n\nRidge regression uses a penalty to shrink coefficients towards zero.\nLasso also shrinks coefficients but can set some to exactly zero.\nThese methods are effective for large datasets with many variables.\nFast computation has revived interest in these techniques recently.\nCross-validation is crucial for selecting the optimal tuning parameter, Lambda.\nScaling of variables is important when applying Ridge regression.\nRidge regression reduces variance while maintaining bias, leading to better mean squared error.\n\n\nKey Insights\n\nShrinkage Techniques: Ridge regression and Lasso are modern approaches to regularization, balancing model fit and complexity. Shrinking coefficients helps mitigate overfitting, particularly in high-dimensional data.\nTuning Parameter Lambda: The choice of Lambda is critical; it determines the strength of the penalty. Using cross-validation to optimize this parameter is essential for achieving the best model performance.\nBias-Variance Tradeoff: Ridge regression effectively controls variance without significantly increasing bias, thereby minimizing mean squared error. This tradeoff is vital for model accuracy.\nLarge Datasets: As datasets grow in size and complexity, shrinkage methods become increasingly relevant. They are designed to handle situations where the number of predictors can exceed the number of observations.\nImportance of Scaling: Unlike least squares, the performance of Ridge regression is sensitive to the scale of the predictors. Standardizing variables ensures comparability and effectiveness of the shrinkage.\nContinuous Shrinkage: Ridge regression produces coefficients that are close to zero but rarely exactly zero, which differs from Lasso. This characteristic can be advantageous for retaining all predictors in the model.\nCurrent Research Trends: Shrinkage methods are a hot topic in statistical research, with ongoing developments aimed at enhancing their effectiveness and applicability across various fields."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-lasso",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-lasso",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Lasso",
    "text": "Summary: Lasso\n\n\nThe Lasso regression technique improves upon ridge regression by both shrinking coefficients and performing variable selection, setting some coefficients to zero.\n\n\nHighlights\n\nLasso regression shrinks coefficients while allowing for variable selection.\nIt uses an \\(L_1\\) penalty, contrasting with ridge’s \\(L_2\\) penalty.\nThe concept of sparsity is central to Lasso’s effectiveness.\nIncreased computational efficiency has popularized Lasso in recent years.\nLasso is particularly useful in high-dimensional datasets with many features.\nCross-validation is essential for selecting the optimal lambda value.\nPerformance varies: Lasso excels in sparse models, while ridge may perform better in dense ones.\n\n\nKey Insights\n\nLasso vs. Ridge: Lasso regression not only shrinks coefficients but also sets some to zero, enabling simpler models through variable selection. This property makes it particularly valuable in high-dimensional settings where many variables may be irrelevant.\n\\(L_1\\) vs. \\(L_2\\) Penalty: The \\(L_1\\) penalty used in Lasso creates a constraint that promotes sparsity, while the \\(L_2\\) penalty in ridge regression tends to retain all variables with smaller coefficients. This difference is crucial for effective model building.\nSparsity: The concept of sparsity refers to models that only include a small subset of variables. Sparse models are easier to interpret and can enhance predictive performance when only a few predictors are relevant.\nComputational Advances: Recent improvements in computational power and techniques in convex optimization have made applying Lasso feasible even on large datasets, broadening its applicability across various fields.\nReal-World Applications: In situations like medical diagnostics, where finding a minimal number of significant predictors is vital, Lasso provides a practical solution by efficiently identifying key variables among thousands of measurements.\nChoosing Lambda: The tuning parameter lambda is critical; cross-validation is typically used to determine its optimal value, balancing model complexity and predictive accuracy.\nModel Performance: The effectiveness of Lasso and ridge regression varies based on the underlying data structure. Lasso performs better with sparse true models, while ridge regression may be more effective when many predictors are significant."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-selecting-the-tuning-parameter-lambda",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-selecting-the-tuning-parameter-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Selecting the tuning parameter (lambda)",
    "text": "Summary: Selecting the tuning parameter (lambda)\n\n\nSelecting the tuning parameter (lambda) for ridge regression and lasso is crucial, as it significantly influences model performance. Cross-validation is an effective method for this selection.\n\n\nHighlights\n\nLambda is crucial: Affects the solution from full least squares to zero coefficients.\n⚖️ Regularization importance: Zero lambda means no regularization; high lambda leads to zero solutions.\nCross-validation advantage: Ideal for tuning parameters as it doesn’t require the unknown number of parameters (\\(d\\)).\nRidge example: With lambda of 100, all variables appear included, but coefficients are shrunk.\nCross-validation curves: Show how errors change with varying lambda values for both ridge and lasso.\nLasso effectiveness: Properly identifies non-zero coefficients while setting others to zero.\nSimulation success: In a simulated scenario, the model accurately identifies the correct number of non-zero coefficients.\n\n\nKey Insights\n\nImportance of Lambda: The tuning parameter lambda significantly influences the model’s complexity and overall performance. Choosing lambda wisely is essential for achieving the desired balance between bias and variance.\nCross-validation as a solution: Cross-validation provides a robust framework for assessing model performance across different lambda values without needing to know the exact number of parameters, making it a practical choice for tuning.\nDegree of freedom confusion: In ridge regression, even when coefficients are shrunk, counting parameters can be misleading, as all variables remain included in the model.\nRegularization trade-offs: The process of regularization through ridge and lasso not only simplifies models but also introduces nuanced definitions of model complexity, changing our understanding of ‘degrees of freedom.’\nError analysis via curves: Cross-validation curves reveal how model errors fluctuate with lambda, helping visualize optimal tuning points.\nLasso’s precision: Lasso regression demonstrates its strength in feature selection, effectively pinpointing relevant variables while ignoring the irrelevant ones, enhancing interpretability."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-dimension-reduction",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-dimension-reduction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Dimension reduction",
    "text": "Summary: Dimension reduction\n\n\nDimension reduction transforms original predictors into fewer linear combinations, improving model fitting while maintaining low bias and variance.\n\n\nHighlights\n\nDimension Reduction simplifies models by using fewer predictors.\nNew predictors are linear combinations of original ones.\nFitting uses least squares on transformed predictors.\nAim: Reduce dimensions from \\(P\\) predictors to \\(M\\) (\\(M &lt; P\\)).\nBalances bias and variance effectively.\nSimilar to Ridge and Lasso, but with different coefficient constraints.\nWorks best when \\(M &lt; P\\); otherwise, it results in standard least squares.\n\n\nKey Insights\n\nEfficiency in Modeling: Dimension reduction allows for a simpler model with fewer predictors, leading to potentially better performance without losing significant information. This method is advantageous in high-dimensional datasets.\nConstruction of New Predictors: By creating new predictors through linear combinations, we can capture essential relationships in the data while reducing complexity, which may help in enhancing interpretability.\nBias-Variance Trade-off: This approach effectively manages the bias-variance trade-off, leading to models with lower bias and variance compared to using all original features, which is crucial for better generalization to unseen data.\nUse of Least Squares: While retaining the least squares fitting method, this approach modifies the predictor space, allowing for a fresh perspective on regression problems and leading to potentially improved outcomes.\nRelation to Ridge and Lasso: Although dimension reduction shares similarities with Ridge and Lasso in terms of model fitting, it introduces unique constraints on coefficients, which can lead to different insights about the data.\nImportance of Dimensions: The effectiveness of dimension reduction hinges on the condition that \\(M\\) (new predictors) is less than \\(P\\) (original predictors). If \\(M = P\\), the method reduces to standard least squares, negating its advantages.\nInnovation in Coefficient Form: The requirement for coefficients to adopt a specific structure in dimension reduction can provide insights into the relationships among predictors, enhancing model interpretability and utility."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#summary-principal-components-regression-pcr",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#summary-principal-components-regression-pcr",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: Principal Components Regression (PCR)",
    "text": "Summary: Principal Components Regression (PCR)\n\n\nPrincipal Components Regression (PCR) reduces dimensionality by finding principal components and using them in least squares regression for efficient modeling.\n\n\nHighlights\n\nPrincipal Components Regression (PCR) uses a two-step procedure to reduce dimensionality.\nFirst, principal components with the highest variance are identified from the data.\nThe first principal component is aligned with the direction of maximum variance.\nThe second principal component is uncorrelated with the first and captures additional variance.\nUsing few principal components can effectively summarize complex datasets.\nChoosing the optimal number of components is crucial for minimizing mean squared error.\nPartial Least Squares (PLS) improves upon PCR by considering response variables in component selection.\n\n\nKey Insights\n\nDimensionality Reduction: PCR simplifies models by reducing the number of predictors while retaining essential information, aiding in interpretation and computation. This is particularly useful with datasets containing many variables relative to observations.\nUncorrelated Components: The process ensures that the principal components are uncorrelated, which helps in creating more robust models by minimizing multicollinearity issues common in regression analysis.\nModel Selection: The selection of the number of components directly impacts model performance. Cross-validation is recommended to find the optimal number of components for the best predictive accuracy.\nEfficiency in Prediction: PCR can significantly enhance prediction accuracy when dealing with high-dimensional data by focusing on variance rather than individual variable contributions.\nAssumption of Variance-Response Relationship: The effectiveness of PCR hinges on the assumption that high variance directions in predictors correlate with the response, which may not always hold true.\nPartial Least Squares: PLS offers a supervised alternative to PCR by incorporating response variable information, potentially leading to better predictive models, although it may not always outperform PCR.\nModern Applications: Techniques like PCR and PLS are increasingly relevant in fields with large datasets, where simpler models are needed to prevent overfitting and enhance interpretability."
  },
  {
    "objectID": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-2",
    "href": "lecture_slides/06_model_selection/06_model_selection.html#dimension-reduction-methods-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dimension Reduction Methods",
    "text": "Dimension Reduction Methods\n\n\n\nNotice that from definition (1),\n\n\\[\n    \\sum_{m=1}^M \\theta_m z_{im} = \\sum_{m=1}^M \\theta_m \\sum_{j=1}^p \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\sum_{m=1}^M \\theta_m \\phi_{mj} x_{ij} = \\sum_{j=1}^p \\beta_j x_{ij},\n\\] where\n\\[\n    \\beta_j = \\sum_{m=1}^M \\theta_m \\phi_{mj}. \\quad \\text{(3)}\n\\]\n\nHence model (2) can be thought of as a special case of the original linear regression model.\nDimension reduction serves to constrain the estimated \\(\\beta_j\\) coefficients, since now they must take the form (3).\nCan win in the bias-variance tradeoff."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#overview",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nMoving Beyond Linearity\nPolynomial Regression\nStep Functions\nRegression Splines\n\n\n\nSmoothing Splines\nLocal Regression\nGeneralized Additive Models\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#moving-beyond-linearity",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#moving-beyond-linearity",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Moving Beyond Linearity",
    "text": "Moving Beyond Linearity\n\nThe truth is never linear!\nOr almost never!\nBut often the linearity assumption is good enough.\nWhen it’s not…\n\npolynomials,\n\nstep functions,\n\nsplines,\n\nlocal regression, and\n\ngeneralized additive models\n\noffer a lot of flexibility, without losing the ease and interpretability of linear models."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d + \\epsilon_i\n\\]\nDegree-4 Polynomial"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#details",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details",
    "text": "Details\n\nCreate new variables \\(X_1 = X, \\, X_2 = X^2\\), etc., and then treat as multiple linear regression.\nNot really interested in the coefficients; more interested in the fitted function values at any value \\(x_0\\):\n\\[\n\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 + \\hat{\\beta}_2 x_0^2 + \\hat{\\beta}_3 x_0^3 + \\hat{\\beta}_4 x_0^4.\n\\]\nSince \\(\\hat{f}(x_0)\\) is a linear function of the \\(\\hat{\\beta}_\\ell\\), can get a simple expression for pointwise-variances ([(x_0)]) at any value \\(x_0\\). In the figure, we have computed the fit and pointwise standard errors on a grid of values for \\(x_0\\). We show \\(\\hat{f}(x_0) \\pm 2 \\cdot \\text{se}[\\hat{f}(x_0)]\\).\nWe either fix the degree \\(d\\) at some reasonably low value or use cross-validation to choose \\(d\\)."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#details-continued",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#details-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details continued",
    "text": "Details continued\n\nLogistic regression follows naturally. For example, in the figure we model\n\\[\n\\text{Pr}(y_i &gt; 250 \\mid x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}{1 + \\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}.\n\\]\nTo get confidence intervals, compute upper and lower bounds on on the logit scale, and then invert to get on the probability scale.\nCan do separately on several variables—just stack the variables into one matrix, and separate out the pieces afterwards (see GAMs later).\nCaveat: Polynomials have notorious tail behavior — very bad for extrapolation.\nCan fit using \\(y \\sim \\text{poly}(x, \\text{degree} = 3)\\) in formula."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step Functions",
    "text": "Step Functions\nAnother way of creating transformations of a variable — cut the variable into distinct regions.\n\\[\nC_1(X) = I(X &lt; 35), \\quad C_2(X) = I(35 \\leq X &lt; 50), \\dots, C_3(X) = I(X \\geq 65)\n\\]\nPiecewise Constant"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-continued",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step functions continued",
    "text": "Step functions continued\n\nEasy to work with. Creates a series of dummy variables representing each group.\nUseful way of creating interactions that are easy to interpret. For example, interaction effect of \\(\\text{Year}\\) and \\(\\text{Age}\\):\n\\[\nI(\\text{Year} &lt; 2005) \\cdot \\text{Age}, \\quad I(\\text{Year} \\geq 2005) \\cdot \\text{Age}\n\\] would allow for different linear functions in each age category.\nIn R: I(year &lt; 2005) or cut(age, c(18, 25, 40, 65, 90)).\nChoice of cutpoints or knots can be problematic. For creating nonlinearities, smoother alternatives such as splines are available."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#piecewise-polynomials",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#piecewise-polynomials",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Piecewise Polynomials",
    "text": "Piecewise Polynomials\n\nInstead of a single polynomial in \\(X\\) over its whole domain, we can rather use different polynomials in regions defined by knots.\n\n\n\\[\n    y_i =\n    \\begin{cases}\n    \\beta_{01} + \\beta_{11}x_i + \\beta_{21}x_i^2 + \\beta_{31}x_i^3 + \\epsilon_i & \\text{if } x_i &lt; c; \\\\\n    \\beta_{02} + \\beta_{12}x_i + \\beta_{22}x_i^2 + \\beta_{32}x_i^3 + \\epsilon_i & \\text{if } x_i \\geq c.\n    \\end{cases}\n\\]\n\n\nBetter to add constraints to the polynomials, e.g., continuity.\nSplines have the “maximum” amount of continuity."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#splines-visualization",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Splines Visualization",
    "text": "Splines Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop-Left Panel: A third-degree polynomial is fitted to the data on the left side of the knot at \\(X = 50\\), and another (separate) third-degree polynomial is fitted on the right side. There is no continuity constraint imposed at the knot, meaning the two polynomials may not meet at the same function value at \\(X = 50\\).\nTop-Right Panel: Again, a third-degree polynomial is fitted on each side of \\(X = 50\\). However, in this case, the polynomials are forced to be continuous at the knot. In other words, they must share the same function value at \\(X = 50\\).\nBottom-Left Panel: As in the top-right panel, a third-degree polynomial is fitted on each side of \\(X = 50\\), but with an additional constraint that enforces continuity of the first and second derivatives at \\(X = 50\\). This ensures a smoother transition between the left and right segments of the piecewise function.\nBottom-Right Panel: A linear regression model is fitted on each side of \\(X = 50\\). The model is constrained to be continuous at the knot, so both linear segments meet at the same value at \\(X = 50\\)."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#linear-splines",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#linear-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Splines",
    "text": "Linear Splines\n\nThe predictor space is partitioned at a set of specified points called knots \\(\\{\\xi_k\\}\\). A linear spline is a piecewise linear polynomial that remains continuous at each knot.\nWe construct linear spline models by augmenting a linear predictor with piecewise components that activate past designated knot locations, yielding a flexible yet interpretable approach to modeling relationships. Specifically:\n\n\n\nBasis-Function Representation: The model is written as\n\n\n\\[\n   y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\cdots + \\beta_{K+1} b_{K+1}(x_i) + \\epsilon_i,\n\\]\nwhere each \\(b_k(\\cdot)\\) is a basis function.\nOne basis function, \\(b_1(x_i)\\), is simply \\(x_i\\). The others, \\(b_{k+1}(x_i) = (x_i - \\xi_k)_+\\), capture local deviations after each knot \\(\\xi_k\\).\nThe notation \\((\\cdot)_+\\) denotes the “positive part,” meaning \\(\\max\\{0, \\cdot\\}\\). Therefore, \\((x_i - \\xi_k)_+ = x_i - \\xi_k\\) if \\(x_i &gt; \\xi_k\\), and 0 otherwise.\n\n\nPiecewise Linear Behavior: Because \\(b_{k+1}(x_i)\\) only becomes non-zero when \\(x_i\\) exceeds the knot \\(\\xi_k\\), the spline behaves like a linear function with additional slopes kicking in after each knot.\n\n\nEssentially, below the smallest knot, the model is a simple linear function of \\(x_i\\). Once \\(x_i\\) passes a knot \\(\\xi_k\\), the corresponding term \\((x_i - \\xi_k)_+\\) begins to contribute, allowing the slope to change. This creates segments of potentially different slopes while maintaining continuity at the knots.\n\n\n\nContinuity at Knots: Despite having distinct linear segments, the spline remains continuous at each \\(\\xi_k\\). The continuity follows naturally from how \\((\\cdot)_+\\) is defined. At a knot, \\((x_i - \\xi_k)_+\\) transitions from 0 to a linear increase, ensuring no jumps in the fitted function.\nRelevance\n\n\nFlexibility: Linear splines allow for piecewise changes in slope rather than forcing a single global linear relationship. This can capture more nuanced relationships between predictors and responses.\nInterpretability: Each knot \\(\\xi_k\\) marks a point where the slope can adjust, making it straightforward to interpret how the effect of \\(x_i\\) differs below and above that knot.\nComparison to Polynomials: Unlike higher-order polynomials, splines can avoid the global distortion that arises from polynomial terms. A single outlier or a data pattern in one region does not overly influence the fit across the entire range of \\(x\\)."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#linear-splines-visualization",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#linear-splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Splines Visualization",
    "text": "Linear Splines Visualization\n\n\n\n\n\n\n\n\n\n\n\nTop plot: shows two linear fits over the domain \\(0 \\le x \\le 1\\). The blue line represents a single global linear function (extending as the dashed line beyond the knot at \\(x = 0.6\\)), whereas the orange line demonstrates how adding a spline basis function allows the slope to change precisely at \\(x = 0.6\\).\nBottom plot: displays the corresponding basis function \\(b(x) = (x - 0.6)_{+}\\), which is defined to be zero for \\(x \\le 0.6\\) and increases linearly for \\(x &gt; 0.6\\). Because \\(b(x)\\) starts at zero at the knot, it does not introduce a jump—thus ensuring continuity—but it permits the slope to differ on either side of \\(x = 0.6\\).\n\nBy including this basis function (and a coefficient for it) in a linear model, one can capture a “bend” or change in slope at the specified knot. More generally, introducing additional such functions at different knots yields a piecewise linear model that remains continuous but adapts its slope in each region."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#cubic-splines",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#cubic-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nA cubic spline with knots at \\(\\xi_k, \\, k = 1, \\dots, K\\) is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot.\nAgain we can represent this model with truncated power basis functions:\n\\[\ny_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + \\cdots + \\beta_{K+3} b_{K+3}(x_i) + \\epsilon_i,\n\\]\n\\(b_1(x_i) = x_i,\\)\n\\(b_2(x_i) = x_i^2,\\)\n\\(b_3(x_i) = x_i^3,\\)\n\\(b_{k+3}(x_i) = (x_i - \\xi_k)_+^3, \\quad k = 1, \\dots, K\\)\n\nwhere\n\\[\n(x_i - \\xi_k)_+^3 =\n\\begin{cases}\n(x_i - \\xi_k)^3 & \\text{if } x_i &gt; \\xi_k, \\\\\n0 & \\text{otherwise}.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#cubic-splines-visualization",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#cubic-splines-visualization",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Splines Visualization",
    "text": "Cubic Splines Visualization"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#natural-cubic-splines",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#natural-cubic-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Natural Cubic Splines",
    "text": "Natural Cubic Splines\nA natural cubic spline extrapolates linearly beyond the boundary knots. This adds \\(4 = 2 \\times 2\\) extra constraints, and allows us to put more internal knots for the same degrees of freedom as a regular cubic spline."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#fitting-splines-in-r",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#fitting-splines-in-r",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fitting Splines in R",
    "text": "Fitting Splines in R\nFitting splines in R is easy: bs(x, ...) for any degree splines, and ns(x, ...) for natural cubic splines, in package splines.\nNatural Cubic Spline"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#knot-placement",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#knot-placement",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Knot Placement",
    "text": "Knot Placement\n\n\nOne strategy is to decide \\(K\\), the number of knots, and then place them at appropriate quantiles of the observed \\(X\\).\nA cubic spline with \\(K\\) knots has \\(K + 4\\) parameters or degrees of freedom.\nA natural spline with \\(K\\) knots has \\(K\\) degrees of freedom.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure: Comparison of a degree-14 polynomial and a natural cubic spline, each with 15df."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines",
    "text": "Smoothing Splines\nSmoothing splines are similar to regression splines, but arise in a slightly different situation. Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty.\nConsider this criterion for fitting a smooth function \\(g(x)\\) to some data:\n\\[\n\\text{minimize}_{g \\in S} \\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int \\left( g''(t) \\right)^2 dt\n\\]\n\nThe first term is RSS, and tries to make \\(g(x)\\) match the data at each \\(x_i\\).\nThe second term is a roughness penalty and controls how wiggly \\(g(x)\\) is. It is modulated by the tuning parameter \\(\\lambda \\geq 0\\).\n\nThe smaller \\(\\lambda\\), the more wiggly the function, eventually interpolating \\(y_i\\) when \\(\\lambda = 0\\).\nAs \\(\\lambda \\to \\infty\\), the function \\(g(x)\\) becomes linear."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-continued",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines Continued",
    "text": "Smoothing Splines Continued\nThe solution is a natural cubic spline, with a knot at every unique value of \\(x_i\\). The roughness penalty still controls the roughness via \\(\\lambda\\).\nSome details:\n\nSmoothing splines avoid the knot-selection issue, leaving a single \\(\\lambda\\) to be chosen.\n\n\n\nThe vector of \\(n\\) fitted values can be written as \\(\\hat{g}_\\lambda = \\mathbf{S}_\\lambda \\mathbf{y}\\), where \\(\\mathbf{S}_\\lambda\\) is a \\(n \\times n\\) matrix (determined by the \\(x_i\\) and \\(\\lambda\\)).\nThe effective degrees of freedom are given by\n\n\n\\[\n  \\text{df}_\\lambda = \\sum_{i=1}^n \\{ \\mathbf{S}_\\lambda \\}_{ii}.\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-continued-choosing-lambda",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-continued-choosing-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines Continued — Choosing \\(\\lambda\\)",
    "text": "Smoothing Splines Continued — Choosing \\(\\lambda\\)\n\nWe can specify \\(\\text{df}\\) rather than \\(\\lambda\\)!\nThe leave-one-out (LOO) cross-validated error is given by\n\n\n\\[\n  \\text{RSS}_{\\text{cv}}(\\lambda) = \\sum_{i=1}^n \\left( y_i - \\hat{g}_\\lambda^{(-i)}(x_i) \\right)^2 = \\sum_{i=1}^n \\left[ \\frac{y_i - \\hat{g}_\\lambda(x_i)}{1 - \\{\\mathbf{S}_\\lambda\\}_{ii}} \\right]^2.\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-spline",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-spline",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Spline",
    "text": "Smoothing Spline"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#local-regression",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#local-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Local Regression",
    "text": "Local Regression\n\nWith a sliding weight function, we fit separate linear fits over the range of \\(X\\) by weighted least squares.\nSee text for more details, and loess() function in R."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#generalized-additive-models",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#generalized-additive-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\nGeneralized additive models allow us to extend the methods covered in this lecture to deal with multiple predictors.\nAllows for flexible nonlinearities in several variables, but retains the additive structure of linear models.\n\\[\ny_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\cdots + f_p(x_{ip}) + \\epsilon_i.\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#gam-details",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#gam-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "GAM Details",
    "text": "GAM Details\n\nCan fit a GAM simply using, e.g., natural splines:\nCoefficients are not that interesting; fitted functions are.\n\n\n\nCan mix terms — some linear, some nonlinear — and compare models with ANOVA.\nCan use smoothing splines or local regression as well:\n\nGAMs are additive, although low-order interactions can be included in a natural way using, e.g., bivariate smoothers."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#gams-for-classification",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#gams-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "GAMs for Classification",
    "text": "GAMs for Classification\n\\[\n\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + f_1(X_1) + f_2(X_2) + \\cdots + f_p(X_p).\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nWhile linear models are simple and often effective, real-world data frequently exhibit non-linear relationships, necessitating more flexible modeling approaches.\n\nNon-Linearity is Common: Most real-world data are not well-represented by simple linear trends, underscoring the importance of non-linear modeling techniques.\n\nPolynomial Regression: Extends linear models by including polynomial terms, enabling the capture of curved relationships. This method offers flexibility but may exhibit instability, especially at the tails, where standard error bands tend to widen due to sparse data.\n\nStep Functions: Provide an alternative approach by dividing continuous variables into discrete intervals, resulting in piecewise constant fits. This makes them particularly useful when natural breakpoints exist in the data.\n\nInteractions with Dummy Variables: Step functions are intuitive to work with and allow for straightforward inclusion of interaction effects through the creation of dummy variables.\n\nChallenges of Step Functions: The performance of step functions heavily depends on the selection of cut points. Poorly chosen boundaries can obscure important patterns or lead to overfitting, highlighting the need for domain expertise when applying this method.\n\nTakeaway: Both polynomial regression and step functions are valuable tools for modeling non-linear relationships. However, their effectiveness depends on thoughtful implementation, particularly in managing standard errors and selecting appropriate cut points."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#moving-beyond-linearity-1",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#moving-beyond-linearity-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Moving Beyond Linearity",
    "text": "Moving Beyond Linearity\n\nThe truth is never linear!\nOr almost never!\nBut often the linearity assumption is good enough.\nWhen it’s not…\n\npolynomials,\n\nstep functions,\n\nsplines,\n\nlocal regression, and\n\ngeneralized additive models\n\noffer a lot of flexibility, without losing the ease and interpretability of linear models."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression-wage-data",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression-wage-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression: Wage Data",
    "text": "Polynomial Regression: Wage Data\n\nPolynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. It provides a simple way to provide a non-linear fit to data.\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\dots + \\beta_d x_i^d + \\epsilon_i\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nLeft Plot: We fit a fourth-degree polynomial model to predict Wage using Age as the predictor. The data become sparse toward the higher end of Age, so there is relatively little information to guide the model’s fit in that region. As a result, the standard errors increase toward the tail—a phenomenon often referred to as “leverage.”\n\n\n\nRight Plot: We fit a fourth-degree polynomial model for a logistic regression. Similar to the left plot, the data diminish at the tail end of the predictor range, leaving fewer observations to inform the fit and leading to wider confidence intervals in that region."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression-details",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression: Details",
    "text": "Polynomial Regression: Details\n\n\nCreate new variables \\(X_1 = X, \\, X_2 = X^2\\), etc., and then treat as multiple linear regression.\nIt is linear in the coefficients, but it is a non linear function of \\(x\\).\nNot really interested in the coefficients; more interested in the fitted function values at any value \\(x_0\\):\n\n\n\\[\n    \\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 + \\hat{\\beta}_2 x_0^2 + \\hat{\\beta}_3 x_0^3 + \\hat{\\beta}_4 x_0^4.\n\\]\n\n\nSince \\(\\hat{f}(x_0)\\) is a linear function of the \\(\\hat{\\beta}_\\ell\\), can get a simple expression for pointwise-variances (\\(\\text{Var}[\\hat{f}(x_0)]\\)) at any value \\(x_0\\).\n\nIn the figure, we have computed the fit and pointwise standard errors on a grid of values for \\(x_0\\). We show \\(\\hat{f}(x_0) \\pm 2 \\cdot \\text{se}[\\hat{f}(x_0)]\\).\n\nHow to choose \\(d\\), the polinomial degree? We either fix the degree \\(d\\) at some reasonably low value or use cross-validation to choose \\(d\\)."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression-details-1",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#polynomial-regression-details-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Polynomial Regression: Details",
    "text": "Polynomial Regression: Details\n\n\nLogistic regression follows naturally. For example, in the figure we model\n\n\n\\[\n    \\text{Pr}(y_i &gt; 250 \\mid x_i) = \\frac{\\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}{1 + \\exp(\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\dots + \\beta_d x_i^d)}.\n\\]\n\n\nTo get confidence intervals, compute upper and lower bounds on on the logit scale, and then invert to get on the probability scale.\nCan do separately on several variables—just stack the variables into one matrix, and separate out the pieces afterwards (see GAMs later).\nCaveat: Polynomials have notorious tail behavior — very bad for extrapolation. So, it is not recommended to trust predictions near the end of the data."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-1",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step Functions",
    "text": "Step Functions\n\nStep functions cut the range of a variable into \\(K\\) distinct regions in order to produce a qualitative variable. This has the effect of fitting a piecewise constant function.\n\\[\nC_1(X) = I(X &lt; 35), \\quad C_2(X) = I(35 \\leq X &lt; 50), \\dots, C_3(X) = I(X \\geq 65)\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-2",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step Functions",
    "text": "Step Functions\n\nA step function model partitions the range of a predictor variable \\(X\\) into multiple intervals and creates a set of dummy (0/1) indicators—one for each interval. By fitting a standard linear model with these dummy variables, the resulting function is piecewise constant: within each interval, the fitted value remains the same.\n\nLocal (Step Functions): Because each interval in a step function acts independently, changes in one interval have minimal or no effect on the fitted values in other intervals. Consequently, step functions allow a locally controlled fit, where data in a specific region of \\(X\\) only affect the parameters corresponding to that interval.\nGlobal (Polynomials): In contrast, polynomial models rely on parameters that apply across the entire range of \\(X\\). Consequently, if you alter a single data point or a small set of points in one region, those changes can influence the fitted function everywhere else. This global dependence can lead to dramatic shifts in the estimated curve.\nSimplicity: Step functions are conceptually straightforward and easy to implement, requiring only the definition of intervals and fitting dummy variables.\nLocal Control: Their piecewise nature can be beneficial when the true relationship changes abruptly or when you want to minimize the effect of outliers in one region on the fit elsewhere.\nBut there are some Drawbacks:\n\nBlocky Appearance: The fitted function may appear abrupt or “blocky,” as it is piecewise constant rather than smooth.\n\nBoundary Sensitivity: Choosing the number and location of breakpoints is somewhat subjective and can significantly impact the model’s fit.\n\nOverall, step functions provide an intuitive, locally controlled alternative to global polynomial models. However, the abrupt transitions and the need to specify breakpoints can limit their practical appeal—particularly for applications where a smooth or continuous functional form is desired."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#regression-splines-1",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#regression-splines-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nRegression splines are an extension and more flexible than polynomials and step functions.\nThey involve dividing the range of \\(X\\) into \\(K\\) distinct regions. Within each region, a polynomial function is fit to the data.\nHowever, these polynomials are constrained so that they join smoothly at the region boundaries, or knots. Provided that the interval is divided into enough regions, this can produce an extremely flexible fit."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary-2",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nPolynomial Regression\n\nExpands a linear model by adding powers of the predictor(s).\n\nSimple to implement but can behave poorly at boundaries.\n\nStep Functions\n\nPartitions the predictor space into intervals; fits piecewise constant segments.\n\nHighly interpretable but can appear “blocky” and requires choosing cutpoints.\n\nRegression Splines\n\nUses piecewise polynomials, constrained to meet smoothly at knots.\n\nGreater flexibility than polynomials or step functions, with continuity at knots.\n\nNatural Cubic Splines\n\nEnforce additional constraints for boundary behavior.\n\nAllow more robust modeling without overfitting at the extremes.\n\n\n\n\n\n\nSmoothing Splines\n\nAutomates knot selection (a knot at each data point); regularizes via a penalty on curvature.\n\nBalances fidelity to data (\\(\\mathrm{RSS}\\)) and smoothness (\\(\\lambda\\)).\n\n\\(\\lambda\\) (or effective degrees of freedom) is chosen via cross-validation or by specifying df directly.\n\nLocal Regression\n\nFits a series of local linear models weighted by proximity to each target \\(x\\).\nUseful for highly variable, nonlinear relationships.\n\nGeneralized Additive Models (GAMs)\n\nAllows non-linear fits in multiple predictors yet maintains an additive structure.\n\nSupports smooth terms (splines, local regression) combined with linear terms.\n\n\nBottom Line\n\nBy moving beyond a strictly linear framework—using polynomials, splines, or GAMs—analysts can more accurately capture complex patterns while retaining interpretability and control over model complexity."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#lienar-splines-example",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#lienar-splines-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Lienar Splines: Example",
    "text": "Lienar Splines: Example\n\nTo illustrate a linear spline with one knot at \\(x = 50\\), suppose we have a response variable \\(y\\) (e.g., a person’s yearly wage) and a single predictor \\(x\\) (e.g., age):\n\n\n\nDefine the Basis Functions: We choose to place a single knot at \\(x = 50\\). According to the slide’s notation, the basis functions are:\n\n\n\\[\n   b_1(x_i) = x_i, \\quad\n   b_2(x_i) = (x_i - 50)_{+} \\,=\\,\n   \\begin{cases}\n     x_i - 50, & \\text{if } x_i &gt; 50,\\\\\n     0, & \\text{if } x_i \\le 50.\n   \\end{cases}\n\\]\n\n\nSpecify the Model: The corresponding linear spline model is:\n\n\n\\[\n   y_i = \\beta_0 + \\beta_1 \\, b_1(x_i) + \\beta_2 \\, b_2(x_i) + \\epsilon_i,\n\\]\n\n\nor more explicitly:\n\n\n\\[\n   y_i = \\beta_0 + \\beta_1 \\, x_i + \\beta_2 \\, (x_i - 50)_{+} + \\epsilon_i.\n\\]\n\n\n\n3.For \\(x_i \\le 50\\):\n\n\n\\((x_i - 50)_+ = 0\\). Hence, the model reduces to\n\n\n\\[\n     y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n\n\nwhich is a simple linear relationship with slope \\(\\beta_1\\) for ages up to 50.\n\n\n4.For \\(x_i &gt; 50\\):\n\n\n\\((x_i - 50)_+ = x_i - 50\\). Thus, the model becomes\n\n\n\\[\n\\begin{aligned}\ny_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - 50) + \\epsilon_i \\\\\n    &= [\\beta_0 - 50 \\beta_2] + (\\beta_1 + \\beta_2) x_i + \\epsilon_i\n\\end{aligned}\n\\]\n\n\nmeaning the slope for ages beyond 50 is \\(\\beta_1 + \\beta_2\\). The intercept adjusts accordingly to ensure the function remains continuous at \\(x=50\\).\n\n\n\n\nLocal Flexibility: Below 50, the effect of age on wage is governed by \\(\\beta_1\\). Above 50, the slope can change to \\(\\beta_1 + \\beta_2\\).\nContinuity at 50: Because the spline is forced to match up at \\(x = 50\\), there is no abrupt jump in the fitted curve.\nSimplicity of Implementation: We only introduced one additional term \\((x_i - 50)_+\\) to capture the potential change in slope after age 50."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#natural-cubic-spline",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#natural-cubic-spline",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Natural Cubic Spline",
    "text": "Natural Cubic Spline"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-spline-degrees-of-freedom",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-spline-degrees-of-freedom",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Spline: Degrees of Freedom",
    "text": "Smoothing Spline: Degrees of Freedom"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#local-regression-1",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#local-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Local Regression",
    "text": "Local Regression\n\nLocal regression is similar to splines, but differs in an important way. The regions are allowed to overlap, and indeed they do so in a very smooth way.\n\n\n\n\n\n\n\n\n\nWith a sliding weight function, we fit separate linear fits over the range of \\(X\\) by weighted least squares.\n\n\nIt is a smart way to fit non-linear functions by fitting local linear functions on the data!"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary-4",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#generalized-additive-models-1",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#generalized-additive-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\nGeneralized additive models (GAMs) allow us to extend the methods covered in this lecture to deal with multiple predictors.\nGAMs allows for flexible nonlinearities in several variables, but retains the additive structure of linear models.\n\\[\ny_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \\cdots + f_p(x_{ip}) + \\epsilon_i.\n\\]"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-3",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#step-functions-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step functions",
    "text": "Step functions\n\nEasy to work with. Creates a series of dummy variables representing each group.\nUseful way of creating interactions that are easy to interpret. For example, interaction effect of \\(\\text{Year}\\) and \\(\\text{Age}\\):\n\n\n\\[\n    I(\\text{Year} &lt; 2005) \\cdot \\text{Age}, \\quad I(\\text{Year} \\geq 2005) \\cdot \\text{Age}\n\\]\nwould allow for different linear functions in each age category.\n\n\nChoice of cutpoints or knots can be problematic. For creating nonlinearities, smoother alternatives such as splines are available."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary-1",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nSplines and piecewise polynomials build on the concept of dividing the predictor domain into segments, offering enhanced flexibility and precision over traditional polynomial models. By using different polynomial functions in various regions and ensuring smooth transitions at knots, these methods provide a powerful approach to capturing non-linear relationships in data.\n\nPiecewise Polynomials: Replace a single global polynomial with multiple polynomials fitted across segments, resulting in a more tailored and adaptable fit.\n\nContinuity Constraints: Enforcing continuity at knots ensures smooth transitions between polynomial segments, avoiding abrupt changes and better representing underlying trends.\n\nNatural Cubic Splines: Add further constraints by requiring smoothness in the function’s second derivative at knots and controlling boundary behavior, ensuring robustness and reducing overfitting at extremes.\n\nFlexibility and Local Adaptation: Piecewise polynomials adapt to local data behavior, making them particularly effective for capturing non-linear trends and abrupt changes in data. This adaptability is essential in real-world datasets that exhibit complex patterns.\n\nStrategic Knot Placement: Properly placing knots, such as at quantiles of the predictor variable, ensures that each segment has adequate data coverage, resulting in more reliable and stable estimates.\n\nImportance of Boundary Constraints: Natural cubic splines excel at controlling the function’s behavior near boundaries, mitigating overfitting and improving interpretability in regions with sparse data.\n\nAdvantages Over Traditional Polynomials: Splines overcome the limitations of global polynomial fitting, such as excessive wiggliness or global influence of outliers, providing smoother, more interpretable fits that align closely with the data’s natural structure.\nConclusion: Splines offer a superior method for non-linear modeling, combining flexibility, local adaptation, and smoothness, making them an indispensable tool in modern statistical analysis."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary-3",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#summary-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-bringing-more-math",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-bringing-more-math",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: bringing more math",
    "text": "Smoothing Splines: bringing more math\nSmoothing splines are similar to regression splines, but arise in a slightly different situation. Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty.\nConsider this criterion for fitting a smooth function \\(g(x)\\) to some data:\n\\[\n\\text{minimize}_{g \\in S} \\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int \\left( g''(t) \\right)^2 dt\n\\]\n\nThe first term is RSS, and tries to make \\(g(x)\\) match the data at each \\(x_i\\).\nThe second term is a roughness penalty and controls how wiggly \\(g(x)\\) is. It is modulated by the tuning parameter \\(\\lambda \\geq 0\\).\n\nThe smaller \\(\\lambda\\), the more wiggly the function, eventually interpolating \\(y_i\\) when \\(\\lambda = 0\\).\nAs \\(\\lambda \\to \\infty\\), the function \\(g(x)\\) becomes linear."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-adding-mathematical-rigor",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-adding-mathematical-rigor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: Adding Mathematical Rigor",
    "text": "Smoothing Splines: Adding Mathematical Rigor\n\nSmoothing splines are an extension of regression splines but arise in a different context. They are derived by minimizing a residual sum of squares criterion subject to a penalty for roughness, balancing data fidelity and smoothness.\nThe optimization criterion for fitting a smooth function \\(g(x)\\) to data is given by:\n\\[\n\\text{minimize}_{g \\in S}  \\sum_{i=1}^n \\left(y_i - g(x_i)\\right)^2 + \\lambda \\int \\left( g''(t) \\right)^2 dt\n\\]\nKey Components:\n\nResidual Sum of Squares (RSS): The first term, \\(\\sum_{i=1}^n \\left(y_i - g(x_i)\\right)^2\\), ensures that the function \\(g(x)\\) fits the observed data points \\((x_i, y_i)\\) closely.\nRoughness Penalty: The second term, \\(\\lambda \\int \\left( g''(t) \\right)^2 dt\\), penalizes the curvature of \\(g(x)\\) by integrating the square of its second derivative. This term controls how “wiggly” the function is and enforces smoothness.\nTuning Parameter \\(\\lambda\\): The parameter \\(\\lambda \\geq 0\\) determines the tradeoff between fit and smoothness:\n\nWhen \\(\\lambda = 0\\): The penalty vanishes, and the function \\(g(x)\\) becomes fully flexible, interpolating all data points.\nAs \\(\\lambda \\to \\infty\\): The penalty dominates, and \\(g(x)\\) becomes increasingly smooth, eventually reducing to a simple linear function.\n\n\n\nIntuition: Smoothing splines allow for flexible, smooth fits that adapt to the structure of the data while avoiding excessive overfitting. By tuning \\(\\lambda\\), analysts can strike a balance between capturing meaningful patterns and maintaining a smooth, interpretable curve."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-1",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines",
    "text": "Smoothing Splines\n\nSmoothing splines yield a natural cubic spline solution with a knot at every unique value of \\(x_i\\). Rather than choosing specific knot locations, practitioners select a single tuning parameter \\(\\lambda\\), which balances data fidelity and smoothness through the roughness penalty.\n\nAutomatic Knot Placement: By placing a knot at each unique \\(x_i\\), smoothing splines avoid the need to manually choose knot positions. The smoothness parameter \\(\\lambda\\) becomes the key lever for controlling model complexity.\nSmoother Matrix: The fitted values can be written as\n\n\n\\[\n    \\hat{g}_\\lambda = \\mathbf{S}_\\lambda\\,\\mathbf{y},\n\\]\nwhere \\(\\mathbf{S}_\\lambda\\) is an \\(n \\times n\\) matrix determined by the locations of the \\(x_i\\) and the penalty \\(\\lambda\\).\n\n\nEffective Degrees of Freedom: The complexity of the smoothing spline is measured by the effective degrees of freedom, computed as\n\n\n\\[\n    \\text{df}_\\lambda = \\sum_{i=1}^n \\{\\mathbf{S}_\\lambda\\}_{ii},\n\\]\ni.e., the trace of the smoother matrix. A larger trace indicates a more flexible fit, while a smaller trace corresponds to a smoother, less flexible curve."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-choosing-lambda",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-choosing-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines — Choosing \\(\\lambda\\)",
    "text": "Smoothing Splines — Choosing \\(\\lambda\\)\n\nSpecifying \\(\\text{df}\\) Instead of \\(\\lambda\\)\n\nA key advantage of smoothing splines is the option to specify the effective degrees of freedom (\\(\\text{df}\\)) directly, rather than choosing \\(\\lambda\\) outright. This allows you to control the spline’s flexibility (or smoothness) in more intuitive terms.\n\n\nLOOCV\n\nAlternatively, \\(\\lambda\\) can be selected to minimize the leave-one-out (LOO) cross-validation error, given by\n\n\n\n\\[\n\\text{RSS}_{\\text{cv}}(\\lambda)\n      = \\sum_{i=1}^n \\Bigl( y_i - \\hat{g}_\\lambda^{(-i)}(x_i) \\Bigr)^2\n      = \\sum_{i=1}^n \\left[\\frac{y_i - \\hat{g}_\\lambda(x_i)}{1 - \\{\\mathbf{S}_\\lambda\\}_{ii}} \\right]^2,\n\\]\nwhere \\(\\hat{g}_\\lambda^{(-i)}\\) denotes the fitted function obtained by leaving out the \\(i\\)-th observation, and \\(\\mathbf{S}_\\lambda\\) is the smoother matrix.\nWhether you fix the degrees of freedom directly or optimize \\(\\lambda\\) via cross-validation, the goal is to strike a balance between fidelity to the data and smoothness of the spline."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-specifying-degrees-of-freedom",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-specifying-degrees-of-freedom",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: Specifying Degrees of Freedom",
    "text": "Smoothing Splines: Specifying Degrees of Freedom\n\n\nEffective Degrees of Freedom (df)\n\n\nInstead of selecting \\(\\lambda\\) directly, we choose an equivalent “degrees of freedom” value, \\(\\text{df}_\\lambda\\).\nThe relationship between \\(\\lambda\\) and \\(\\text{df}\\) is such that increasing \\(\\lambda\\) (stronger penalty) lowers \\(\\text{df}\\), leading to a smoother (less flexible) spline.\n\n\nAdvantages\n\n\nIntuitive Interpretation: \\(\\text{df}\\) tells us how many “parameters” (roughly) our model is using, making it easier to grasp complexity.\nDirect Control: Analysts may already have an idea of how flexible their model needs to be, and can set \\(\\text{df}\\) accordingly.\n\n\nPractical Tip\n\n\nEmpirically, one might try a range of df values (e.g., 4, 5, 6, …) and pick the one that balances interpretability and fit (potentially guided by an information criterion or cross-validation)."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-leave-one-out-cross-validation-loocv",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-leave-one-out-cross-validation-loocv",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: Leave-One-Out Cross-Validation (LOOCV)",
    "text": "Smoothing Splines: Leave-One-Out Cross-Validation (LOOCV)\n\n\n\n\nWe can use Cross-validation to choose \\(\\alpha\\). For each candidate \\(\\alpha\\), we leave out one data point at a time, fit the spline, and measure the prediction error on the held-out point.\nMathematical Form\n\n\n\\[\n     \\text{RSS}_{\\text{cv}}(\\alpha)\n       = \\sum_{i=1}^n \\left(y_i - \\hat{g}_\\alpha^{(-i)}(x_i)\\right)^2,\n\\]\n\n\nwhere \\(\\hat{g}_\\alpha^{(-i)}\\) denotes the spline fitted without the \\(i\\)-th data point.\nIn practice, this can be computed more efficiently via the influence matrix \\(\\mathbf{S}_\\alpha\\):\n\n\n\\[\n       \\text{RSS}_{\\text{cv}}(\\alpha)\n         = \\sum_{i=1}^n \\left[\\frac{y_i - \\hat{g}_\\alpha(x_i)}{1 - \\{\\mathbf{S}_\\alpha\\}_{ii}} \\right]^2.\n\\]\n\n\n\nAdvantages\n\n\nData-Driven: No need to guess \\(\\text{df}\\) or \\(\\alpha\\); we let cross-validation find the best trade-off.\nRobust: Tends to choose a value that generalizes well to new data.\n\n\nPractical Tip\n\n\nEvaluate \\(\\text{RSS}_{\\text{cv}}\\) for a grid of \\(\\alpha\\) values (e.g., \\(\\alpha \\in \\{0.01, 0.1, 1, 10\\}\\)) and pick the one that minimizes the cross-validation error."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-loocv-to-specify-alpha",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-loocv-to-specify-alpha",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: LOOCV to Specify \\(\\alpha\\)",
    "text": "Smoothing Splines: LOOCV to Specify \\(\\alpha\\)\n\n\n\n\nWe can use Cross-validation to choose \\(\\alpha\\).\n\n\nFor each candidate \\(\\alpha\\), we leave out one data point at a time, fit the spline, and measure the prediction error on the held-out point.\n\n\nMathematical Form\n\n\n\\[\n     \\text{RSS}_{\\text{cv}}(\\alpha)\n       = \\sum_{i=1}^n \\left(y_i - \\hat{g}_\\alpha^{(-i)}(x_i)\\right)^2,\n\\]\n\n\nwhere \\(\\hat{g}_\\alpha^{(-i)}\\) denotes the spline fitted without the \\(i\\)-th data point.\nIn practice, this can be computed more efficiently via the influence matrix \\(\\mathbf{S}_\\alpha\\):\n\n\n\\[\n       \\text{RSS}_{\\text{cv}}(\\alpha)\n         = \\sum_{i=1}^n \\left[\\frac{y_i - \\hat{g}_\\alpha(x_i)}{1 - \\{\\mathbf{S}_\\alpha\\}_{ii}} \\right]^2.\n\\]\n\n\n\nAdvantages\n\n\nData-Driven: No need to guess \\(\\text{df}\\) or \\(\\alpha\\); we let cross-validation find the best trade-off.\nRobust: Tends to choose a value that generalizes well to new data.\n\n\nPractical Tip\n\n\nEvaluate \\(\\text{RSS}_{\\text{cv}}\\) for a grid of \\(\\alpha\\) values (e.g., \\(\\alpha \\in \\{0.01, 0.1, 1, 10\\}\\)) and pick the one that minimizes the cross-validation error."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Moving Beyond Linearity\nPolynomial Regression\nStep Functions\nRegression Splines\n\n\n\nSmoothing Splines\nLocal Regression\nGeneralized Additive Models\n\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#overview",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nTree-based Methods\nThe Basics of Decision Trees\nDetails of the tree-building process\nPredictions\nClassification Trees\n\n\n\nTrees Versus Linear Models\nBagging\nRandom Forests\nBoosting\nVariable Importance Measure\nBART\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tree-based-methods",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tree-based-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tree-based Methods",
    "text": "Tree-based Methods\n\nHere we describe tree-based methods for regression and classification.\nThese involve stratifying or segmenting the predictor space into a number of simple regions.\nSince the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision-tree methods."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pros-and-cons",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pros-and-cons",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pros and Cons",
    "text": "Pros and Cons\n\nTree-based methods are simple and useful for interpretation.\nHowever, they typically are not competitive with the best supervised learning approaches in terms of prediction accuracy.\nHence we also discuss bagging, random forests, and boosting. These methods grow multiple trees which are then combined to yield a single consensus prediction.\nCombining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#the-basics-of-decision-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#the-basics-of-decision-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Basics of Decision Trees",
    "text": "The Basics of Decision Trees\n\nDecision trees can be applied to both regression and classification problems.\nWe first consider regression problems, and then move on to classification."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-salary-data-how-would-you-stratify-it",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-salary-data-how-would-you-stratify-it",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball salary data: how would you stratify it?",
    "text": "Baseball salary data: how would you stratify it?\n\n\n\nBefore examining the structure of a decision tree, let us begin by considering how to stratify the data.\n\n\n\n\n\n\n\n\n\nHere, the response variable is player salary, which is color-coded in the figure from lower salaries (blue and green) to higher salaries (yellow and red).\n\nOur objective is to separate high-salary players from those with lower salaries.\n\nBy inspecting the plot, we observe that players earning higher salaries tend to cluster in the upper portion, whereas players with lower salaries appear in an “L-shaped” cluster below.\nOne straightforward approach is to establish a vertical threshold around five years of career experience, effectively isolating many of the higher-salary players.\nTo refine the partitioning further, we might introduce a horizontal threshold slightly above 100 hits, creating three distinct segments of the feature space.\nThis step-by-step segmentation process is precisely how decision trees operate. By recursively applying threshold-based rules, we isolate increasingly homogeneous subsets of players and, in turn, more accurately predict salary levels."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#decision-tree-for-these-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#decision-tree-for-these-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Decision tree for these data",
    "text": "Decision tree for these data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe top node represents the full dataset.\nThe first split is based on years of experience:\n\nPlayers with less than 4.5 years → Left branch.\nPlayers with more than 4.5 years → Right branch.\n\nThis split closely aligns with our initial estimate of 5 years.\nAmong players with more than 4.5 years, the tree applies a second split:\n\nFewer than 117.5 hits → Left branch.\nMore than 117.5 hits → Right branch.\n\nThis recursive splitting refines the salary prediction.\nWhat Do the Numbers Represent?\nThe values at the bottom nodes indicate the average log salary of players in that group.\nSince a log transformation was applied, these values represent average log salaries rather than raw salaries.\nFinal Segmentation\nThe decision tree ultimately divides players into three distinct salary groups:\n\n\nHighest salary\nMedium salary\nLowest salary\n\n\nThese categories closely match—though not exactly—the three regions we initially identified.\nBy systematically applying these splits, decision trees segment data into meaningful, homogeneous groups.\nThe final tree has two internal nodes and three terminal nodes (leaves). The number in each leaf is the mean of the response for the observations that fall there."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of previous figure",
    "text": "Details of previous figure\n\nFor the Hitters data, a regression tree for predicting the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year.\nAt a given internal node, the label (of the form \\(X_j &lt; t_k\\)) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to \\(X_j \\geq t_k\\). For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to \\(\\text{Years} &lt; 4.5\\), and the right-hand branch corresponds to \\(\\text{Years} \\geq 4.5\\).\nThe tree has two internal nodes and three terminal nodes, or leaves. The number in each leaf is the mean of the response for the observations that fall there."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#results",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\n\n\n\n\nThe tree stratifies or segments the players into three regions of predictor space:\n\\[\nR_1 = \\{X \\ | \\ \\text{Years} &lt; 4.5\\}\n\\]\n\\[\nR_2 = \\{X \\ | \\ \\text{Years} \\geq 4.5, \\text{Hits} &lt; 117.5\\}\n\\]\n\\[\nR_3 = \\{X \\ | \\ \\text{Years} \\geq 4.5, \\text{Hits} \\geq 117.5\\}\n\\]"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#terminology-for-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#terminology-for-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Terminology for Trees",
    "text": "Terminology for Trees\n\nIn keeping with the tree analogy, the regions \\(R_1\\), \\(R_2\\), and \\(R_3\\) are known as terminal nodes.\nDecision trees are typically drawn upside down, in the sense that the leaves are at the bottom of the tree.\nThe points along the tree where the predictor space is split are referred to as internal nodes.\nIn the Hitters tree, the two internal nodes are indicated by the text \\(\\text{Years} &lt; 4.5\\) and \\(\\text{Hits} &lt; 117.5\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#interpretation-of-results",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#interpretation-of-results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\n\nYears is the most important factor in determining Salary, and players with less experience earn lower salaries than more experienced players.\nGiven that a player is less experienced, the number of Hits that he made in the previous year seems to play little role in his Salary.\nBut among players who have been in the major leagues for five or more years, the number of Hits made in the previous year does affect Salary, and players who made more Hits last year tend to have higher salaries.\nSurely an over-simplification, but compared to a regression model, it is easy to display, interpret and explain."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-the-tree-building-process",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-the-tree-building-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of the tree-building process",
    "text": "Details of the tree-building process\n\nWe divide the predictor space — that is, the set of possible values for \\(X_1, X_2, \\dots, X_p\\) — into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, \\dots, R_J\\).\nFor every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#more-details-of-the-tree-building-process",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#more-details-of-the-tree-building-process",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More details of the tree-building process",
    "text": "More details of the tree-building process\n\nIn theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or boxes, for simplicity and for ease of interpretation of the resulting predictive model.\nThe goal is to find boxes \\(R_1, \\dots, R_J\\) that minimize the RSS, given by\n\n\n\\[\n\\sum_{j=1}^{J} \\sum_{i \\in R_j} \\left( y_i - \\hat{y}_{R_j} \\right)^2,\n\\]\nwhere \\(\\hat{y}_{R_j}\\) is the mean response for the training observations within the \\(j\\)-th box."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#more-details-of-the-tree-building-process-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#more-details-of-the-tree-building-process-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "More details of the tree-building process",
    "text": "More details of the tree-building process\n\nUnfortunately, it is computationally infeasible to consider every possible partition of the feature space into \\(J\\) boxes.\nFor this reason, we take a top-down, greedy approach that is known as recursive binary splitting.\nThe approach is top-down because it begins at the top of the tree and then successively splits the predictor space; each split is indicated via two new branches further down on the tree.\nIt is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details— Continued",
    "text": "Details— Continued\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe first select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X | X_j &lt; s\\}\\) and \\(\\{X | X_j \\geq s\\}\\) leads to the greatest possible reduction in RSS.\nNext, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions.\nHowever, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions.\nAgain, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions",
    "text": "Predictions\n\nWe predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.\nA five-region example of this approach is shown in the next slide."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions",
    "text": "Predictions\nWe predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of previous figure",
    "text": "Details of previous figure\n\nResults from performing boosting and random forests on the fifteen-class gene expression data set in order to predict cancer versus normal.\nThe test error is displayed as a function of the number of trees. For the two boosted models, \\(\\lambda = 0.01\\). Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant.\nThe test error rate for a single tree is 24%."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pruning-a-tree",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pruning-a-tree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pruning a tree",
    "text": "Pruning a tree\n\nThe process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance.\n\n\n\nA better strategy is to grow a very large tree \\(T_0\\), and then prune it back in order to obtain a subtree.\nCost complexity pruning — also known as weakest link pruning — is used to do this.\nWe consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\). For each value of \\(\\alpha\\), there corresponds a subtree \\(T \\subset T_0\\) such that\n\n\n\\[\n\\sum_{m=1}^{|T|} \\sum_{i : x_i \\in R_m} \\left( y_i - \\hat{y}_{R_m} \\right)^2 + \\alpha |T|\n\\]\nis as small as possible.\nHere \\(|T|\\) indicates the number of terminal nodes of the tree \\(T\\), \\(R_m\\) is the rectangle (i.e., the subset of predictor space) corresponding to the \\(m\\)-th terminal node, and \\(\\hat{y}_{R_m}\\) is the mean of the training observations in \\(R_m\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pruning-a-tree-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#pruning-a-tree-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pruning a tree— continued",
    "text": "Pruning a tree— continued\n\nA better strategy is to grow a very large tree \\(T_0\\), and then prune it back in order to obtain a subtree.\nCost complexity pruning — also known as weakest link pruning — is used to do this.\nWe consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\). For each value of \\(\\alpha\\), there corresponds a subtree \\(T \\subset T_0\\) such that\n\n\\[\n\\sum_{m=1}^{|T|} \\sum_{i : x_i \\in R_m} \\left( y_i - \\hat{y}_{R_m} \\right)^2 + \\alpha |T|\n\\]\nis as small as possible. Here \\(|T|\\) indicates the number of terminal nodes of the tree \\(T\\), \\(R_m\\) is the rectangle (i.e., the subset of predictor space) corresponding to the \\(m\\)-th terminal node, and \\(\\hat{y}_{R_m}\\) is the mean of the training observations in \\(R_m\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#choosing-the-best-subtree",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#choosing-the-best-subtree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the best subtree",
    "text": "Choosing the best subtree\n\nThe tuning parameter \\(\\alpha\\) controls a trade-off between the subtree’s complexity and its fit to the training data.\nWe select an optimal value \\(\\hat{\\alpha}\\) using cross-validation.\nWe then return to the full data set and obtain the subtree corresponding to \\(\\hat{\\alpha}\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary-tree-algorithm",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary-tree-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary: tree algorithm",
    "text": "Summary: tree algorithm\n\nUse recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\nApply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha\\).\nUse K-fold cross-validation to choose \\(\\alpha\\). For each \\(k = 1, \\dots, K\\):\n3.1 Repeat Steps 1 and 2 on the \\(\\frac{K-1}{K}\\)-th fraction of the training data, excluding the \\(k\\)-th fold. 3.2 Evaluate the mean squared prediction error on the data in the left-out \\(k\\)-th fold, as a function of \\(\\alpha\\).\nAverage the results, and pick \\(\\alpha\\) to minimize the average error.\nReturn the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example continued",
    "text": "Baseball example continued\n\nFirst, we randomly divided the data set in half, yielding 132 observations in the training set and 131 observations in the test set.\nWe then built a large regression tree on the training data and varied \\(\\alpha\\) in order to create subtrees with different numbers of terminal nodes.\nFinally, we performed six-fold cross-validation in order to estimate the cross-validated MSE of the trees as a function of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example continued",
    "text": "Baseball example continued"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued-2",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-continued-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example continued",
    "text": "Baseball example continued"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#classification-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#classification-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nVery similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.\nFor a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-classification-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-classification-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of classification trees",
    "text": "Details of classification trees\n\nJust as in the regression setting, we use recursive binary splitting to grow a classification tree.\nIn the classification setting, RSS cannot be used as a criterion for making the binary splits.\nA natural alternative to RSS is the classification error rate. This is simply the fraction of the training observations in that region that do not belong to the most common class:\n\n\\[\nE = 1 - \\max_k(\\hat{p}_{mk}).\n\\]\nHere \\(\\hat{p}_{mk}\\) represents the proportion of training observations in the \\(m\\)-th region that are from the \\(k\\)-th class.\n\nHowever, classification error is not sufficiently sensitive for tree-growing, and in practice, two other measures are preferable."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#gini-index-and-deviance",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#gini-index-and-deviance",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gini index and Deviance",
    "text": "Gini index and Deviance\nThe Gini index is defined by\n\\[\nG = \\sum_{k=1}^K \\hat{p}_{mk}(1 - \\hat{p}_{mk}),\n\\]\na measure of total variance across the \\(K\\) classes. The Gini index takes on a small value if all of the \\(\\hat{p}_{mk}\\)’s are close to zero or one.\n\nFor this reason, the Gini index is referred to as a measure of node purity — a small value indicates that a node contains predominantly observations from a single class.\n\n\nDeviance or cross-entropy, given by\n\\[\nD = - \\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}.\n\\]\n\nIt turns out that the Gini index and the cross-entropy are very similar numerically."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-heart-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Heart data",
    "text": "Example: Heart data\n\nThese data contain a binary outcome HD for 303 patients who presented with chest pain.\nAn outcome value of Yes indicates the presence of heart disease based on an angiographic test, while No means no heart disease.\nThere are 13 predictors including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements.\nCross-validation yields a tree with six terminal nodes. See next figure."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#trees-versus-linear-models",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#trees-versus-linear-models",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Trees Versus Linear Models",
    "text": "Trees Versus Linear Models\n\nTop Row: True linear boundary; Bottom row: true non-linear boundary.\nLeft column: Linear model; Right column: Tree-based model."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#advantages-and-disadvantages-of-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#advantages-and-disadvantages-of-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advantages and Disadvantages of Trees",
    "text": "Advantages and Disadvantages of Trees\n\nAdvantage: Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\nAdvantage: Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches.\nAdvantage: Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).\nAdvantage: Trees can easily handle qualitative predictors without the need to create dummy variables.\nDisadvantage: Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.\nHowever, by aggregating many decision trees, the predictive performance of trees can be substantially improved. We introduce these concepts next."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging",
    "text": "Bagging\n\nBootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.\nRecall that given a set of \\(n\\) independent observations \\(Z_1, \\dots, Z_n\\), each with variance \\(\\sigma^2\\), the variance of the mean \\(\\bar{Z}\\) of the observations is given by \\(\\sigma^2 / n\\).\nIn other words, averaging a set of observations reduces variance. Of course, this is not practical because we generally do not have access to multiple training sets."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging— continued",
    "text": "Bagging— continued\n\nInstead, we can bootstrap, by taking repeated samples from the (single) training data set.\nIn this approach, we generate \\(B\\) different bootstrapped training data sets. We then train our method on the \\(b\\)-th bootstrapped training set in order to get \\(\\hat{f}^*_b(x)\\), the prediction at a point \\(x\\). We then average all the predictions to obtain\n\n\\[\n\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^*_b(x).\n\\]\nThis is called bagging."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-classification-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-classification-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging classification trees",
    "text": "Bagging classification trees\n\nThe above prescription applied to regression trees.\nFor classification trees: for each test observation, we record the class predicted by each of the \\(B\\) trees, and take a majority vote: the overall prediction is the most commonly occurring class among the \\(B\\) predictions."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-the-heart-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-the-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging the Heart data",
    "text": "Bagging the Heart data\n\n\nBagging and Random Forest results.\n\n\n\n\n\n\n\n\n\n\n\nThe dashed line indicates the test error resulting from a single classification tree.\nThe test error (black and orange) is shown as a function of \\(B\\), the number of bootstrapped training sets used.\nRandom forests were applied with \\(m = \\sqrt{p}\\).\nThe green and blue traces show the Out-of-Bag (OOB) error, which in this case is considerably lower."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-2",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of previous figure",
    "text": "Details of previous figure\n\nResults from performing boosting and random forests on the fifteen-class gene expression data set in order to predict cancer versus normal.\nThe test error is displayed as a function of the number of trees. For the two boosted models, \\(\\lambda = 0.01\\). Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant.\nThe test error rate for a single tree is 24%."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#out-of-bag-error-estimation",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#out-of-bag-error-estimation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Out-of-Bag Error Estimation",
    "text": "Out-of-Bag Error Estimation\n\nIt turns out that there is a very straightforward way to estimate the test error of a bagged model.\nRecall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations.\nThe remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations.\nWe can predict the response for the \\(i\\)th observation using each of the trees in which that observation was OOB. This will yield around \\(B/3\\) predictions for the \\(i\\)th observation, which we average.\nThis estimate is essentially the LOO cross-validation error for bagging, if \\(B\\) is large."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#random-forests",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#random-forests",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Random Forests",
    "text": "Random Forests\n\nRandom forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. This reduces the variance when we average the trees.\nAs in bagging, we build a number of decision trees on bootstrapped training samples.\nBut when building these decision trees, each time a split in a tree is considered, a random selection of \\(m\\) predictors is chosen as split candidates from the full set of \\(p\\) predictors. The split is allowed to use only one of those \\(m\\) predictors.\nA fresh selection of \\(m\\) predictors is taken at each split, and typically we choose \\(m \\approx \\sqrt{p}\\) — that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors (4 out of the 13 for the Heart data)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-gene-expression-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-gene-expression-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Gene Expression Data",
    "text": "Example: Gene Expression Data\n\nWe applied random forests to a high-dimensional biological data set consisting of expression measurements of 4,718 genes measured on tissue samples from 349 patients.\nThere are around 20,000 genes in humans, and individual genes have different levels of activity, or expression, in particular cells, tissues, and biological conditions.\nEach of the patient samples has a qualitative label with 15 different levels: either normal or one of 14 different types of cancer.\nWe use random forests to predict cancer type based on the 500 genes that have the largest variance in the training set.\nWe randomly divided the observations into a training and a test set, and applied random forests to the training set for three different values of the number of splitting variables \\(m\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#results-gene-expression-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#results-gene-expression-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results: Gene Expression Data",
    "text": "Results: Gene Expression Data\nResults from random forests for the fifteen-class gene expression data set with \\(p = 500\\) predictors.\n\n\nThe test error is displayed as a function of the number of trees. Each colored line corresponds to a different value of \\(m\\), the number of predictors available for splitting at each interior tree node.\nRandom forests (\\(m &lt; p\\)) lead to a slight improvement over bagging (\\(m = p\\)). A single classification tree has an error rate of 45.7%."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-3",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of previous figure",
    "text": "Details of previous figure\n\nResults from performing boosting and random forests on the fifteen-class gene expression data set in order to predict cancer versus normal.\nThe test error is displayed as a function of the number of trees. For the two boosted models, \\(\\lambda = 0.01\\). Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant.\nThe test error rate for a single tree is 24%."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Boosting",
    "text": "Boosting\n\nLike bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. We only discuss boosting for decision trees.\nRecall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\nNotably, each tree is built on a bootstrap data set, independent of the other trees.\nBoosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-algorithm-for-regression-trees",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-algorithm-for-regression-trees",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Boosting Algorithm for Regression Trees",
    "text": "Boosting Algorithm for Regression Trees\n\n\nSet \\(\\hat{f}(x) = 0\\) and \\(r_i = y_i\\) for all \\(i\\) in the training set.\nFor \\(b = 1, 2, \\dots, B\\), repeat:\n\n\n2.1 Fit a tree \\(\\hat{f}^b\\) with \\(d\\) splits (\\(d + 1\\) terminal nodes) to the training data \\((X, r)\\).\n2.2 Update \\(\\hat{f}\\) by adding in a shrunken version of the new tree:\n\\[\n   \\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x).\n\\]\n2.3 Update the residuals,\n\n\n\\[\n   r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i).\n\\]\n\n\nOutput the boosted model,\n\n\n\\[\n\\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x).\n\\]"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#what-is-the-idea-behind-this-procedure",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#what-is-the-idea-behind-this-procedure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is the idea behind this procedure?",
    "text": "What is the idea behind this procedure?\n\nUnlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly.\nGiven the current model, we fit a decision tree to the residuals from the model. We then add this new decision tree into the fitted function in order to update the residuals.\nEach of these trees can be rather small, with just a few terminal nodes, determined by the parameter \\(d\\) in the algorithm.\nBy fitting small trees to the residuals, we slowly improve \\(\\hat{f}\\) in areas where it does not perform well. The shrinkage parameter \\(\\lambda\\) slows the process down even further, allowing more and different shaped trees to attack the residuals.\nBoosting for classification is similar in spirit to boosting for regression, but is a bit more complex. To learn the deatails, check the Elements of Statistical Learning book, chapter 10."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-for-classification",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-for-classification",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Boosting for classification",
    "text": "Boosting for classification\n\nBoosting for classification is similar in spirit to boosting for regression, but is a bit more complex. To learn the deatails, check the Elements of Statistical Learning book, chapter 10."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#gene-expression-data-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#gene-expression-data-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gene expression data continued",
    "text": "Gene expression data continued"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-4",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-previous-figure-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of previous figure",
    "text": "Details of previous figure\n\nResults from performing boosting and random forests on the fifteen-class gene expression data set in order to predict cancer versus normal.\nThe test error is displayed as a function of the number of trees. For the two boosted models, \\(\\lambda = 0.01\\). Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant.\nThe test error rate for a single tree is 24%."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tuning-parameters-for-boosting",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tuning-parameters-for-boosting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tuning Parameters for Boosting",
    "text": "Tuning Parameters for Boosting\n\nThe number of trees \\(B\\). Unlike bagging and random forests, boosting can overfit if \\(B\\) is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select \\(B\\).\nThe shrinkage parameter \\(\\lambda\\). A small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small \\(\\lambda\\) can require using a very large value of \\(B\\) in order to achieve good performance.\nThe number of splits \\(d\\) in each tree, which controls the complexity of the boosted ensemble. Often \\(d = 1\\) works well, in which case each tree is a stump, consisting of a single split and resulting in an additive model. More generally \\(d\\) is the interaction depth, and controls the interaction order of the boosted model, since \\(d\\) splits can involve at most \\(d\\) variables."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#another-regression-example",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#another-regression-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Another Regression Example",
    "text": "Another Regression Example\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Elements of Statistical Learning book, chapter 15."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#another-classification-example",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#another-classification-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Another Classification Example",
    "text": "Another Classification Example\n\nFrom Elements of Statistical Learning, chapter 15."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#variable-importance-measure",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#variable-importance-measure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Variable Importance Measure",
    "text": "Variable Importance Measure\n\nFor bagged/RF regression trees:\n\nRecord the total amount that the RSS is decreased due to splits over a given predictor, averaged over all \\(B\\) trees.\nA large value indicates an important predictor.\n\nFor bagged/RF classification trees:\n\nAdd up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all \\(B\\) trees.\n\n\n\nVariable Importance Plot for the Heart Data"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\nDecision trees are simple and interpretable models for regression and classification.\nHowever, they are often not competitive with other methods in terms of prediction accuracy.\nBagging, random forests, and boosting are effective methods for improving the prediction accuracy of trees:\n\nThey work by growing many trees on the training data and then combining the predictions of the resulting ensemble of trees.\n\nRandom forests and boosting are among the state-of-the-art methods for supervised learning, though their results can be difficult to interpret."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nDecision Trees\n\nPartition the predictor space into simple regions.\nEasy to interpret and visualize.\nProne to overfitting without pruning or regularization.\n\nBagging\n\nTrains multiple trees on bootstrap samples.\nAverages predictions to reduce variance.\nOut-of-Bag (OOB) error provides an internal estimate of test error.\n\nRandom Forests\n\nA variant of bagging that selects a random subset of predictors at each split.\nReduces correlation among trees, improving variance reduction.\nTypically uses \\(m \\approx \\sqrt{p}\\) features at each split.\n\n\n\n\nBoosting\n\nBuilds trees sequentially, each learning from the residuals of the previous trees.\nInvolves a shrinkage parameter (\\(\\lambda\\)) to control the learning rate.\nCan achieve strong predictive performance, but can be less interpretable.\n\nBayesian Additive Regression Trees (BART)\n\nCombines ideas from bagging and boosting in a Bayesian framework.\nMaintains \\(K\\) trees in parallel, updating each by random perturbations.\nCan handle both regression and classification.\nOffers built-in measures of uncertainty and often works well with minimal tuning."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tree-based-methods-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tree-based-methods-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tree-based Methods",
    "text": "Tree-based Methods\n\nWe will discuss tree-based methods for regression and classification.\nThese involve stratifying or segmenting the predictor space into a number of simple regions.\nSince the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision-tree methods."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#the-basics-of-decision-trees-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#the-basics-of-decision-trees-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Basics of Decision Trees",
    "text": "The Basics of Decision Trees\n\nDecision trees can be applied to both regression and classification problems.\nWe first consider regression problems, and then move on to classification."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-salary-hitters-data-how-would-you-stratify-it",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-salary-hitters-data-how-would-you-stratify-it",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball salary (Hitters) data: how would you stratify it?",
    "text": "Baseball salary (Hitters) data: how would you stratify it?\n\n\n\nBefore examining the structure of a decision tree, let us begin by considering how to stratify the data.\n\n\n\n\n\n\n\n\n\nHere, the response variable is player salary, which is color-coded in the figure from lower salaries (blue and green) to higher salaries (yellow and red).\n\nOur objective is to separate high-salary players from those with lower salaries.\n\nBy inspecting the plot, we observe that players earning higher salaries tend to cluster in the upper portion, whereas players with lower salaries appear in an “L-shaped” cluster below.\nOne straightforward approach is to establish a vertical threshold around five years of career experience, effectively isolating many of the higher-salary players.\nTo refine the partitioning further, we might introduce a horizontal threshold slightly above 100 hits, creating three distinct segments of the feature space.\nThis step-by-step segmentation process is precisely how decision trees operate. By recursively applying threshold-based rules, we isolate increasingly homogeneous subsets of players and, in turn, more accurately predict salary levels."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-the-tree-building-process-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#details-of-the-tree-building-process-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of the tree-building process",
    "text": "Details of the tree-building process\n\nWe divide the predictor space — that is, the set of possible values for \\(X_1, X_2, \\dots, X_p\\) — into \\(J\\) distinct and non-overlapping regions, \\(R_1, R_2, \\dots, R_J\\).\nFor every observation that falls into the region \\(R_j\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_j\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-2",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions",
    "text": "Predictions"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-example",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions: Example",
    "text": "Predictions: Example\n\n\nA five-region example\n\n\n\n\n\n\n\n\n\n\n\nTop Left: A partition of two-dimensional feature space that could not result from recursive binary splitting.\nTop Right: The output of recursive binary splitting on a two-dimensional example.\nBottom Left: A tree corresponding to the partition in the top right panel.\nBottom Right: A perspective plot of the prediction surface corresponding to that tree."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-example-details",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#predictions-example-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Predictions: Example Details",
    "text": "Predictions: Example Details\n\n\nTop right & Bottom left: Regions that can be achieved by a decision tree.\nStep-by-step breakdown:\n\nThe first split occurs at t1, making a vertical partition at \\(x_1\\).\nThe left region is further divided at t2 (a split on \\(x_2\\)), creating Region 1 and Region 2.\nThe right-hand partition is further split at t3, making another vertical split, forming Region 3.\nFinally, a horizontal split at t4 on \\(x_2\\) divides it into two new regions.\n\n\nAt the end of this process, the space is divided into five distinct regions.\n\n\nMaking Predictions with the Decision Tree\n\nEach terminal node approximates the regression function by computing the mean of training observations in that region.\nTo predict a test observation:\n\n\nStart at the top and check its \\(x_1\\) value.\nMove left if \\(x_1 &lt; t1\\), otherwise move right.\nFollow subsequent splits at each internal node until reaching a terminal region.\nThe final prediction is the mean response value in that region.\n\n\nThis results in a piecewise constant function, which is visualized in the plot on the bottom right."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tree-algorithm",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#tree-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tree algorithm",
    "text": "Tree algorithm\n\nUse recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\nApply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha\\).\nUse K-fold cross-validation to choose \\(\\alpha\\). For each \\(k = 1, \\dots, K\\):\n3.1 Repeat Steps 1 and 2 on the \\(\\frac{K-1}{K}\\)-th fraction of the training data, excluding the \\(k\\)-th fold. 3.2 Evaluate the mean squared prediction error on the data in the left-out \\(k\\)-th fold, as a function of \\(\\alpha\\).\nAverage the results, and pick \\(\\alpha\\) to minimize the average error.\nReturn the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-the-full-tree-before-pruning",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-the-full-tree-before-pruning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example: The Full Tree Before Pruning",
    "text": "Baseball example: The Full Tree Before Pruning"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-cross-validation-for-the-prune-tree",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#baseball-example-cross-validation-for-the-prune-tree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Baseball example: Cross Validation for the Prune Tree",
    "text": "Baseball example: Cross Validation for the Prune Tree\n\n\nAlong the horizontal axis, we have tree size, which is controlled by the alpha parameter (\\(\\alpha\\)). This parameter directly influences the complexity of the decision tree.\n\nWhen \\(\\alpha = 0\\), there is no penalty on tree size, meaning the model grows to its largest possible tree, which in this case contains 12 terminal nodes.\nAs \\(\\alpha\\) increases, a stronger penalty is applied to larger trees, gradually reducing the number of terminal nodes.\nAs \\(\\alpha\\) continues to increase, the model prunes away more splits, simplifying the tree structure.\nAt the extreme, when \\(\\alpha\\) is large enough, the tree is reduced to a single node, meaning no splits occur, and the model collapses into a single global mean prediction.\nThe green curve is what we get from cross validation and it’s minimized at around three terminal nodes!"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#classification-trees-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#classification-trees-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nVery similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one.\nFor a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#classification-trees-2",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#classification-trees-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Classification Trees",
    "text": "Classification Trees\n\nJust as in the regression setting, we use recursive binary splitting to grow a classification tree.\nIn the classification setting, RSS cannot be used as a criterion for making the binary splits.\nA natural alternative to RSS is the classification error rate. This is simply the fraction of the training observations in that region that do not belong to the most common class:\n\n\n\\[\nE = 1 - \\max_k(\\hat{p}_{mk}).\n\\]\nHere \\(\\hat{p}_{mk}\\) represents the proportion of training observations in the \\(m\\)-th region that are from the \\(k\\)-th class.\n\nHowever, classification error is not sufficiently sensitive for tree-growing, and in practice, two other measures are preferable."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-heart-data-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-heart-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Heart data",
    "text": "Example: Heart data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt the top, we see the fully grown tree.\n\nThe first split occurs on FEL (a thallium stress test), followed by splits on CA (calcium). The terminal nodes classify observations as “No” (no heart disease) or “Yes” (heart disease) based on majority class.\nSome terminal nodes with the same classification still have splits. This suggests that while both nodes predict “No,” one is purer than the other, as identified by the Gini index.\n\nSince this tree is likely too complex, cross-validation was used to find an optimal size.\n\nThe right panel shows training, validation, and test errors, with the cross-validation error curve guiding the selection of a tree size. A tree with six terminal nodes performed best, balancing complexity and accuracy.\n\nThe pruned tree (size six) is shown on the right, derived using the cost-complexity parameter (\\(\\alpha\\)). This subtree of the original tree achieved an estimated 25% classification error—a significant improvement in generalization."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#trees-versus-linear-models-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#trees-versus-linear-models-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Trees Versus Linear Models",
    "text": "Trees Versus Linear Models\n\n\nTop Row: True linear boundary;\nBottom row: true non-linear boundary.\nLeft column: Linear model;\nRight column: Tree-based model."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging",
    "text": "Bagging\n\n\nBootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method. It is particularly useful and frequently used in the context of decision trees.\nRecall that given a set of \\(n\\) independent observations \\(Z_1, \\dots, Z_n\\), each with variance \\(\\sigma^2\\), the variance of the mean \\(\\bar{Z}\\) of the observations is given by \\(\\sigma^2 / n\\).\n\nThis means that as \\(n\\) increases (i.e., we take more independent observations and average them), the variance of the mean decreases. The more independent samples we have, the more stable our estimate becomes.\n\nIn other words, averaging a set of observations reduces variance. In practice, we do not have access to multiple independent training sets, which would allow us to directly apply the above variance reduction principle.\n\nHowever, bagging overcomes this limitation by using bootstrapping—randomly sampling (with replacement) from a single training set to create multiple datasets. These datasets are used to train multiple models, whose predictions are then averaged, effectively reducing variance in the same way that averaging multiple independent observations does.\nThus, bagging approximates the variance-reducing effect of having multiple training sets by repeatedly resampling from the same dataset."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-2",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bagging-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bagging",
    "text": "Bagging\nWith bootstrap, by taking repeated samples from the (single) training data set, we generate \\(B\\) different bootstrapped training data sets.\nWe then train our method on the \\(b\\)-th bootstrapped training set in order to get \\(\\hat{f}^*_b(x)\\), the prediction at a point \\(x\\). We then average all the predictions to obtain\n\\[\n\\hat{f}_{\\text{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^*_b(x).\n\\]\nThis is called bagging.\n\nThe above prescription applied to regression trees.\nFor classification trees: for each test observation, we record the class predicted by each of the \\(B\\) trees, and take a majority vote: the overall prediction is the most commonly occurring class among the \\(B\\) predictions."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#test-error-vs.-oob-error-in-bagging",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#test-error-vs.-oob-error-in-bagging",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Test Error vs. OOB Error in Bagging",
    "text": "Test Error vs. OOB Error in Bagging\n\n\nData Overlap & Model Correlation\n\nEach model is trained on a bootstrap sample, leaving out some observations for OOB evaluation.\n\nOverlap among bootstrap samples introduces correlation among models, causing OOB error to differ from an independent test error.\n\nSample Size & Representativeness\n\nOOB estimates use only the leftover observations from each bootstrap draw, often fewer and less representative than a true external test set.\n\nA well-chosen test set is typically larger and fully independent, providing a more stable performance estimate.\n\nRandom Fluctuations & Variance\n\nOOB error depends on random sampling; it can have higher variance than test error.\n\nAn external test set, assuming it is independent, generally offers a less biased measure of generalization error.\n\nPossible Bias in OOB Error\n\nWhile OOB error acts like an “internal cross-validation,” it may exhibit slight bias—optimistic or pessimistic—depending on the dataset and model specifics.\n\nCertain data characteristics or model sensitivities can amplify discrepancies between OOB and true test performance."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#random-forests-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#random-forests-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Random Forests",
    "text": "Random Forests\n\nKey Idea\nRandom forests enhance bagged trees by introducing a small modification that decorrelates the individual trees, thus reducing variance when their predictions are averaged.\nConstruction\n\nBootstrapped Samples: As in bagging, multiple decision trees are trained on bootstrapped subsets of the original dataset.\n\nRandom Predictor Selection: At each split in a tree, only a random subset of \\(m\\) predictors (out of \\(p\\) total) is considered. The best split is chosen exclusively from these \\(m\\) predictors.\n\nTypical Parameter Choice\nA new subset of \\(m\\) predictors is drawn at every split. Common practice sets \\(m \\approx \\sqrt{p}\\). For example, with \\(p = 13\\) predictors, 4 might be considered at each split.\nThis random predictor selection helps ensure the trees are less correlated, thereby improving the variance reduction achieved by averaging."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#boosting-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Boosting",
    "text": "Boosting\n\nLike bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification.\nRecall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\nNotably, each tree is built on a bootstrap data set, independent of the other trees.\nBoosting works in a similar way, except that the trees are grown sequentially: each tree is grown using information from previously grown trees and it is added to the collection of trees if it contributes to the performance improvement."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-gene-expression-data-continued",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#example-gene-expression-data-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Gene expression data continued",
    "text": "Example: Gene expression data continued\nResults from performing boosting and random forests on the fifteen-class gene expression data set in order to predict cancer versus normal.\n\n\nThe test error is displayed as a function of the number of trees.\nFor the two boosted models, \\(\\lambda = 0.01\\). Depth-1 trees, when a single split where applied, slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant.\nThe test error rate for a single tree is 24%."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#variable-importance-measure-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#variable-importance-measure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Variable Importance Measure",
    "text": "Variable Importance Measure\n\n\n\nFor bagged/RF regression trees:\n\nRecord the total amount that the RSS is decreased due to splits over a given predictor, averaged over all \\(B\\) trees.\nA large value indicates an important predictor.\n\nFor bagged/RF classification trees:\n\nAdd up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all \\(B\\) trees.\n\n\n\n\n\nVariable Importance Plot for the Heart Data"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bayesian-additive-regression-trees-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bayesian-additive-regression-trees-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayesian Additive Regression Trees",
    "text": "Bayesian Additive Regression Trees\n\nRecall that bagging and random forests make predictions from an average of regression trees, each of which is built using a random sample of data and/or predictors. Each tree is built separately from the others.\nBy contrast, boosting uses a weighted sum of trees, each of which is constructed by fitting a tree to the residual of the current fit. Thus, each new tree attempts to capture signal that is not yet accounted for by the current set of trees.\nBayesian additive regression trees (BART), an ensemble method that uses decision trees as its building blocks, is related to both random forests and boosting:\n\nEach tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting.\n\nThe main novelty in BART is the way in which new trees are generated.\nBART can be applied to regression, classification, and other problems; we will focus here just on regression."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bayesian-additive-regression-trees-details",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bayesian-additive-regression-trees-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayesian Additive Regression Trees — Details",
    "text": "Bayesian Additive Regression Trees — Details\n\nBART is related to both random forests and boosting:\nEach tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting.\nThe main novelty in BART is the way in which new trees are generated.\nBART can be applied to regression, classification, and other problems; we will focus here just on regression."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-algorithm-the-idea",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-algorithm-the-idea",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART algorithm — the idea",
    "text": "BART algorithm — the idea"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bayesian-additive-regression-trees-some-notation",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bayesian-additive-regression-trees-some-notation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bayesian Additive Regression Trees — Some Notation",
    "text": "Bayesian Additive Regression Trees — Some Notation\n\nWe let \\(K\\) denote the number of regression trees, and \\(B\\) the number of iterations for which the BART algorithm will be run.\nThe notation \\(\\hat{f}^{b}_{k}(x)\\) represents the prediction at \\(x\\) for the \\(k\\)th regression tree used in the \\(b\\)th iteration.\nAt the end of each iteration, the \\(K\\) trees from that iteration will be summed, i.e.,\n\n\n\\[\n  \\hat{f}^{b}(x) = \\sum_{k=1}^{K} \\hat{f}^{b}_{k}(x)\n\\]\nfor \\(b = 1, \\dots, B\\)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-iterations",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-iterations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART Iterations",
    "text": "BART Iterations\n\nIn the first iteration of the BART algorithm, all trees are initialized to have a single root node, with\n\\[\n  \\hat{f}^{1}_{k}(x) = \\frac{1}{nK} \\sum_{i=1}^{n} y_i\n\\]\nrepresenting the mean of the response values divided by the total number of trees. Thus,\n\\[\n  \\hat{f}^{1}(x) = \\sum_{k=1}^{K} \\hat{f}^{1}_{k}(x) = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n\\]\nIn subsequent iterations, BART updates each of the \\(K\\) trees, one at a time. In the \\(b\\)th iteration, to update the \\(k\\)th tree, we subtract from each response value the predictions from all but the \\(k\\)th tree, in order to obtain a partial residual\n\\[\n  r_i = y_i - \\sum_{k' &lt; k} \\hat{f}^{b}_{k'}(x_i) - \\sum_{k' &gt; k} \\hat{f}^{b-1}_{k'}(x_i), \\quad i = 1, \\dots, n\n\\]"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#new-trees-are-chosen-by-perturbations",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#new-trees-are-chosen-by-perturbations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "New trees are chosen by perturbations",
    "text": "New trees are chosen by perturbations\n\nRather than fitting a fresh tree to this partial residual, BART randomly chooses a perturbation to the tree from the previous iteration \\(\\hat{f}^{b-1}_{k}\\) from a set of possible perturbations, favoring ones that improve the fit to the partial residual.\nThere are two components to this perturbation:\n\nWe may change the structure of the tree by adding or pruning branches.\n\nWe may change the prediction in each terminal node of the tree."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#examples-of-possible-perturbations-to-a-tree",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#examples-of-possible-perturbations-to-a-tree",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Examples of possible perturbations to a tree",
    "text": "Examples of possible perturbations to a tree"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#what-does-bart-deliver",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#what-does-bart-deliver",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What does BART Deliver?",
    "text": "What does BART Deliver?\n\nThe output of BART is a collection of prediction models,\n\\[\n  \\hat{f}^{b}(x) = \\sum_{k=1}^{K} \\hat{f}^{b}_{k}(x), \\quad \\text{for } b = 1,2, \\dots, B.\n\\]\nTo obtain a single prediction, we simply take the average after some \\(L\\) burn-in iterations,\n\\[\n  \\hat{f}(x) = \\frac{1}{B - L} \\sum_{b=L+1}^{B} \\hat{f}^{b}(x).\n\\]\n\nThe perturbation-style moves guard against overfitting since they limit how hard we fit the data in each iteration.\nWe can also compute quantities other than the average: for instance, the percentiles of \\(f^{L+1}(x), \\dots, f^{B}(x)\\) provide a measure of uncertainty of the final prediction."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-applied-to-the-heart-data",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-applied-to-the-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART applied to the Heart data",
    "text": "BART applied to the Heart data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(K = 200\\) trees; the number of iterations is increased to 10,000.\nDuring the initial iterations (in gray), the test and training errors jump around a bit.\nAfter this initial burn-in period, the error rates settle down.\nThe tree perturbation process largely avoids overfitting."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-is-a-bayesian-method",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-is-a-bayesian-method",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART is a Bayesian Method",
    "text": "BART is a Bayesian Method\n\nIt turns out that the BART method can be viewed as a Bayesian approach to fitting an ensemble of trees: each time we randomly perturb a tree in order to fit the residuals, we are in fact drawing a new tree from a posterior distribution.\nFurthermore, the BART algorithm can be viewed as a Markov chain Monte Carlo procedure for fitting the BART model.\nWe typically choose large values for \\(B\\) and \\(K\\), and a moderate value for \\(L\\): for instance, \\(K = 200\\), \\(B = 1,000\\), and \\(L = 100\\) are reasonable choices. BART has been shown to have impressive out-of-box performance — that is, it performs well with minimal tuning."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-algorithm-intuition",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-algorithm-intuition",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART Algorithm Intuition",
    "text": "BART Algorithm Intuition"
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-algorithm-intuition-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-algorithm-intuition-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART Algorithm Intuition",
    "text": "BART Algorithm Intuition\n\n\nMultiple Trees in Parallel\n\nChoose a number of trees, \\(K\\) (often in the hundreds), and a total of \\(B\\) iterations.\n\nAll \\(K\\) trees start as a single root node and are updated in parallel through successive iterations.\n\nRandom Perturbations\n\nAt each iteration, each tree is modified via a “perturbation,” which can involve:\n\nAdding or removing a split.\n\nAdjusting the predicted values in terminal nodes.\n\n\nEach tree is updated based on partial residuals, improving how the ensemble fits the data.\n\nIterative Evolution\n\nOver \\(B\\) iterations, trees evolve—some gain new splits, others lose them, and node predictions adjust.\n\nThis process is akin to a Markov chain over tree configurations, guided by how well each tree explains current residuals.\n\nFinal Prediction by Averaging\n\nAfter \\(B\\) iterations, you have an ensemble of \\(K\\) trees that collectively capture the posterior distribution.\n\nThe final prediction at a point \\(x\\) is typically the average of predictions from all \\(K\\) trees at the final iteration (or across multiple post–burn-in iterations)."
  },
  {
    "objectID": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-bayesian-additive-regression-trees-1",
    "href": "lecture_slides/08_tree_based_methods/08_tree_based_methods.html#bart-bayesian-additive-regression-trees-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "BART: Bayesian Additive Regression Trees",
    "text": "BART: Bayesian Additive Regression Trees\n\nRecall that bagging and random forests make predictions from an average of regression trees, each of which is built using a random sample of data and/or predictors. Each tree is built separately from the others.\nBy contrast, boosting uses a weighted sum of trees, each of which is constructed by fitting a tree to the residual of the current fit. Thus, each new tree attempts to capture signal that is not yet accounted for by the current set of trees.\nBayesian additive regression trees (BART), an ensemble method that uses decision trees as its building blocks, is related to both random forests and boosting:\n\nEach tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting.\n\nThe main novelty in BART is the way in which new trees are generated.\nBART can be applied to regression, classification, and other problems; we will focus here just on regression."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#overview",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nSupport Vector Classifier\nSVM with Nonlinear Boundary\nNonlinearities and Kernels\n\n\n\nSVMs: More Than 2 Classes\nSupport Vector versus Logistic Regression\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-machines",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-machines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nHere we approach the two-class classification problem in a direct way:\n\nWe try and find a plane that separates the classes in feature space.\nIf we cannot, we get creative in two ways:\n\nWe soften what we mean by “separates”, and\nWe enrich and enlarge the feature space so that separation is possible."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#what-is-a-hyperplane",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#what-is-a-hyperplane",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is a Hyperplane?",
    "text": "What is a Hyperplane?\nA hyperplane in \\(p\\) dimensions is a flat affine subspace of dimension \\(p - 1\\).\nIn general, the equation for a hyperplane has the form\n\\[\n  \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p = 0\n\\]\n\nIn \\(p = 2\\) dimensions, a hyperplane is a line.\nIf \\(\\beta_0 = 0\\), the hyperplane goes through the origin; otherwise, it does not.\nThe vector \\(\\beta = (\\beta_1, \\beta_2, \\dots, \\beta_p)\\) is called the normal vector — it points in a direction orthogonal to the surface of the hyperplane."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#hyperplane-in-2-dimensions",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#hyperplane-in-2-dimensions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hyperplane in 2 Dimensions",
    "text": "Hyperplane in 2 Dimensions"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#separating-hyperplanes",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#separating-hyperplanes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Separating Hyperplanes",
    "text": "Separating Hyperplanes\n\n\nIf \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\\), then \\(f(X) &gt; 0\\) for points on one side of the hyperplane, and \\(f(X) &lt; 0\\) for points on the other.\nIf we code the colored points as \\(Y_i = +1\\) for blue and \\(Y_i = -1\\) for purple, then if \\(Y_i \\cdot f(X_i) &gt;0\\) for all \\(i\\), \\(f(X) = 0\\) defines a separating hyperplane."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#maximal-margin-classifier",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#maximal-margin-classifier",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Maximal Margin Classifier",
    "text": "Maximal Margin Classifier\nThe idea is: Among all separating hyperplanes, find the one that makes the biggest gap or margin between the two classes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstrained optimization problem:\n\\[\n\\text{maximize } M \\quad \\beta_0, \\beta_1, \\dots, \\beta_p\n\\]\nsubject to:\n\\[\n\\sum_{j=1}^p \\beta_j^2 = 1,\n\\]\n\\[\ny_i (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\geq M \\quad \\text{for all } i = 1, \\dots, N.\n\\]\nThus, we determine the hyperplane parameters that yield the largest possible margin \\(m\\) while ensuring that every data point lies at least \\(m\\) units away from the hyperplane. This guarantees a maximum separation between the two classes, leading to a robust classification model."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#non-separable-data",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#non-separable-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non-separable Data",
    "text": "Non-separable Data\n\nThe data are not separable by a linear boundary. This is often the case, unless \\(N &lt; p\\)."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#noisy-data",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#noisy-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Noisy Data",
    "text": "Noisy Data\n\nSometimes the data are separable, but noisy. In the right plot we add only one new data point in it results in a big change in the hyperplane that separates the two classes. This can lead to a poor solution for the maximal-margin classifier.\n\nThe support vector classifier maximizes a soft margin and is a good option to deal both with non-separable and noisy data."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector Classifier",
    "text": "Support Vector Classifier\n\n\\[\n\\text{maximize } M \\quad \\beta_0, \\beta_1, \\dots, \\beta_p, \\epsilon_1, \\dots, \\epsilon_n\n\\]\nsubject to:\n\\[\n\\sum_{j=1}^p \\beta_j^2 = 1,\n\\]\n\\[\ny_i (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\geq M(1 - \\epsilon_i),\n\\]\n\\[\n\\epsilon_i \\geq 0, \\quad \\sum_{i=1}^n \\epsilon_i \\leq C.\n\\]"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#c-is-a-regularization-parameter",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#c-is-a-regularization-parameter",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "\\(C\\) is a Regularization Parameter",
    "text": "\\(C\\) is a Regularization Parameter\n\nThis layout demonstrates the effect of the regularization parameter \\(C\\) on the SVM classifier with four subplots, each representing a different \\(C\\) value. Adjustments to annotations or layout can be made as needed. Let me know if further modifications are required!"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#linear-boundary-can-fail",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#linear-boundary-can-fail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Boundary Can Fail",
    "text": "Linear Boundary Can Fail\nSometimes a linear boundary simply won’t work, no matter what value of \\(C\\). What to do?"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#feature-expansion",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#feature-expansion",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Feature Expansion",
    "text": "Feature Expansion\n\nEnlarge the space of features by including transformations; e.g. \\(X_1^2, X_1^3, X_1X_2, X_1X_2^2, \\dots\\). Hence go from a \\(p\\)-dimensional space to a \\(M &gt; p\\)-dimensional space (not the same \\(M\\) as we used for margin).\nFit a support-vector classifier in the enlarged space.\nThis results in non-linear decision boundaries in the original space.\nExample: Suppose we use \\((X_1, X_2, X_1^2, X_2^2, X_1X_2)\\) instead of just \\((X_1, X_2)\\). Then the decision boundary would be of the form: \\[\n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1X_2 = 0\n\\]\n\nThis leads to nonlinear decision boundaries in the original space (quadratic conic sections)."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#cubic-polynomials",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#cubic-polynomials",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cubic Polynomials",
    "text": "Cubic Polynomials\nHere we use a basis expansion of cubic polynomials. From 2 variables to 9:\n\\[\n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 + \\beta_6 X_1^3 + \\beta_7 X_2^3 + \\beta_8 X_1^2 X_2 + \\beta_9 X_1 X_2^2 = 0\n\\]\n\n\nThe support-vector classifier in the enlarged space solves the problem in the lower-dimensional space."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#nonlinearities-and-kernels",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#nonlinearities-and-kernels",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nonlinearities and Kernels",
    "text": "Nonlinearities and Kernels\n\nPolynomials (especially high-dimensional ones) get wild rather fast.\nThere is a more elegant and controlled way to introduce nonlinearities in support-vector classifiers — through the use of kernels.\nBefore we discuss these, we must understand the role of inner products in support-vector classifiers."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#inner-products-and-support-vectors",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#inner-products-and-support-vectors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Inner Products and Support Vectors",
    "text": "Inner Products and Support Vectors\n\nThe decision function of a Linear SVC is expressed as:\n\\[\nf(x) = \\beta_0 + \\sum_{i=1}^{n} \\alpha_i \\langle x, x_i \\rangle\n\\]\nwhere:\n\n\\(f(x)\\) represents the classification function that determines whether a new point \\(x\\) belongs to one class or another.\n\\(\\beta_0\\) is the intercept (bias term).\n\\(\\alpha_i\\) are the Lagrange multipliers, which determine the influence of each training example on the decision boundary.\n\\(x_i\\): Training Data Points: \\(x_i\\) represents a training example**, which is one of the points in the training dataset.\n\nEach \\(x_i\\) is a feature vector in a \\(p\\)-dimensional space \\(x_i = (x_{i1}, x_{i2}, \\dots, x_{ip})\\)\n\n\\(x\\): A New Input Example (Test Point): \\(x\\) is a new data point** (not necessarily in the training set) for which we want to make a prediction.\n\nLike \\(x_i\\), it is also a feature vector in the same \\(p\\)-dimensional space \\(x = (x_1, x_2, \\dots, x_p)\\)\nThe classifier evaluates \\(f(x)\\) to determine the class of \\(x\\).\nThe decision function \\(f(x)\\) compares this new point to all training points \\(x_i\\) using their inner products.\n\n\\(\\langle x, x_i \\rangle\\) represents the inner product (dot product) between the input vector \\(x\\) and each training point \\(x_i\\), which captures the similarity between them.\n\\(n\\) is the total number of training points."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#kernels-and-support-vector-machines",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#kernels-and-support-vector-machines",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Kernels and Support Vector Machines",
    "text": "Kernels and Support Vector Machines\n\nIf we can compute inner products between observations, we can fit a SV classifier.\nSome special kernel functions can do this for us. E.g., \\[\nK(x_i, x_{i'}) = \\left(1 + \\sum_{j=1}^p x_{ij} x_{i'j}\\right)^d\n\\] computes the inner products needed for \\(d\\)-dimensional polynomials — \\(\\binom{p+d}{d}\\) basis functions!\nThe solution has the form: \\[\nf(x) = \\beta_0 + \\sum_{i \\in S} \\hat{\\alpha}_i K(x, x_i).\n\\]"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#radial-kernel",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#radial-kernel",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Radial Kernel",
    "text": "Radial Kernel\n\nOne of the most popular kernel!\n\\[\nK(x_i, x_{i'}) = \\exp\\left(-\\gamma \\sum_{j=1}^p (x_{ij} - x_{i'j})^2 \\right).\n\\]\nwhere \\(\\gamma\\) is a tuning parameter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nf(x) = \\beta_0 + \\sum_{i \\in S} \\hat{\\alpha}_i K(x, x_i).\n\\] - The solid black line encloses a region classified as one class (pink points).\n\nThe dashed lines in the image represent decision boundaries created by the Radial Basis Function (RBF) kernel in a Support Vector Machine (SVM). These boundaries separate different regions of classification based on the nonlinear transformation performed by the Radial Kernel.\nImplicit feature space; very high dimensional.\nControls variance by squashing down most dimensions severely."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-heart-data",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-heart-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Heart Data",
    "text": "Example: Heart Data\n\nROC curve is obtained by changing the threshold \\(t\\) in \\(\\hat{f}(X) &gt; t\\), and recording false positive and true positive rates as \\(t\\) varies. Here we see ROC curves on training data."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-continued-heart-test-data",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-continued-heart-test-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example Continued: Heart Test Data",
    "text": "Example Continued: Heart Test Data"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#svms-more-than-2-classes",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#svms-more-than-2-classes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "SVMs: More Than 2 Classes?",
    "text": "SVMs: More Than 2 Classes?\nThe SVM as defined works for \\(K = 2\\) classes. What do we do if we have \\(K &gt; 2\\) classes?\nOVA\nOne versus All. Fit \\(K\\) different 2-class SVM classifiers \\(\\hat{f}_k(x)\\), \\(k = 1, \\dots, K\\); each class versus the rest. Classify \\(x^*\\) to the class for which \\(\\hat{f}_k(x^*)\\) is largest.\nOVO\nOne versus One. Fit all \\(\\binom{K}{2}\\) pairwise classifiers \\(\\hat{f}_{k\\ell}(x)\\). Classify \\(x^*\\) to the class that wins the most pairwise competitions.\nWhich to choose? If \\(K\\) is not too large, use OVO."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-versus-logistic-regression",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-versus-logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector versus Logistic Regression?",
    "text": "Support Vector versus Logistic Regression?\nWith \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\\), we can rephrase support-vector classifier optimization as:\n\\[\n\\text{minimize}_{\\beta_0, \\beta_1, \\dots, \\beta_p} \\left\\{ \\sum_{i=1}^n \\max \\big[ 0, 1 - y_i f(x_i) \\big] + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right\\}\n\\]\n\nThis has the form loss plus penalty.\n\nThe loss is known as the hinge loss.\n\nVery similar to the loss in logistic regression (negative log-likelihood)."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#which-to-use-svm-or-logistic-regression",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#which-to-use-svm-or-logistic-regression",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Which to Use: SVM or Logistic Regression",
    "text": "Which to Use: SVM or Logistic Regression\n\nWhen classes are (nearly) separable, SVM does better than Logistic Regression. So does LDA.\nWhen not, Logistic Regression (with ridge penalty) and SVM are very similar.\nIf you wish to estimate probabilities, Logistic Regression is the choice.\nFor nonlinear boundaries, kernel SVMs are popular. Can use kernels with Logistic Regression and LDA as well, but computations are more expensive."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#summary",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nHyperplanes and Normal Vectors\n\nA hyperplane is a \\((p-1)\\)-dimensional boundary in \\(p\\)-dimensional space.\n\nIts orientation is determined by the normal vector \\(\\beta\\), which is orthogonal to the hyperplane.\n\nMaximal Margin and Soft Margins\n\nThe maximal margin classifier seeks the widest possible gap between classes.\n\nSoft margins allow some misclassifications (via slack variables \\(\\epsilon_i\\)) when data are not perfectly separable.\n\n\\(C\\) as a Regularization Parameter\n\nLarge \\(C\\) → Strict margin (less regularization), fewer misclassifications, but higher risk of overfitting.\n\nSmall \\(C\\) → Softer margin (more regularization), more misclassifications allowed, but often better generalization.\n\nKernels for Nonlinear Boundaries\n\nKernel functions (e.g., polynomial, radial basis) map data into higher-dimensional spaces for nonlinear decision boundaries.\n\nThe inner product \\(\\langle x, x_i \\rangle\\) becomes \\(K(x, x_i)\\) in kernel SVMs, enabling complex separations.\n\n\n\n\n\n\nSupport Vectors and Sparsity\n\nOnly a subset of training points (support vectors) have \\(\\alpha_i &gt; 0\\).\n\nThis sparsity makes SVMs computationally efficient and robust to outliers.\n\nSVM vs. Logistic Regression\n\nSVM uses hinge loss (piecewise linear), focusing on margin violations.\n\nLogistic Regression uses log-likelihood loss, penalizing all misclassifications smoothly.\n\nWhen classes are nearly separable, SVM often outperforms logistic regression (and LDA).\n\nLogistic regression provides probabilistic outputs; SVM is more geometric.\n\nMulticlass Extensions\n\nImplemented via One-vs-All (OVA) or One-vs-One (OVO) schemes for \\(K&gt;2\\) classes.\n\nStandardization\n\nStandardize features so that no single variable dominates the margin.\n\nSimilar to Ridge and Lasso, SVM is sensitive to feature scales.\n\n\nKey Message:\nSVMs excel at maximizing margins and can handle complex decision boundaries via kernels. The choice of \\(C\\), kernel, and feature scaling profoundly affects performance and generalization."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#hyperplane-in-2-dimensions-1",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#hyperplane-in-2-dimensions-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hyperplane in 2 Dimensions",
    "text": "Hyperplane in 2 Dimensions\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure demonstrates how hyperplanes separate data in classification or regression contexts: each point’s location relative to the hyperplane is captured by the signed distance.\n\nHyperplane in 2D (Blue Line)\n\nIn two dimensions, a hyperplane is simply a straight line.\n\nThe blue line in the figure is defined by the equation\n\\[\n\\beta_1 x_1 + \\beta_2 x_2 - 6 = 0.\n\\]\nAll points \\((x_1, x_2)\\) satisfying this equation lie exactly on the hyperplane.\n\n\n\n\nNormal Vector (Red Arrow)\n\nThe red arrow represents the normal vector \\(\\beta = (\\beta_1, \\beta_2)\\).\n\nThis vector is orthogonal (perpendicular) to the hyperplane, meaning it points in the direction that is shortest from the line to any point off the line.\nThe values \\(\\beta_1 = 0.8\\) and \\(\\beta_2 = 0.6\\) are displayed in the bottom right corner.\n\nSince the sum of their squares equals one (\\(0.8^2 + 0.6^2 = 1\\)), this vector is a unit vector.\n\nRole of the Normal Vector in Classification\n\n\nThe normal vector determines:\n\nThe orientation of the hyperplane (i.e., which way it is “tilted” in space).\nWhich side of the hyperplane a point belongs to.\nThe shortest distance of a point from the hyperplane.\nIf we substitute a point \\(X = (X_1, X_2, ..., X_p)\\) into the hyperplane equation: \\[\nf(X) = \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p - 6\n\\]\n\nIf \\(f(X) &gt; 0\\) → The point is on one side of the hyperplane.\nIf \\(f(X) &lt; 0\\) → The point is on the opposite side.\nIf \\(f(X) = 0\\) → The point lies on the hyperplane.\n\n\nIllustrated Examples\n\nOne point is 1.6, meaning it is 1.6 units away from the hyperplane in the direction of the normal vector.\n\nAnother point is -4, indicating it is 4 units away on the opposite side of the hyperplane.\n\nPoints on the blue line always have a function value of 0 because they satisfy \\(\\beta_1 x_1 + \\beta_2 x_2 - 6 = 0\\).\n\nKey Insight\n\n\nIn higher dimensions, the same idea generalizes: a hyperplane is a \\((p-1)\\)-dimensional flat surface in \\(p\\)-dimensional space, and its normal vector determines both orientation and distance computations."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#separating-hyperplanes-1",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#separating-hyperplanes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Separating Hyperplanes",
    "text": "Separating Hyperplanes\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft Plot:\n\nThe dataset contains two classes (blue and mauve points).\nMultiple candidate hyperplanes are shown as black lines.\nThese hyperplanes attempt to separate the two classes, but not all do so optimally.\nRight Plot:\n\nA single separating hyperplane is shown.\nThe background is shaded to indicate decision regions:\n\nThe blue-shaded region contains points classified as blue.\nThe mauve-shaded region contains points classified as mauve.\n\n\n\n\nA general equation of a hyperplane in a \\(p\\)-dimensional space: \\[\n  f(X) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\n\nThis function \\(f(X)\\) determines which side of the hyperplane a data point falls on:\n\nIf \\(f(X) &gt; 0\\) → The point lies on one side of the hyperplane.\nIf \\(f(X) &lt; 0\\) → The point lies on the other side.\nIf \\(f(X) = 0\\) → The point lies exactly on the hyperplane.\n\n\nDefining a Separating Hyperplane in Classification\n\nIn SVM classification, each data point \\(X_i\\) has a corresponding label \\(Y_i\\), where:\n\n\\(Y_i = +1\\) for one class (blue).\n\\(Y_i = -1\\) for the other class (mauve).\n\nA perfectly separating hyperplane must satisfy the condition: \\[\nY_i \\cdot f(X_i) &gt; 0 \\quad \\text{for all } i\n\\]\nThis means:\n\nIf \\(Y_i = +1\\), then \\(f(X_i)\\) must be positive (point is on the correct side).\nIf \\(Y_i = -1\\), then \\(f(X_i)\\) must be negative (point is on the correct side).\nIf this condition holds for all points, then the hyperplane separates the two classes."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier-1",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector Classifier",
    "text": "Support Vector Classifier\n\n\\[\n\\text{maximize } M \\quad \\beta_0, \\beta_1, \\dots, \\beta_p, \\epsilon_1, \\dots, \\epsilon_n\n\\]\nsubject to:\n\\[\n\\sum_{j=1}^p \\beta_j^2 = 1,\n\\]\n\\[\ny_i (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\geq M(1 - \\epsilon_i),\n\\]\n\\[\n\\epsilon_i \\geq 0, \\quad \\sum_{i=1}^n \\epsilon_i \\leq C.\n\\]"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier-2",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector Classifier",
    "text": "Support Vector Classifier\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft Plot: Data is Separable, but Soft Margin is Used\nInstead of using the narrowest possible margin, a wider margin has been enforced.\n\nSome points end up on the wrong side of their margin.\nSpecifically, point 8 (blue) and another pink point violate their respective margin constraints.\n\nThis illustrates the trade-off of using a soft margin: a wider margin often improves generalization but allows for some classification errors.\n\nThe margin is determined by more than just the closest support vectors, incorporating the influence of points slightly beyond the margin.\n\nRight Plot: Data is Not Linearly Separable\nA soft-margin classifier is essential, as we must allow some misclassification.\n\nSome blue points are inside the margin or even misclassified (wrong side of the decision boundary).\nSome pink points also violate the margin.\n\nThe soft margin formulation enables a balance between margin width and classification accuracy, allowing the model to find a reasonable decision boundary even when perfect separation is not possible.\n\n\n\nMathematical Formulation of the Soft-Margin SVM \\[\n\\max_{β_0, β_1, ..., β_p, \\epsilon_1, ..., \\epsilon_n} M\n\\]\nSubject to the constraints:\n\n\nNormalization Constraint:\n\\[\n\\sum_{j=1}^{p} \\beta_j^2 = 1\n\\]\n\nEnsures that the normal vector \\(\\beta\\) is a unit vector, standardizing the optimization process.\n\nMargin Constraint with Slack Variables:\n\\[\ny_i (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\geq M(1 - \\epsilon_i)\n\\]\n\nNormally, we require that each point lies at least \\(M\\) units away from the hyperplane.\nHowever, the slack variables \\(\\epsilon_i\\) allow some flexibility:\n\nIf \\(\\epsilon_i = 0\\), the point is correctly classified outside the margin.\nIf \\(0 &lt; \\epsilon_i &lt; 1\\), the point is inside the margin but still on the correct side.\nIf \\(\\epsilon_i \\geq 1\\), the point is misclassified.\n\n\nTotal Slack Budget Constraint:\n\\[\n\\sum_{i=1}^{n} \\epsilon_i \\leq C, \\quad \\epsilon_i \\geq 0\n\\]\n\nThe parameter \\(C\\) controls the total amount of slack allowed in the model:\n\nLarge \\(C\\) → Less tolerance for margin violations (closer to hard-margin SVM).\nSmall \\(C\\) → More tolerance for misclassification and margin violations.\n\n\n\n\nIntuition Behind Soft Margins:\n\nIn a hard-margin SVM, no points are allowed inside the margin.\nIn soft-margin SVM, we introduce slack variables \\(\\epsilon_i\\) to allow some violations of the margin.\nThis provides a regularization effect:\n\nPrevents overfitting in cases where a perfect separation exists but may be too rigid.\nAllows classification in non-linearly separable cases by balancing misclassification and margin width."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier---ilustration",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier---ilustration",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector Classifier - Ilustration",
    "text": "Support Vector Classifier - Ilustration\n\n\n\n\n\nLeft Plot: Data is Separable, but Soft Margin is Used\nInstead of using the narrowest possible margin, a wider margin has been enforced.\n\nSome points end up on the wrong side of their margin.\nSpecifically, point 8 (blue) and another pink point violate their respective margin constraints.\n\nThis illustrates the trade-off of using a soft margin: a wider margin often improves generalization but allows for some classification errors.\n\nThe margin is determined by more than just the closest support vectors, incorporating the influence of points slightly beyond the margin.\n\n\n\n\nRight Plot: Data is Not Linearly Separable\nA soft-margin classifier is essential, as we must allow some misclassification.\n\nSome blue points are inside the margin or even misclassified (wrong side of the decision boundary).\nSome pink points also violate the margin.\n\nThe soft margin formulation enables a balance between margin width and classification accuracy, allowing the model to find a reasonable decision boundary even when perfect separation is not possible."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier---ilustration-details",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-classifier---ilustration-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector Classifier - Ilustration Details",
    "text": "Support Vector Classifier - Ilustration Details\n\n\n\n\nMathematical Formulation of the Soft-Margin SVM \\[\n\\max_{β_0, β_1, ..., β_p, \\epsilon_1, ..., \\epsilon_n} M\n\\]\nSubject to the 3 constraints:\n\n\nNormalization Constraint:\n\\[\n\\sum_{j=1}^{p} \\beta_j^2 = 1\n\\]\n\nEnsures that the normal vector \\(\\beta\\) is a unit vector, standardizing the optimization process.\n\nMargin Constraint with Slack Variables:\n\\[\ny_i (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\geq M(1 - \\epsilon_i)\n\\]\n\nNormally, we require that each point lies at least \\(M\\) units away from the hyperplane.\nHowever, the slack variables \\(\\epsilon_i\\) allow some flexibility:\n\nIf \\(\\epsilon_i = 0\\), the point is correctly classified outside the margin.\nIf \\(0 &lt; \\epsilon_i &lt; 1\\), the point is inside the margin but still on the correct side.\nIf \\(\\epsilon_i \\geq 1\\), the point is misclassified.\n\n\n\n\n\nTotal Slack Budget Constraint:\n\\[\n\\sum_{i=1}^{n} \\epsilon_i \\leq C, \\quad \\epsilon_i \\geq 0\n\\]\n\nThe parameter \\(C\\) controls the total amount of slack allowed in the model:\n\nLarge \\(C\\) → Less tolerance for margin violations (closer to hard-margin SVM).\nSmall \\(C\\) → More tolerance for misclassification and margin violations.\n\n\n\n\nIntuition Behind Soft Margins:\n\nIn a hard-margin SVM, no points are allowed inside the margin.\nIn soft-margin SVM, we introduce slack variables \\(\\epsilon_i\\) to allow some violations of the margin.\nThis provides a regularization effect:\n\nPrevents overfitting in cases where a perfect separation exists but may be too rigid.\nAllows classification in non-linearly separable cases by balancing misclassification and margin width."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#c-is-a-regularization-parameter---ilustration",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#c-is-a-regularization-parameter---ilustration",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "\\(C\\) is a Regularization Parameter - Ilustration",
    "text": "\\(C\\) is a Regularization Parameter - Ilustration"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#c-is-a-regularization-parameter---ilustration-details",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#c-is-a-regularization-parameter---ilustration-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "\\(C\\) is a Regularization Parameter - Ilustration Details",
    "text": "\\(C\\) is a Regularization Parameter - Ilustration Details\n\n\n\nThe four plots illustrate different values of \\(C\\), showing the trade-off between margin width and classification flexibility.\n\nTop-Left Plot (Largest \\(C\\))\n\nHere, \\(C\\) is very large, meaning the model strongly penalizes misclassified points.\nAs a result, the decision boundary is rigid, and only a few points violate the margin.\nMany points are on the wrong side of their respective margins, but the model still prioritizes keeping the margin as wide as possible while satisfying the constraint.\n\nTop-Right & Bottom-Left Plots (Intermediate \\(C\\))\n\nAs \\(C\\) is decreased, the model allows more margin violations (misclassified or within-margin points).\nThe margin becomes tighter because the optimization prioritizes correcting misclassified points rather than maximizing margin width.\nMore points now contribute to determining the decision boundary, leading to increased stability.\n\nBottom-Right Plot (Smallest \\(C\\))\n\nWith a small \\(C\\), the model tolerates many margin violations and allows points to be misclassified.\nThe decision boundary adjusts to reduce the number of misclassified points, even at the cost of a narrower margin.\nThis results in a more flexible model that is less sensitive to small variations in data.\n\n\n\n\nThe Bias-Variance Trade-off in SVM\nLarge \\(C\\) (Less Regularization, Harder Margin) → Low Bias, High Variance\n\nFewer margin violations → Less flexibility, but possibly overfitting.\nModel depends on a few critical support vectors, making it more sensitive to small changes in data.\n\nSmall \\(C\\) (More Regularization, Softer Margin) → High Bias, Low Variance\n\nMore margin violations → More flexibility, but possibly underfitting.\nMany points influence the decision boundary, leading to a more stable model.\n\nStandardizing Variables is Important in SVM\nSVM treats all variables equally, meaning features with different scales can disproportionately influence the margin.\nStandardizing (e.g., transforming variables to have zero mean and unit variance) ensures that:\n\nThe distance metric (Euclidean distance) is meaningful.\nThe optimization process does not favor one feature due to its larger magnitude.\n\nThis is the same reason why standardization is important in Ridge and Lasso regression.\nConclusion\n\\(C\\) controls the trade-off between margin width and misclassification tolerance.\nA higher \\(C\\) enforces a stricter margin (less regularization) but may overfit the data.\nA lower \\(C\\) allows more misclassification (more regularization) but may generalize better.\nStandardization is crucial for SVM to ensure fair treatment of all variables."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#linear-boundary-can-fail-1",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#linear-boundary-can-fail-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Linear Boundary Can Fail",
    "text": "Linear Boundary Can Fail\nSometimes a linear boundary simply won’t work, no matter what value of \\(C\\). What to do?"
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#nonlinearities-and-kernels-1",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#nonlinearities-and-kernels-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Nonlinearities and Kernels",
    "text": "Nonlinearities and Kernels\n\nPolynomials are not always the greatest choice (especially high-dimensional ones) as they get wild rather fast.\nThere is a more elegant and controlled way to introduce nonlinearities in support-vector classifiers — through the use of kernels.\nBefore we discuss these, we must understand the role of inner products in support-vector classifiers."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#inner-products",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#inner-products",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Inner Products",
    "text": "Inner Products\n\nThe inner product (dot product) between two vectors \\(x_i\\) and \\(x_i'\\):\n\\[\n\\langle x_i, x_i' \\rangle = \\sum_{j=1}^{p} x_{ij} x_{i'j}\n\\]\nwhere:\n\n\\(x_i\\) and \\(x_i'\\) are two vectors in \\(p\\)-dimensional space.\nThe summation runs over all dimensions \\(j\\) from 1 to \\(p\\).\nEach term \\(x_{ij} x_{i'j}\\) represents the product of corresponding components of the two vectors.\nInterpretation: The inner product (dot product) measures the similarity between two vectors in a given space. It is computed by multiplying corresponding elements of the two vectors and summing up the results.\nIf \\(x_i\\) and \\(x_i'\\) are aligned (pointing in the same direction) → The inner product is large and positive.\nIf \\(x_i\\) and \\(x_i'\\) are perpendicular (orthogonal) → The inner product is zero.\nIf \\(x_i\\) and \\(x_i'\\) point in opposite directions → The inner product is negative."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#computational-efficiency-using-inner-products",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#computational-efficiency-using-inner-products",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Computational Efficiency: Using Inner Products",
    "text": "Computational Efficiency: Using Inner Products\nInstead of directly computing \\(\\beta\\) coefficients for each feature, the SVC relies only on pairwise inner products between training points. This reduces the complexity of parameter estimation.\n\nTo estimate \\(\\alpha_1, \\dots, \\alpha_n\\) and \\(\\beta_0\\), we only need the \\(\\binom{n}{2}\\) pairwise inner products \\(\\langle x_i, x_i \\rangle\\).\nThis simplifies the optimization problem, making it computationally feasible even for high-dimensional datasets."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#the-role-of-support-vectors",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#the-role-of-support-vectors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Role of Support Vectors",
    "text": "The Role of Support Vectors\n\n\nOne of the key properties of Support Vector Machines (SVMs) is sparsity in the solution.\nMost of the \\(\\hat{\\alpha}_i\\) values will be zero, because the SVM optimization process ignores non-critical points.\n\nOnly a small subset of training points (the support vectors) have \\(\\alpha_i &gt; 0\\) and influence the decision boundary. This sparsity property makes SVMs both efficient and interpretable.\n\nThus, the decision function simplifies to: \\[\nf(x) = \\beta_0 + \\sum_{i \\in S} \\hat{\\alpha}_i \\langle x, x_i \\rangle\n\\]\nwhere:\n\n\\(S\\) is the support set, the set of indices \\(i\\) such that \\(\\hat{\\alpha}_i &gt; 0\\).\nThese points, called support vectors, are the only points that determine the optimal hyperplane.\n\nThis means that most training points do not influence the classifier, making SVMs computationally efficient because only a small subset of training points (the support vectors) needs to be stored and used for classification.\nWhy is This Important?\n\nSparsity: Since only a few points contribute to the final decision boundary, SVMs are efficient and robust to noise.\nComputational Simplicity: Instead of solving for \\(n\\) parameters explicitly, we only compute the inner products for support vectors, making the method scalable to large datasets.\nExtensibility to Nonlinear SVMs: This formulation is the foundation for Kernel SVMs, where the inner product \\(\\langle x, x_i \\rangle\\) is replaced by a nonlinear kernel function to allow nonlinear decision boundaries."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-heart-data-1",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-heart-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Heart Data",
    "text": "Example: Heart Data\n\nHere we see ROC curves on training data.\n\n\n\n\n\n\n\n\n\n\nLeft: We compare a Linear Suport Vector Machine with Linear Discriminant Analysis (LDA).\nRight: We compare a Linear Suport Vector Machine with the SVM using a radial kernel with different values of \\(\\gamma\\).The larger \\(\\gamma\\) the more wiggly the decision boundary."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-heart-data-2",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#example-heart-data-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Heart Data",
    "text": "Example: Heart Data\nHere we see ROC curves on testing data."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#svms-more-than-2-classes-1",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#svms-more-than-2-classes-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "SVMs: More Than 2 Classes?",
    "text": "SVMs: More Than 2 Classes?\nThe SVM as defined works for \\(K = 2\\) classes. What do we do if we have \\(K &gt; 2\\) classes?\n\nThere are two famous options:\n\nOVA (One versus All): Fit \\(K\\) different 2-class SVM classifiers \\(\\hat{f}_k(x)\\), \\(k = 1, \\dots, K\\); each class versus the rest. Classify \\(x^*\\) to the class for which \\(\\hat{f}_k(x^*)\\) is largest.\nOVO (One versus One): Fit all \\(\\binom{K}{2}\\) pairwise classifiers \\(\\hat{f}_{k\\ell}(x)\\). Classify \\(x^*\\) to the class that wins the most pairwise competitions.\n\nWhich to choose?\n\nIf \\(K\\) is not too large, use OVO."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-versus-logistic-regression-1",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-versus-logistic-regression-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector versus Logistic Regression",
    "text": "Support Vector versus Logistic Regression\n\nWith \\(f(X) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\\), we can rephrase support-vector classifier optimization as:\n\\[\n\\text{minimize}_{\\beta_0, \\beta_1, \\dots, \\beta_p} \\left\\{ \\sum_{i=1}^n \\max \\big[ 0, 1 - y_i f(x_i) \\big] + \\lambda \\sum_{j=1}^p \\beta_j^2 \\right\\}\n\\]\nwhere:\n\n\\(f(X) = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_pX_p\\) represents the linear decision function.\n\\(y_i\\) is the class label (\\(+1\\) or \\(-1\\)).\nThe term \\(\\max [0, 1 - y_i f(x_i)]\\) is the hinge loss, which penalizes misclassified or margin-violating points.\nThe term \\(\\lambda \\sum_{j=1}^{p} \\beta_j^2\\) is a regularization term (often L2 regularization), which helps control the model complexity and prevent overfitting.\nThis structure is loss plus penalty, meaning SVM aims to minimize classification errors while maintaining a large margin between classes."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-versus-logistic-regression-2",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-versus-logistic-regression-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector versus Logistic Regression",
    "text": "Support Vector versus Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot compares the loss functions of SVM (hinge loss) and logistic regression (negative log-likelihood loss).\nSVM hinge loss (black line):\n\nZero loss when \\(y_i f(x_i) \\geq 1\\) (correctly classified with a sufficient margin).\nLinear penalty for points within the margin (\\(0 &lt; y_i f(x_i) &lt; 1\\)).\nConstant penalty for misclassified points (\\(y_i f(x_i) &lt; 0\\)).\n\n\n\n\nLogistic Regression loss (green line):\n\nSmooth, continuously decreasing function.\nAssigns nonzero loss to all points, meaning it never fully ignores correctly classified points.\n\nThe key difference:\n\nSVM loss is piecewise linear and focuses on margin violations.\nLogistic regression loss is smooth and penalizes all points proportionally.\n\nMain Takeaway\n\nBoth SVM and Logistic Regression use a loss function + regularization framework.\nSVM uses hinge loss, which only cares about points inside or outside the margin.\nLogistic regression uses log-likelihood loss, which penalizes all points, even well-classified ones.\nSVM is more robust to outliers because it ignores well-classified points beyond the margin.\nLogistic Regression is probabilistic, while SVM is geometric, focusing on maximizing the margin."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-machines-1",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#support-vector-machines-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nHere we approach the two-class classification problem in a direct way:\n\nWe try and find a plane that separates the classes in feature space.\nIf we cannot, we get creative in two ways:\n\nWe soften what we mean by “separates”, and\nWe enrich and enlarge the feature space so that separation is possible."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#hyperplane-in-2-dimensions-details",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#hyperplane-in-2-dimensions-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hyperplane in 2 Dimensions: Details",
    "text": "Hyperplane in 2 Dimensions: Details\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure demonstrates how hyperplanes separate data in classification or regression contexts: each point’s location relative to the hyperplane is captured by the signed distance.\n\nHyperplane in 2D (Blue Line)\n\nIn two dimensions, a hyperplane is simply a straight line.\n\nThe blue line in the figure is defined by the equation\n\\[\n\\beta_1 x_1 + \\beta_2 x_2 - 6 = 0.\n\\]\nAll points \\((x_1, x_2)\\) satisfying this equation lie exactly on the hyperplane.\n\n\n\n\nNormal Vector (Red Arrow)\n\nThe red arrow represents the normal vector \\(\\beta = (\\beta_1, \\beta_2)\\).\n\nThis vector is orthogonal (perpendicular) to the hyperplane, meaning it points in the direction that is shortest from the line to any point off the line.\nThe values \\(\\beta_1 = 0.8\\) and \\(\\beta_2 = 0.6\\) are displayed in the bottom right corner.\n\nSince the sum of their squares equals one (\\(0.8^2 + 0.6^2 = 1\\)), this vector is a unit vector.\n\nRole of the Normal Vector in Classification\n\n\nThe normal vector determines:\n\nThe orientation of the hyperplane (i.e., which way it is “tilted” in space).\nWhich side of the hyperplane a point belongs to.\nThe shortest distance of a point from the hyperplane.\nIf we substitute a point \\(X = (X_1, X_2, ..., X_p)\\) into the hyperplane equation: \\[\nf(X) = \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p - 6\n\\]\n\nIf \\(f(X) &gt; 0\\) → The point is on one side of the hyperplane.\nIf \\(f(X) &lt; 0\\) → The point is on the opposite side.\nIf \\(f(X) = 0\\) → The point lies on the hyperplane.\n\n\nIllustrated Examples\n\nOne point is 1.6, meaning it is 1.6 units away from the hyperplane in the direction of the normal vector.\n\nAnother point is -4, indicating it is 4 units away on the opposite side of the hyperplane.\n\nPoints on the blue line always have a function value of 0 because they satisfy \\(\\beta_1 x_1 + \\beta_2 x_2 - 6 = 0\\).\n\nKey Insight\n\n\nIn higher dimensions, the same idea generalizes: a hyperplane is a \\((p-1)\\)-dimensional flat surface in \\(p\\)-dimensional space, and its normal vector determines both orientation and distance computations."
  },
  {
    "objectID": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#separating-hyperplanes-details",
    "href": "lecture_slides/09_support_vector_machines/09_support_vector_machines.html#separating-hyperplanes-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Separating Hyperplanes: Details",
    "text": "Separating Hyperplanes: Details\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeft Plot:\n\nThe dataset contains two classes (blue and mauve points).\nMultiple candidate hyperplanes are shown as black lines.\nThese hyperplanes attempt to separate the two classes, but not all do so optimally.\nRight Plot:\n\nA single separating hyperplane is shown.\nThe background is shaded to indicate decision regions:\n\nThe blue-shaded region contains points classified as blue.\nThe mauve-shaded region contains points classified as mauve.\n\n\n\n\nA general equation of a hyperplane in a \\(p\\)-dimensional space: \\[\n  f(X) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\\]\n\nThis function \\(f(X)\\) determines which side of the hyperplane a data point falls on:\n\nIf \\(f(X) &gt; 0\\) → The point lies on one side of the hyperplane.\nIf \\(f(X) &lt; 0\\) → The point lies on the other side.\nIf \\(f(X) = 0\\) → The point lies exactly on the hyperplane.\n\n\nDefining a Separating Hyperplane in Classification\n\nIn SVM classification, each data point \\(X_i\\) has a corresponding label \\(Y_i\\), where:\n\n\\(Y_i = +1\\) for one class (blue).\n\\(Y_i = -1\\) for the other class (mauve).\n\nA perfectly separating hyperplane must satisfy the condition: \\[\nY_i \\cdot f(X_i) &gt; 0 \\quad \\text{for all } i\n\\]\nThis means:\n\nIf \\(Y_i = +1\\), then \\(f(X_i)\\) must be positive (point is on the correct side).\nIf \\(Y_i = -1\\), then \\(f(X_i)\\) must be negative (point is on the correct side).\nIf this condition holds for all points, then the hyperplane separates the two classes."
  },
  {
    "objectID": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-loocv-to-specify-lambda",
    "href": "lecture_slides/07_beyond_linearity/07_beyond_linearity.html#smoothing-splines-loocv-to-specify-lambda",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Smoothing Splines: LOOCV to Specify \\(\\lambda\\)",
    "text": "Smoothing Splines: LOOCV to Specify \\(\\lambda\\)\n\n\n\n\nWe can use Cross-validation to choose \\(\\lambda\\).\n\n\nFor each candidate \\(\\lambda\\), we leave out one data point at a time, fit the spline, and measure the prediction error on the held-out point.\n\n\nMathematical Form\n\n\n\\[\n     \\text{RSS}_{\\text{cv}}(\\lambda)\n       = \\sum_{i=1}^n \\left(y_i - \\hat{g}_\\lambda^{(-i)}(x_i)\\right)^2,\n\\]\n\n\nwhere \\(\\hat{g}_\\lambda^{(-i)}\\) denotes the spline fitted without the \\(i\\)-th data point.\nIn practice, this can be computed more efficiently via the influence matrix \\(\\mathbf{S}_\\lambda\\):\n\n\n\\[\n       \\text{RSS}_{\\text{cv}}(\\lambda)\n         = \\sum_{i=1}^n \\left[\\frac{y_i - \\hat{g}_\\lambda(x_i)}{1 - \\{\\mathbf{S}_\\lambda\\}_{ii}} \\right]^2.\n\\]\n\n\n\nAdvantages\n\n\nData-Driven: No need to guess \\(\\text{df}\\) or \\(\\lambda\\); we let cross-validation find the best trade-off.\nRobust: Tends to choose a value that generalizes well to new data.\n\n\nPractical Tip\n\n\nEvaluate \\(\\text{RSS}_{\\text{cv}}\\) for a grid of \\(\\lambda\\) values (e.g., \\(\\lambda \\in \\{0.01, 0.1, 1, 10\\}\\)) and pick the one that minimizes the cross-validation error."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#overview",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nUnsupervised Learning\nPrincipal Components Analysis\nMatrix Completion and Missing Values\n\n\n\nClustering\nK-means clustering\nHierarchical Clustering\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#unsupervised-learning",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#unsupervised-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nUnsupervised vs Supervised Learning:\n\nMost of this course focuses on supervised learning methods such as regression and classification.\nIn that setting we observe both a set of features \\(X_1, X_2, \\ldots, X_p\\) for each object, as well as a response or outcome variable \\(Y\\). The goal is then to predict \\(Y\\) using \\(X_1, X_2, \\ldots, X_p\\).\nHere we instead focus on unsupervised learning, where we observe only the features \\(X_1, X_2, \\ldots, X_p\\). We are not interested in prediction, because we do not have an associated response variable \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#the-goals-of-unsupervised-learning",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#the-goals-of-unsupervised-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Goals of Unsupervised Learning",
    "text": "The Goals of Unsupervised Learning\n\nThe goal is to discover interesting things about the measurements: is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations?\nWe discuss two methods:\n\nprincipal components analysis, a tool used for data visualization or data pre-processing before supervised techniques are applied, and\nclustering, a broad class of methods for discovering unknown subgroups in data."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#the-challenge-of-unsupervised-learning",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#the-challenge-of-unsupervised-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Challenge of Unsupervised Learning",
    "text": "The Challenge of Unsupervised Learning\n\nUnsupervised learning is more subjective than supervised learning, as there is no simple goal for the analysis, such as prediction of a response.\nBut techniques for unsupervised learning are of growing importance in a number of fields:\n\nsubgroups of breast cancer patients grouped by their gene expression measurements,\ngroups of shoppers characterized by their browsing and purchase histories,\nmovies grouped by the ratings assigned by movie viewers."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-advantage",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-advantage",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Another advantage",
    "text": "Another advantage\n\nIt is often easier to obtain unlabeled data — from a lab instrument or a computer — than labeled data, which can require human intervention.\nFor example, it is difficult to automatically assess the overall sentiment of a movie review: is it favorable or not?"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\n\nPCA produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nApart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-details",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Analysis: details",
    "text": "Principal Components Analysis: details\n\n\nThe first principal component of a set of features \\(X_1, X_2, \\ldots, X_p\\) is the normalized linear combination of the features \\[\n  Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\ldots + \\phi_{p1}X_p\n\\]\nHere:\n\n\\(Z_1\\) is the first principal component.\n\\(X_1, X_2, ..., X_p\\) are the original features (variables).\n\\(\\phi_{j1}\\) (where \\(j = 1, 2, \\dots, p\\)) are the loadings or weights assigned to each feature in the linear combination.\nThese weights determine how much each feature contributes to the principal component.\n\nThe loadings \\(\\phi_{11}, \\ldots, \\phi_{p1}\\) of the first principal component make up the principal component loading vector, \\[\n  \\phi_1 = (\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1})^T.\n\\]\n\nBy normalized, we mean that \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\). This ensures that the sum of squared weights equals 1, preventing any arbitrary scaling and ensuring the principal components remain standardized."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-example",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PCA: example",
    "text": "PCA: example\n\nThe population size (pop) and ad spending (ad) for 100 different cities are shown as purple circles. The green solid line indicates the first principal component direction, and the blue dashed line indicates the second principal component direction."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-of-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-of-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Computation of Principal Components",
    "text": "Computation of Principal Components\n\nSuppose we have a \\(n \\times p\\) data set \\(\\mathbf{X}\\). Since we are only interested in variance, we assume that each of the variables in \\(\\mathbf{X}\\) has been centered to have mean zero (that is, the column means of \\(\\mathbf{X}\\) are zero).\nWe then look for the linear combination of the sample feature values of the form \\[\nz_{i1} = \\phi_{11}x_{i1} + \\phi_{21}x_{i2} + \\ldots + \\phi_{p1}x_{ip} \\tag{1}\n\\] for \\(i = 1, \\ldots, n\\) that has the largest sample variance, subject to the constraint that \\(\\sum_{j=1}^p \\phi_{j1}^2 = 1\\).\nSince each of the \\(x_{ij}\\) has mean zero, then so does \\(z_{i1}\\) (for any values of \\(\\phi_{j1}\\)). Hence the sample variance of the \\(z_{i1}\\) can be written as \\[\n  \\frac{1}{n} \\sum_{i=1}^n z_{i1}^2.\n\\]"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Computation: continued",
    "text": "Computation: continued\n\nPlugging in (1) the first principal component loading vector solves the optimization problem\n\n\\[\n  \\text{maximize}_{\\phi_{11}, \\ldots, \\phi_{p1}} \\frac{1}{n} \\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{j1} x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^p \\phi_{j1}^2 = 1.\n\\]\n\nThis problem can be solved via a singular-value decomposition of the matrix \\(\\mathbf{X}\\), a standard technique in linear algebra.\nWe refer to \\(Z_1\\) as the first principal component, with realized values \\(z_{11}, \\ldots, z_{n1}\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#geometry-of-pca",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#geometry-of-pca",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Geometry of PCA",
    "text": "Geometry of PCA\n\nThe loading vector \\(\\phi_1\\) with elements \\(\\phi_{11}, \\phi_{21}, \\ldots, \\phi_{p1}\\) defines a direction in feature space along which the data vary the most.\nIf we project the \\(n\\) data points \\(x_1, \\ldots, x_n\\) onto this direction, the projected values are the principal component scores \\(z_{11}, \\ldots, z_{n1}\\) themselves."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Further principal components",
    "text": "Further principal components\n\nThe second principal component is the linear combination of \\(X_1, \\ldots, X_p\\) that has maximal variance among all linear combinations that are uncorrelated with \\(Z_1\\).\nThe second principal component scores \\(z_{12}, z_{22}, \\ldots, z_{n2}\\) take the form \\[\n  z_{i2} = \\phi_{12}x_{i1} + \\phi_{22}x_{i2} + \\ldots + \\phi_{p2}x_{ip},\n\\] where \\(\\phi_2\\) is the second principal component loading vector, with elements \\(\\phi_{12}, \\phi_{22}, \\ldots, \\phi_{p2}\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Further principal components: continued",
    "text": "Further principal components: continued\n\nIt turns out that constraining \\(Z_2\\) to be uncorrelated with \\(Z_1\\) is equivalent to constraining the direction \\(\\phi_2\\) to be orthogonal (perpendicular) to the direction \\(\\phi_1\\). And so on.\nThe principal component directions \\(\\phi_1, \\phi_2, \\phi_3, \\ldots\\) are the ordered sequence of right singular vectors of the matrix \\(\\mathbf{X}\\), and the variances of the components are \\(\\frac{1}{n}\\) times the squares of the singular values. There are at most \\(\\min(n - 1, p)\\) principal components."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#illustration",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#illustration",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Illustration",
    "text": "Illustration\n\nUSArrests data: For each of the fifty states in the United States, the data set contains the number of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. We also record UrbanPop (the percent of the population in each state living in urban areas).\nThe principal component score vectors have length \\(n = 50\\), and the principal component loading vectors have length \\(p = 4\\).\nPCA was performed after standardizing each variable to have mean zero and standard deviation one."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#usarrests-data-pca-plot",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#usarrests-data-pca-plot",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "USArrests data: PCA plot",
    "text": "USArrests data: PCA plot\n\nThe PCA plot shows the first and second principal components for the USArrests dataset, with arrows indicating the loadings for the variables UrbanPop, Rape, Assault, and Murder. State names are displayed based on their principal component scores."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#figure-details",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#figure-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Figure details",
    "text": "Figure details"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-loadings",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-loadings",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PCA loadings",
    "text": "PCA loadings\n\n\n\n\nPC1\nPC2\n\n\n\n\nMurder\n0.5358995\n-0.4181809\n\n\nAssault\n0.5831836\n-0.1879856\n\n\nUrbanPop\n0.2781909\n0.8728062\n\n\nRape\n0.5434321\n0.1673186"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-interpretation-of-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-interpretation-of-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Another Interpretation of Principal Components",
    "text": "Another Interpretation of Principal Components\n\nThe left plot shows a 3D visualization with projections onto the first two principal components, while the right plot displays the data in 2D using the first and second principal components."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-find-the-hyperplane-closest-to-the-observations",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-find-the-hyperplane-closest-to-the-observations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PCA find the hyperplane closest to the observations",
    "text": "PCA find the hyperplane closest to the observations\n\nThe first principal component loading vector has a very special property: it defines the line in \\(p\\)-dimensional space that is closest to the \\(n\\) observations (using average squared Euclidean distance as a measure of closeness).\nThe notion of principal components as the dimensions that are closest to the \\(n\\) observations extends beyond just the first principal component.\nFor instance, the first two principal components of a data set span the plane that is closest to the \\(n\\) observations, in terms of average squared Euclidean distance."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#scaling-of-the-variables-matters",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#scaling-of-the-variables-matters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Scaling of the variables matters",
    "text": "Scaling of the variables matters\n\nIf the variables are in different units, scaling each to have standard deviation equal to one is recommended.\nIf they are in the same units, you might or might not scale the variables."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Proportion Variance Explained",
    "text": "Proportion Variance Explained\n\nTo understand the strength of each component, we are interested in knowing the proportion of variance explained (PVE) by each one.\nThe total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as\n\n\\[\n  \\sum_{j=1}^p \\text{Var}(X_j) = \\sum_{j=1}^p \\frac{1}{n} \\sum_{i=1}^n x_{ij}^2,\n\\]\nand the variance explained by the \\(m\\)-th principal component is\n\\[\n  \\text{Var}(Z_m) = \\frac{1}{n} \\sum_{i=1}^n z_{im}^2.\n\\]\n\nIt can be shown that\n\n\\[\n  \\sum_{j=1}^p \\text{Var}(X_j) = \\sum_{m=1}^M \\text{Var}(Z_m),\n\\]\nwith \\(M = \\min(n-1, p)\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Proportion Variance Explained: continued",
    "text": "Proportion Variance Explained: continued\n\nTherefore, the PVE of the \\(m\\)-th principal component is given by the positive quantity between 0 and 1:\n\n\\[\n  \\frac{\\sum_{i=1}^n z_{im}^2}{\\sum_{j=1}^p \\sum_{i=1}^n x_{ij}^2}.\n\\]\n\nThe PVEs sum to one. We sometimes display the cumulative PVEs."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#how-many-principal-components-should-we-use",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#how-many-principal-components-should-we-use",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How many principal components should we use?",
    "text": "How many principal components should we use?\nIf we use principal components as a summary of our data, how many components are sufficient?\n\nThere is no simple answer to this question as the response will be conditioned on the data.\nThe “scree plot” on the previous slide can be used as a guide: we look for an “elbow”.\n\nEssentially, this means that we can determine the optimal number of principal components by identifying the point where the marginal gain in the Proportion of Variance Explained (PVE) becomes negligible as additional components are added."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Clustering",
    "text": "Clustering\n\nClustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set.\nWe seek a partition of the data into distinct groups so that the observations within each group are quite similar to each other.\nTo make this concrete, we must define what it means for two or more observations to be similar or different.\nIndeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-vs-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#pca-vs-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PCA vs Clustering",
    "text": "PCA vs Clustering\n\nPCA looks for a low-dimensional representation of the observations that explains a good fraction of the variance.\nClustering looks for homogeneous subgroups among the observations."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering-for-market-segmentation",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering-for-market-segmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Clustering for Market Segmentation",
    "text": "Clustering for Market Segmentation\n\nSuppose we have access to a large number of measurements (e.g., median household income, occupation, distance from nearest urban area, and so forth) for a large number of people.\nOur goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product.\nThe task of performing market segmentation amounts to clustering the people in the data set."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#two-clustering-methods",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#two-clustering-methods",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Two clustering methods",
    "text": "Two clustering methods\n\nIn K-means clustering, we seek to partition the observations into a pre-specified number of clusters.\nIn hierarchical clustering, we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a dendrogram, that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to \\(n\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-means clustering",
    "text": "K-means clustering\n\nA simulated data set with 150 observations in 2-dimensional space. Panels show the results of applying K-means clustering with different values of \\(K\\), the number of clusters. The color of each observation indicates the cluster to which it was assigned using the K-means clustering algorithm. Note that there is no ordering of the clusters, so the cluster coloring is arbitrary. These cluster labels were not used in clustering; instead, they are the outputs of the clustering procedure."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-k-means-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-k-means-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of K-means clustering",
    "text": "Details of K-means clustering\nLet \\(C_1, \\ldots, C_K\\) denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:\n\n\\(C_1 \\cup C_2 \\cup \\ldots \\cup C_K = \\{1, \\ldots, n\\}\\). In other words, each observation belongs to at least one of the \\(K\\) clusters.\n\\(C_k \\cap C_{k'} = \\emptyset\\) for all \\(k \\neq k'\\). In other words, the clusters are non-overlapping: no observation belongs to more than one cluster.\n\nFor instance, if the \\(i\\)-th observation is in the \\(k\\)-th cluster, then \\(i \\in C_k\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-k-means-clustering-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-k-means-clustering-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of K-means clustering: continued",
    "text": "Details of K-means clustering: continued\n\nThe idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible.\nThe within-cluster variation for cluster \\(C_k\\) is a measure \\(\\text{WCV}(C_k)\\) of the amount by which the observations within a cluster differ from each other.\nHence, we want to solve the problem:\n\n\\[\n  \\text{minimize}_{C_1, \\ldots, C_K} \\left\\{ \\sum_{k=1}^K \\text{WCV}(C_k) \\right\\}.\n\\]\n\nIn words, this formula says that we want to partition the observations into \\(K\\) clusters such that the total within-cluster variation, summed over all \\(K\\) clusters, is as small as possible."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#how-to-define-within-cluster-variation",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#how-to-define-within-cluster-variation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How to Define Within-Cluster Variation?",
    "text": "How to Define Within-Cluster Variation?\n\nTypically, we use Euclidean distance: \\[\n  \\text{WCV}(C_k) = \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2, \\tag{3}\n\\] where \\(|C_k|\\) denotes the number of observations in the \\(k\\)-th cluster.\nCombining (2) and (3) gives the optimization problem that defines K-means clustering: \\[\n  \\text{minimize}_{C_1, \\ldots, C_K} \\left\\{ \\sum_{k=1}^K \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2 \\right\\}. \\tag{4}\n\\]"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-algorithm",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-Means Clustering Algorithm",
    "text": "K-Means Clustering Algorithm\n\nRandomly assign a number, from 1 to \\(K\\), to each of the observations. These serve as initial cluster assignments for the observations.\nIterate until the cluster assignments stop changing:\n2.a For each of the \\(K\\) clusters, compute the cluster centroid. The \\(k\\)-th cluster centroid is the vector of the \\(p\\) feature means for the observations in the \\(k\\)-th cluster.\n2.b Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#properties-of-the-algorithm",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#properties-of-the-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Properties of the Algorithm",
    "text": "Properties of the Algorithm\n\nThis algorithm is guaranteed to decrease the value of the objective function (4) at each step. Why?\nNote that \\[\n\\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2 = 2 \\sum_{i \\in C_k} \\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2,\n\\] where \\(\\bar{x}_{kj} = \\frac{1}{|C_k|} \\sum_{i \\in C_k} x_{ij}\\) is the mean for feature \\(j\\) in cluster \\(C_k\\).\nHowever, it is not guaranteed to give the global minimum."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nTop left: The observations are shown.\nTop center: In Step 1 of the algorithm, each observation is randomly assigned to a cluster.\nTop right: In Step 2(a), the cluster centroids are computed. These are shown as large colored disks. Initially, the centroids are almost completely overlapping because the initial cluster assignments were chosen at random.\nBottom left: In Step 2(b), each observation is assigned to the nearest centroid.\nBottom center: Step 2(a) is once again performed, leading to new cluster centroids.\nBottom right: The results obtained after 10 iterations."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-different-starting-values",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-different-starting-values",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Different Starting Values",
    "text": "Example: Different Starting Values"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nK-means clustering was performed six times on the data from the previous figure with \\(K = 3\\), each time with a different random assignment of the observations in Step 1 of the K-means algorithm.\nAbove each plot is the value of the objective (4).\nThree different local optima were obtained, one of which resulted in a smaller value of the objective and provides better separation between the clusters.\nThose labeled in red all achieved the same best solution, with an objective value of 235.8."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nK-means clustering requires us to pre-specify the number of clusters \\(K\\). This can be a disadvantage (later we discuss strategies for choosing \\(K\\)).\nHierarchical clustering is an alternative approach which does not require that we commit to a particular choice of \\(K\\).\nIn this section, we describe bottom-up or agglomerative clustering. This is the most common type of hierarchical clustering and refers to the fact that a dendrogram is built starting from the leaves and combining clusters up to the trunk."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea",
    "text": "Hierarchical Clustering: The Idea\nBuilds a hierarchy in a “bottom-up” fashion…\nStep 1"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-2",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Step 2)",
    "text": "Hierarchical Clustering: The Idea (Step 2)\nMerging Closest Observations"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-3",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Step 3)",
    "text": "Hierarchical Clustering: The Idea (Step 3)\nMerging Another Closest Pair"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-4",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Step 4)",
    "text": "Hierarchical Clustering: The Idea (Step 4)\nExpanding the Hierarchy"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-final-step",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-final-step",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Final Step)",
    "text": "Hierarchical Clustering: The Idea (Final Step)\nSingle Cluster Representation"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-algorithm",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-algorithm",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering Algorithm",
    "text": "Hierarchical Clustering Algorithm\n\nThe approach in words:\n\nStart with each point in its own cluster.\nIdentify the closest two clusters and merge them.\nRepeat.\nEnds when all points are in a single cluster."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#an-example",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#an-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "An Example",
    "text": "An Example\n\n\n45 observations generated in 2-dimensional space.\nIn reality, there are three distinct classes, shown in separate colors.\nHowever, we will treat these class labels as unknown and will seek to cluster the observations in order to discover the classes from the data."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#application-of-hierarchical-clustering",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#application-of-hierarchical-clustering",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Application of Hierarchical Clustering",
    "text": "Application of Hierarchical Clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach dendrogram represents different thresholds for determining clusters. The dashed horizontal lines indicate where the data is split into distinct clusters.\nLeft: Dendrogram obtained from hierarchically clustering the data from the previous slide, using complete linkage and Euclidean distance. The entire tree is one cluster.\n\n\n\nCenter: The dendrogram in the center panel, cut at a height of 9 (indicated by the dashed line). This cut results in two distinct clusters, shown in different colors.\nRight: The dendrogram from the left-hand panel, now cut at a height of 5. This cut results in three distinct clusters, shown in different colors. Note that the colors were not used in clustering, but are simply used for display purposes in this figure."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure-2",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#details-of-previous-figure-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Previous Figure",
    "text": "Details of Previous Figure\n\nLeft: Dendrogram obtained from hierarchically clustering the data from the previous slide, using complete linkage and Euclidean distance.\nCenter: The dendrogram from the left-hand panel, cut at a height of 9 (indicated by the dashed line). This cut results in two distinct clusters, shown in different colors.\nRight: The dendrogram from the left-hand panel, now cut at a height of 5. This cut results in three distinct clusters, shown in different colors. Note that the colors were not used in clustering, but are simply used for display purposes in this figure."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#types-of-linkage",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#types-of-linkage",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Types of Linkage",
    "text": "Types of Linkage\n\nLinkage methods determine how the distance between two clusters is measured when forming a hierarchical clustering dendrogram.\n\n\n\n\n\n\n\nLinkage\nDescription\n\n\n\n\nComplete\nComputes the maximum pairwise dissimilarity between clusters. This method tends to create compact clusters but is sensitive to outliers.\n\n\nSingle\nComputes the minimum pairwise dissimilarity between clusters. It can lead to elongated, chain-like clusters and is sensitive to noise.\n\n\nAverage\nComputes the average pairwise dissimilarity between clusters. It provides a balance between complete and single linkage, making it widely used.\n\n\nCentroid\nComputes the dissimilarity between centroids (mean vectors) of clusters. However, it may cause inversions, where clusters are not merged in a monotonic order - a situation where a pair of clusters are merged in a non-monotonic way, meaning that the hierarchical structure does not maintain a proper increasing order of distances."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#choice-of-dissimilarity-measure",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#choice-of-dissimilarity-measure",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choice of Dissimilarity Measure",
    "text": "Choice of Dissimilarity Measure\n\n\n\n\nSo far, we have used Euclidean distance to defince the clusters.\nAn alternative is correlation-based distance, which considers two observations to be similar if their features are highly correlated.\nThis is an unusual use of correlation, which is normally computed between variables; here, it is computed between the observation profiles for each pair of observations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure shows a sequence of points over time for 3 different observations. Each curve represents one such series, and the goal is to assess how similar the patterns are across these series.\nIf we use Euclidean distance, we compare observations based on their absolute coordinates. Here, the purple and gold observations are numerically closer together, while the green observation is farther apart from both.\nHowever, if we use correlation-based distance, we look at how the values fluctuate over the index rather than their absolute differences. In this case, the green and gold series exhibit similar trends, moving up and down together, making them more correlated than the purple and green observations.\nThe key idea is that correlation focuses on shape rather than scale. If your problem requires detecting similar trends and patterns rather than absolute differences in values, correlation-based distance is a better choice than Euclidean distance."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#practical-issues",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#practical-issues",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Practical Issues",
    "text": "Practical Issues\n\nScaling of the variables matters! Should the observations or features first be standardized in some way?\n\nFor instance, maybe the variables should be centered to have mean zero and scaled to have a standard deviation of one.\n\nIn the case of hierarchical clustering:\n\nWhat dissimilarity measure should be used?\nWhat type of linkage should be used?\n\nHow many clusters to choose? (in both K-means or hierarchical clustering):\n\nDifficult problem. No agreed-upon method. See Elements of Statistical Learning, Chapter 13, for more details.\n\nWhich features should we use to drive the clustering?"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-breast-cancer-microarray-study",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-breast-cancer-microarray-study",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: Breast Cancer Microarray Study",
    "text": "Example: Breast Cancer Microarray Study\n\n“Repeated observation of breast tumor subtypes in independent gene expression data sets;” Sorlie et al., PNAS 2003.\nGene expression measurements for approximately ~8000 genes, for each of 88 breast cancer patients.\nAverage linkage, correlation metric.\nClustered samples using 500 intrinsic genes:\n\nEach woman was measured before and after chemotherapy.\nIntrinsic genes have the smallest within/between variation."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#conclusions",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#conclusions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusions",
    "text": "Conclusions\n\nUnsupervised learning is important for understanding the variation and grouping structure of a set of unlabeled data, and can be a useful pre-processor for supervised learning.\nIt is intrinsically more difficult than supervised learning because there is no gold standard (like an outcome variable) and no single objective (like test set accuracy).\nIt is an active field of research, with many recently developed tools such as self-organizing maps, independent components analysis, and spectral clustering.\nSee The Elements of Statistical Learning, chapter 14."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-completion-and-missing-values-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-completion-and-missing-values-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Matrix Completion and Missing Values",
    "text": "Matrix Completion and Missing Values\n\nIt is often the case that data matrices X have missing entries, often represented by NAs (Not Available).\nThis is a nuisance, since many of our modeling procedures, such as linear regression and GLMs, require complete data.\nSometimes imputation is the prediction problem! — as in recommender systems.\nOne simple approach is mean imputation — replace missing values for a variable by the mean of the non-missing entries.\n\nThis ignores the correlations among variables; we should be able to exploit these correlations when imputing missing values.\n\nWe assume values are missing at random; i.e., the missingness should not be informative.\nLet’s see an imputation approach based on principal components."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#recommender-systems",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#recommender-systems",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommender Systems",
    "text": "Recommender Systems\n\n\n\n\n\n\n\n\n\n\n\nNetflix users rate movies they have seen, usually a very small fraction of available movies.\nThe Netflix Competition data set had 400,000 users and over 18,000 movies. Only 2% of the matrix had numbers in it and the rest were missing.\nPredicting missing ratings provides a way to recommend movies to users. Matrix completion is one of the primary tools."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#recommender-systems-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#recommender-systems-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommender Systems",
    "text": "Recommender Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer\nJerry Maguire\nOceans\nRoad to Perdition\nA Fortunate Man\nCatch Me If You Can\nDriving Miss Daisy\nThe Two Popes\nThe Laundromat\nCode 8\nThe Social Network\n…\n\n\n\n\nCustomer 1\n.\n.\n.\n.\n.\n.\n.\n4\n.\n.\n…\n\n\nCustomer 2\n.\n3\n.\n.\n.\n.\n.\n3\n.\n3\n…\n\n\nCustomer 3\n.\n.\n.\n2\n4\n.\n.\n.\n.\n.\n…\n\n\nCustomer 4\n3\n.\n.\n.\n.\n2\n.\n.\n.\n.\n…\n\n\nCustomer 5\n5\n1\n.\n.\n4\n.\n.\n.\n.\n.\n…\n\n\nCustomer 6\n.\n.\n.\n2\n.\n.\n4\n.\n.\n.\n…\n\n\nCustomer 7\n.\n.\n.\n.\n5\n.\n.\n.\n3\n.\n…\n\n\nCustomer 8\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n…\n\n\nCustomer 9\n3\n.\n.\n.\n.\n.\n.\n5\n.\n1\n…\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\nNetflix users rate movies they have seen, usually a very small fraction of available movies.\nPredicting missing ratings provides a way to recommend movies to users. Matrix completion is one of the primary tools."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-approximation-via-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-approximation-via-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Matrix Approximation via Principal Components",
    "text": "Matrix Approximation via Principal Components\n\nIt is possible to have an interpretation of principal components in terms of matrix approximation: \\[\n\\text{minimize}_{\\mathbf{A} \\in \\mathbb{R}^{n \\times M}, \\mathbf{B} \\in \\mathbb{R}^{p \\times M}} \\left\\{ \\sum_{j=1}^p \\sum_{i=1}^n \\left( x_{ij} - \\sum_{m=1}^M a_{im}b_{jm} \\right)^2 \\right\\}.\n\\]\n\\(\\mathbf{A}\\) is an \\(n \\times M\\) matrix whose \\((i, m)\\) element is \\(a_{im}\\), and \\(\\mathbf{B}\\) is a \\(p \\times M\\) matrix whose \\((j, m)\\) element is \\(b_{jm}\\).\nIt can be shown that for any value of \\(M\\), the first \\(M\\) principal components provide a solution: \\[\n\\hat{a}_{im} = z_{im} \\quad \\text{and} \\quad \\hat{b}_{jm} = \\phi_{jm}.\n\\]\nBut what to do if the matrix has missing elements?"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-completion-via-principal-components",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#matrix-completion-via-principal-components",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Matrix Completion via Principal Components",
    "text": "Matrix Completion via Principal Components\nWe pose instead a modified version of the approximation criterion: \\[\n\\text{minimize}_{\\mathbf{A} \\in \\mathbb{R}^{n \\times M}, \\mathbf{B} \\in \\mathbb{R}^{p \\times M}} \\left\\{ \\sum_{(i,j) \\in \\mathcal{O}} \\left( x_{ij} - \\sum_{m=1}^M a_{im}b_{jm} \\right)^2 \\right\\},\n\\] where \\(\\mathcal{O}\\) is the set of all observed pairs of indices \\((i, j)\\), a subset of the possible \\(n \\times p\\) pairs.\n\nOnce we solve this problem:\n\nWe can estimate a missing observation \\(x_{ij}\\) using: \\[\n\\hat{x}_{ij} = \\sum_{m=1}^M \\hat{a}_{im}\\hat{b}_{jm},\n\\] where \\(\\hat{a}_{im}\\) and \\(\\hat{b}_{jm}\\) are the \\((i,m)\\) and \\((j,m)\\) elements of the solution matrices \\(\\mathbf{\\hat{A}}\\) and \\(\\mathbf{\\hat{B}}\\).\nWe can (approximately) recover the \\(M\\) principal component scores and loadings, as if data were complete."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#iterative-algorithm-for-matrix-completion",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#iterative-algorithm-for-matrix-completion",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Iterative Algorithm for Matrix Completion",
    "text": "Iterative Algorithm for Matrix Completion\n\n\n\n\nInitialize: create a complete data matrix \\(\\tilde{\\mathbf{X}}\\) by filling in the missing values using mean imputation.\nRepeat: steps (a)–(c) until the objective in (c) fails to decrease:\n\n\n\n\\[\n\\text{minimize}_{\\mathbf{A} \\in \\mathbb{R}^{n \\times M}, \\mathbf{B} \\in \\mathbb{R}^{p \\times M}} \\left\\{ \\sum_{j=1}^p \\sum_{i=1}^n \\left( \\tilde{x}_{ij} - \\sum_{m=1}^M a_{im}b_{jm} \\right)^2 \\right\\},\n\\] by computing the principal components of \\(\\tilde{\\mathbf{X}}\\).\n\n\n\nFor each missing entry \\((i, j) \\notin \\mathcal{O}\\), set: \\[\n\\tilde{x}_{ij} \\leftarrow \\sum_{m=1}^M \\hat{a}_{im} \\hat{b}_{jm}.\n\\]\nCompute the objective: \\[\n\\sum_{(i,j) \\in \\mathcal{O}} \\left( x_{ij} - \\sum_{m=1}^M \\hat{a}_{im} \\hat{b}_{jm} \\right)^2.\n\\]\n\n\n\nReturn the estimated missing entries \\(\\tilde{x}_{ij}\\), \\((i, j) \\notin \\mathcal{O}\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-usaarrests-data",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-usaarrests-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: USAarrests Data",
    "text": "Example: USAarrests Data\n\n\n\n\n\n\n\n\n\n\n\n\nHere \\(\\mathbf{X}\\) has 50 rows (states) and four columns: Murder, Assault, Rape, and UrbanPop.\n\nWe selected 20 states at random, and for each, we selected one of the variables at random, and set its value to NA.\nUsed \\(M = 1\\) principal component in the algorithm.\nCorrelation: 0.63 between original and imputed values."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-continued",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example — Continued",
    "text": "Example — Continued\n\nThe USAarrests data has only four variables, which is on the low end for this method to work well. For this reason, for this demonstration we randomly set at most one variable per state to be missing and only used \\(M = 1\\) principal component.\nIn general, in order to apply this algorithm, we must select \\(M\\), the number of principal components to use for the imputation.\nOne approach is to randomly set to NA some elements that were actually observed, and select \\(M\\) based on how well those known values are recovered. This is closely related to the validation-set approach seen in Chapter 5.\nsoftImpute package in R implements matrix completion algorithms and can manage Netflix-scale matrices."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#summary",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nXXXX\n\n\n\n\n\nXXXX"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#unsupervised-vs-supervised-learning",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#unsupervised-vs-supervised-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Unsupervised vs Supervised Learning",
    "text": "Unsupervised vs Supervised Learning\n\nMost of this course focuses on supervised learning methods such as regression and classification.\nIn that setting we observe both a set of features \\(X_1, X_2, \\ldots, X_p\\) for each object, as well as a response or outcome variable \\(Y\\). The goal is then to predict \\(Y\\) using \\(X_1, X_2, \\ldots, X_p\\).\nHere we instead focus on unsupervised learning, where we observe only the features \\(X_1, X_2, \\ldots, X_p\\). We are not interested in prediction, because we do not have an associated response variable \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\n\nPCA produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nApart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-of-principal-components-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#computation-of-principal-components-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Computation of Principal Components",
    "text": "Computation of Principal Components\n\nPlugging in (1) the first principal component loading vector solves the optimization problem \\[\n  \\text{maximize}_{\\phi_{11}, \\ldots, \\phi_{p1}} \\frac{1}{n} \\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{j1} x_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^p \\phi_{j1}^2 = 1.\n\\]\nThis problem can be solved via a singular-value decomposition of the matrix \\(\\mathbf{X}\\), a standard technique in linear algebra.\nWe refer to \\(Z_1\\) as the first principal component, with realized values \\(z_{11}, \\ldots, z_{n1}\\). And \\(Z_1\\) can be assumed as a new predictor to use, with values for each of the \\(n\\) observations in the dataset."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#further-principal-components-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Further principal components",
    "text": "Further principal components\n\nIt turns out that constraining \\(Z_2\\) to be uncorrelated with \\(Z_1\\) is equivalent to constraining the direction \\(\\phi_2\\) to be orthogonal (perpendicular) to the direction \\(\\phi_1\\). And so on.\nThe principal component directions \\(\\phi_1, \\phi_2, \\phi_3, \\ldots\\) are the ordered sequence of right singular vectors of the matrix \\(\\mathbf{X}\\), and the variances of the components are \\(\\frac{1}{n}\\) times the squares of the singular values. There are at most \\(\\min(n - 1, p)\\) principal components."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-example-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Analysis: Example",
    "text": "Principal Components Analysis: Example\n\nUSArrests data: For each of the fifty states in the United States, the data set contains the number of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. We also record UrbanPop (the percent of the population in each state living in urban areas).\nThe principal component score vectors have length \\(n = 50\\), and the principal component loading vectors have length \\(p = 4\\).\nPCA was performed after standardizing each variable to have mean zero and standard deviation one."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-example-2",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#principal-components-analysis-example-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Principal Components Analysis: Example",
    "text": "Principal Components Analysis: Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis figure is known as a biplot, because it displays both the principal component scores and the principal component loadings.\nThe PCA plot shows the first and second principal components for the USArrests dataset, with arrows indicating the loadings for the variables UrbanPop, Rape, Assault, and Murder. State names are displayed based on their principal component scores.\nThe blue state names represent the scores for the first two principal components.\nThe orange arrows indicate the first two principal component loading vectors (with axes on the top and right). For example, the loading for Rape on the first component is 0.54, and its loading on the second principal component is 0.17 [the word Rape is centered at the point (0.54, 0.17)].\nWe can conclude that the First Principal Component (PC1) primarily captures violence, as all three crime-related variables appear on the right side of the plot. Meanwhile, the Second Principal Component (PC2) appears to represent the share of the urban population, as this variable is positioned at the top of the plot."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-interpretation-of-principal-components-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#another-interpretation-of-principal-components-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Another Interpretation of Principal Components",
    "text": "Another Interpretation of Principal Components\n\nThese two plots are equivalent. The left plot shows a 3D visualization with projections onto the first two principal components, while the right plot displays the data in 2D using the first and second principal components."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#scaling-of-the-variables-matters-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#scaling-of-the-variables-matters-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Scaling of the variables matters",
    "text": "Scaling of the variables matters\n\nIf the variables are in different units, scaling each to have standard deviation equal to one is recommended.\nIf they are in the same units, you might or might not scale the variables."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Proportion Variance Explained",
    "text": "Proportion Variance Explained\n\n\nTo understand the strength of each component, we are interested in knowing the proportion of variance explained (PVE) by each one.\nThe total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as \\[\n  \\sum_{j=1}^p \\text{Var}(X_j) = \\sum_{j=1}^p \\frac{1}{n} \\sum_{i=1}^n x_{ij}^2,\n\\] and the variance explained by the \\(m\\)-th principal component is \\[\n  \\text{Var}(Z_m) = \\frac{1}{n} \\sum_{i=1}^n z_{im}^2.\n\\]\nIt can be shown that \\[\n  \\sum_{j=1}^p \\text{Var}(X_j) = \\sum_{m=1}^M \\text{Var}(Z_m),\n\\] with \\(M = \\min(n-1, p)\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained-2",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#proportion-variance-explained-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Proportion Variance Explained",
    "text": "Proportion Variance Explained\n\n\nTherefore, the PVE of the \\(m\\)-th principal component is given by the positive quantity between 0 and 1: \\[\n  \\frac{\\sum_{i=1}^n z_{im}^2}{\\sum_{j=1}^p \\sum_{i=1}^n x_{ij}^2}.\n\\]\nThe PVEs sum to one. We sometimes display the cumulative PVEs."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#clustering-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Clustering",
    "text": "Clustering\n\nClustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set.\nWe seek a partition of the data into distinct groups so that the observations within each group are quite similar to each other.\nTo make this concrete, we must define what it means for two or more observations to be similar or different.\nIndeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#application-clustering-for-market-segmentation",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#application-clustering-for-market-segmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Application: Clustering for Market Segmentation",
    "text": "Application: Clustering for Market Segmentation\n\nSuppose we have access to a large number of measurements (e.g., median household income, occupation, distance from nearest urban area, and so forth) for a large number of people.\nOur goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product.\nThe task of performing market segmentation amounts to clustering the people in the data set."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-means clustering",
    "text": "K-means clustering\n\n\n\n\n\n\n\n\n\n\nA simulated data set with 150 observations in 2-dimensional space. Panels show the results of applying K-means clustering with different values of \\(K\\), the number of clusters. The color of each observation indicates the cluster to which it was assigned using the K-means clustering algorithm. Note that there is no ordering of the clusters, so the cluster coloring is arbitrary."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-details",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-means clustering: Details",
    "text": "K-means clustering: Details\n\nLet \\(C_1, \\ldots, C_K\\) denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:\n\n\n\\(C_1 \\cup C_2 \\cup \\ldots \\cup C_K = \\{1, \\ldots, n\\}\\). In other words, each observation belongs to at least one of the \\(K\\) clusters.\n\\(C_k \\cap C_{k'} = \\emptyset\\) for all \\(k \\neq k'\\). In other words, the clusters are non-overlapping: no observation belongs to more than one cluster.\n\n\nFor instance, if the \\(i\\)-th observation is in the \\(k\\)-th cluster, then \\(i \\in C_k\\)."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-details-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#k-means-clustering-details-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "K-means clustering: Details",
    "text": "K-means clustering: Details\n\nThe idea behind K-means clustering is that a good clustering is one for which the within-cluster variation is as small as possible.\nThe within-cluster variation for cluster \\(C_k\\) is a measure \\(\\text{WCV}(C_k)\\) of the amount by which the observations within a cluster differ from each other.\nHence, we want to solve the problem: \\[\n  \\text{minimize}_{C_1, \\ldots, C_K} \\left\\{ \\sum_{k=1}^K \\text{WCV}(C_k) \\right\\}.\n\\]\nIn words, this formula says that we want to partition the observations into \\(K\\) clusters such that the total within-cluster variation, summed over all \\(K\\) clusters, is as small as possible."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-with-k-3",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-with-k-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example with \\(K = 3\\)",
    "text": "Example with \\(K = 3\\)"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\n\nK-means clustering requires us to pre-specify the number of clusters \\(K\\). This can be a disadvantage (later we discuss strategies for choosing \\(K\\)).\nHierarchical clustering is an alternative approach which does not require that we commit to a particular choice of \\(K\\).\nWe will check the bottom-up or agglomerative clustering. This is the most common type of hierarchical clustering and refers to the fact that a dendrogram is built starting from the leaves and combining clusters up to the trunk."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Step 1)",
    "text": "Hierarchical Clustering: The Idea (Step 1)\nBuilds a hierarchy in a “bottom-up” fashion…"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-5-final-step",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-the-idea-step-5-final-step",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: The Idea (Step 5: Final Step)",
    "text": "Hierarchical Clustering: The Idea (Step 5: Final Step)\nSingle Cluster Representation"
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-example",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#hierarchical-clustering-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Hierarchical Clustering: Example",
    "text": "Hierarchical Clustering: Example\n\n\n45 observations generated in 2-dimensional space.\nIn reality, there are three distinct classes, shown in separate colors.\nHowever, we will treat these class labels as unknown and will seek to cluster the observations in order to discover the classes from the data."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#types-of-linkage-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#types-of-linkage-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Types of Linkage",
    "text": "Types of Linkage\nLinkage methods determine how the distance between two clusters is measured when forming a hierarchical clustering dendrogram\n\n\n\n\n\n\n\n\n\nLinkage\nDescription\n\n\n\n\nComplete\nMaximal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.\n\n\nSingle\nMinimal inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities.\n\n\nAverage\nMean inter-cluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.\n\n\nCentroid\nDissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-usaarrests-data-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#example-usaarrests-data-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: USAarrests Data",
    "text": "Example: USAarrests Data\n\nThe USAarrests data has only four variables, which is on the low end for this method to work well. For this reason, for this demonstration we randomly set at most one variable per state to be missing and only used \\(M = 1\\) principal component.\nIn general, in order to apply this algorithm, we must select \\(M\\), the number of principal components to use for the imputation.\nOne approach is to randomly set to NA some elements that were actually observed, and select \\(M\\) based on how well those known values are recovered."
  },
  {
    "objectID": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#summary-1",
    "href": "lecture_slides/12_unsupervised_learning/12_unsupervised_learning.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nUnsupervised Learning: Focuses on finding patterns or structures (e.g., subgroups, summaries) within data when no labeled outcome variable is available. Examples include dimensionality reduction and clustering.\nPrincipal Components Analysis (PCA):\n\nSeeks linear combinations of features that capture the greatest variance in the data.\n\nOften used for visualization or as a pre-processing step to reduce dimensionality.\n\nThe proportion of variance explained (PVE) helps determine how many principal components to retain.\n\nMatrix Completion and Missing Values:\n\nExploits correlations among features to impute missing entries in a data matrix.\n\nTools like low-rank approximations (via PCA) can effectively fill in missing data under the assumption that the matrix is only partially observed.\n\n\n\n\n\n\nClustering Methods:\n\nK-means Clustering: Partitions data into \\(K\\) clusters by minimizing within-cluster variation, but requires specifying \\(K\\) in advance.\n\nHierarchical Clustering: Builds a tree (dendrogram) that merges data points (or clusters) iteratively, which can be cut at different heights to form any number of clusters.\n\nDistance & Linkage Choices:\n\nIn clustering, outcomes vary based on how similarity is measured (e.g., Euclidean distance vs. correlation-based distance) and which linkage criterion (e.g., complete, single, average) is chosen.\n\nPractical Considerations:\n\nDecide whether to standardize variables (especially if measured in different units).\n\nSelect the number of clusters (\\(K\\)) or principal components based on interpretability, domain knowledge, and diagnostic tools (like scree plots)."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#overview",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Six General Principals\n\nContext Matters\nVisualization Derives From Data\nLess is More: Separate Signal From Noise\nHierarchy Among Data\nBeauty Counts: Not All Data Are Equally Important\nTelling Your Story\n\n\n\n\nPosters"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#the-forest-and-the-trees-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#the-forest-and-the-trees-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Forest and the Trees",
    "text": "The Forest and the Trees\n\n\n\n\nForest\n\n\n\nWe have explored many implementation details in recent days, focusing on individual aspects of each analysis.\n\nToday, we want to take a step back to think less about the detail and more about the process.\nAfter all, every data analysis has a purpose. How can we achieve it more effectively?"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#essential-elements-of-data-communication",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#essential-elements-of-data-communication",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Essential Elements of Data Communication",
    "text": "Essential Elements of Data Communication\nLet’s break down the data communication process into six general principles:\n\nContext Matters\nVisualization Derives From Data\nLess is More: Separate Signal From Noise\nHierarchy Among Data\nBeauty Counts: Not All Data Are Equally Important\nTelling Your Story"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#context-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#context-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Context",
    "text": "Context\nEvery analysis has a goal and an audience.\n\nIt’s important to separate data exploration from the final analysis. Don’t fall into the temptation of showing everything you did.\nAdapt the report to your audience. Decision-makers aren’t always interested in execution details.\nSo what? Keep a specific learning objective in mind. It will guide which information is relevant for your report.\n\n\nIsolated numbers don’t tell us much. To make evidence-based decisions, it’s necessary to establish an appropriate basis for comparison for the goal of your report."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#context-can-come-from-new-information",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#context-can-come-from-new-information",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Context Can Come from New Information…",
    "text": "Context Can Come from New Information…"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#or-reinforce-existing-information",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#or-reinforce-existing-information",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "…or Reinforce Existing Information",
    "text": "…or Reinforce Existing Information"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#choosing-the-chart-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#choosing-the-chart-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Chart",
    "text": "Choosing the Chart\n\nUse graphs instead of tables!\nWhat type of data?\nHow many dimensions?\nMost reports are consumed in 2D media. Showing more than that can confuse the reader.\nBe careful with scales!"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#scales-can-be-misleading",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#scales-can-be-misleading",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Scales Can Be Misleading",
    "text": "Scales Can Be Misleading\n\n\n\n\nScale Fail"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#avoid-dual-axes",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#avoid-dual-axes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Avoid Dual Axes",
    "text": "Avoid Dual Axes\n\n\n\n\nDual Axis"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#or-triple-axes",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#or-triple-axes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Or Triple Axes!",
    "text": "Or Triple Axes!\n\n\n\n\nTriple Axis"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#fewer-pie-charts",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#fewer-pie-charts",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fewer Pie Charts…",
    "text": "Fewer Pie Charts…\n\n\n\n\nPie vs Bar"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#what",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#what",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What?!",
    "text": "What?!\n\n\nAbout Pie Charts"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#oof",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#oof",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Oof",
    "text": "Oof\n\n\n\n\nChart Fail"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#eliminating-noise",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#eliminating-noise",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Eliminating Noise",
    "text": "Eliminating Noise\n\n\nThe more information in your visualization, the greater the cognitive load.\nYour objective must be to reduce your audience cognitive costs.\n\n\n\nData-Ink Ratio Formula\n\\[\n\\text{Data-Ink Ratio} = \\frac{\\text{Data-Ink}}{\\text{Total ink used to print the graphic}}\n\\]"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#data-ink-ratio",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#data-ink-ratio",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data-Ink Ratio",
    "text": "Data-Ink Ratio\n\nYour objective must be to reduce your audience cognitive costs."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#step-by-step-cleanup",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#step-by-step-cleanup",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Step-by-Step Cleanup",
    "text": "Step-by-Step Cleanup"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#eliminating-the-border",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#eliminating-the-border",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Eliminating the Border",
    "text": "Eliminating the Border"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#cleaning-the-grids",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#cleaning-the-grids",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Cleaning the Grids",
    "text": "Cleaning the Grids"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#removing-the-points",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#removing-the-points",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Removing the Points",
    "text": "Removing the Points"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#processing-the-axes",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#processing-the-axes",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Processing the Axes",
    "text": "Processing the Axes"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#adjusting-the-label",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#adjusting-the-label",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Adjusting the Label",
    "text": "Adjusting the Label"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#adjusting-colors",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#adjusting-colors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Adjusting Colors",
    "text": "Adjusting Colors"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#before-and-after",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#before-and-after",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Before and After",
    "text": "Before and After\n\n\n\n\n\nBefore and After"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#count-the-number-3s",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#count-the-number-3s",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Count the Number 3s",
    "text": "Count the Number 3s"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#count-the-number-3s-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#count-the-number-3s-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Count the Number 3s",
    "text": "Count the Number 3s"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#ways-to-draw-attention",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#ways-to-draw-attention",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ways to Draw Attention",
    "text": "Ways to Draw Attention"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#highlighting-with-colors",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#highlighting-with-colors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Highlighting with Colors",
    "text": "Highlighting with Colors"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#returning-to-our-example",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#returning-to-our-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Returning to Our Example",
    "text": "Returning to Our Example"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#returning-to-our-example-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#returning-to-our-example-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Returning to Our Example",
    "text": "Returning to Our Example"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#use-colors-strategically",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#use-colors-strategically",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Use Colors Strategically",
    "text": "Use Colors Strategically"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#emphasizing-the-main-point",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#emphasizing-the-main-point",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Emphasizing the Main Point",
    "text": "Emphasizing the Main Point"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#emphasizing-the-main-point-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#emphasizing-the-main-point-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Emphasizing the Main Point",
    "text": "Emphasizing the Main Point"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together\nLet’s tell a story starting from the chart below, making step-by-step adaptations we’ve discussed. What is it telling you?"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together-2",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together-3",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together-4",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#bringing-it-all-together-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-2",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-3",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-4",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-5",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-6",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-7",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-narrative-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Narrative",
    "text": "Final Narrative"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#before-and-after-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#before-and-after-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Before and After",
    "text": "Before and After"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#covid-19-evolution",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#covid-19-evolution",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "COVID-19 Evolution",
    "text": "COVID-19 Evolution"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#moving-average",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#moving-average",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Moving Average",
    "text": "Moving Average"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#deaths-in-new-york",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#deaths-in-new-york",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deaths in New York",
    "text": "Deaths in New York"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#additional-material",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#additional-material",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Additional Material",
    "text": "Additional Material\n\nFlowing Data\nInformation is Beautiful\nThe Functional Art\nCOVID in the Financial Times"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#dashboards-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#dashboards-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dashboards",
    "text": "Dashboards\n\n\n\n\nFlexdashboard\n\n\n\nThe goal of flexdashboard is to facilitate the creation of interactive dashboards with R Markdown."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-features",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-features",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexdashboard: Features",
    "text": "Flexdashboard: Features\n\nSupport for a wide variety of components, including htmlwidgets; base graphics, structure, and grid; tabular data; gauges and value boxes; and text annotations.\nFlexible and easy to specify layouts based on rows and columns. Components are intelligently resized to fill the browser and adapted for mobile display.\nStoryboard layouts to present sequences of visualizations and related commentary."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-installation-and-use",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-installation-and-use",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexdashboard: Installation and Use",
    "text": "Flexdashboard: Installation and Use\nAfter installing the package, to create a flexdashboard simply open a new R Markdown document with the output format flexdashboard::flex_dashboard. You can do this from within RStudio using: File &gt; New File &gt; R Markdown...:\n\ninstall.packages(\"flexdashboard\")"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-layout",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-layout",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexdashboard: Layout",
    "text": "Flexdashboard: Layout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDashboards are divided into columns and rows, with output components delineated using level 3 markdown headers (###).\nBy default, dashboards are laid out in a single column, with charts stacked vertically and sized to fill the available height of the browser."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-layout-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-layout-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexdashboard: Layout",
    "text": "Flexdashboard: Layout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDepending on the nature of your dashboard (number of components, ideal component height, etc.), you might prefer a scrolling layout where components occupy their natural height and the browser scrolls when additional vertical space is needed.\nYou can specify this attribute via the vertical_layout: scroll option."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-layout-2",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-layout-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexdashboard: Layout",
    "text": "Flexdashboard: Layout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can also choose to orient the dashboards by row instead of by column by specifying orientation: rows."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-storyboard",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#flexdashboard-storyboard",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexdashboard: Storyboard",
    "text": "Flexdashboard: Storyboard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStoryboards are an alternative to the row and column-based layout schemes.\nThey are suitable for presenting a sequence of data visualizations and related commentary.\nTo create a storyboard layout, add storyboard: true to the dashboard’s preamble. This option includes a set of level 3 dashboard components (###). Each component will receive its own frame in the storyboard, with the section title used as a navigation caption."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#dashboard-additional-material",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#dashboard-additional-material",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dashboard: Additional Material",
    "text": "Dashboard: Additional Material\n\nFlexdashboard Sample Layouts\nFlexdashboard Examples\nUsing Flexdashboard\nInteractive Web-Based Data Visualization with R\nQuarto Dashboards\nShiny Dashboards"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#summary-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#summary-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\nMain Takeaways from this lecture:\n\n\n\nData Communication Principles:\n\nContext matters: Tailor your analysis to the audience and goal.\nFocus on the story: Highlight insights, not the process.\nBeauty and clarity: Simplify visuals, use appropriate colors, and remove unnecessary elements.\n\nVisualization Best Practices:\n\nUse graphs instead of tables where possible.\nAvoid misleading scales and excessive dimensions.\nPrioritize hierarchy and emphasize key data points.\n\n\n\n\n\nPoster:\n\nSimplicity and clarity win over clutter\nEmphasize the predictive analytics approach and metrics\nEnsure visual impact and logical flow\nPractice delivering a concise overview of your work\n\nFinal Message:\n\nLess is more. Reduce complexity to communicate data effectively.\nAlways keep your audience’s decision-making needs at the forefront."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#choosing-the-chart",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#choosing-the-chart",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Choosing the Chart",
    "text": "Choosing the Chart\n\nWhat type of data?\nHow many dimensions?\nMost reports are consumed in 2D media. Showing more than that can confuse the reader.\nBe careful with scales!"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#context",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#context",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Context",
    "text": "Context\nEvery analysis has a goal and an audience.\n\nIt’s important to separate data exploration from the final analysis. Don’t fall into the temptation of showing everything you did.\nAdapt the report to your audience. Decision-makers aren’t always interested in execution details.\nSo what? Keep a specific learning objective in mind. It will guide which information is relevant for your report.\n\n\nIsolated numbers don’t tell us much. To make evidence-based decisions, it’s necessary to establish an appropriate basis for comparison for the goal of your report."
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#use-graphs-instead-of-tables",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#use-graphs-instead-of-tables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Use graphs instead of tables!",
    "text": "Use graphs instead of tables!\n\n\n\n\nTable vs Plot"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#use-graphs-instead-of-tables-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#use-graphs-instead-of-tables-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Use graphs instead of tables!",
    "text": "Use graphs instead of tables!\n\n\n\n\nTable vs Plot"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#check-potential-ideas-the-python-or-r-graph-gallery",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#check-potential-ideas-the-python-or-r-graph-gallery",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Check Potential Ideas! The Python or R Graph Gallery",
    "text": "Check Potential Ideas! The Python or R Graph Gallery\n\n\n\n\n\n\n\nThe Python Graph Gallery\n\n\n\n\n\n\n\n\nThe R Graph Gallery"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#check-potential-ideas-the-python-or-r-graph-galleries",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#check-potential-ideas-the-python-or-r-graph-galleries",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Check Potential Ideas! The Python or R Graph Galleries",
    "text": "Check Potential Ideas! The Python or R Graph Galleries\n\n\n\n\n\n\n\n\nThe Python Graph Gallery\n\n\n\n\n\n\n\n\nThe R Graph Gallery"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poster",
    "text": "Poster\n\n\n\n\nWord Count"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-2",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poster",
    "text": "Poster\n\n\n\n\nHow to Write Good!"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-3",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poster",
    "text": "Poster\n\n\n\n\n\n\n\nWhy a Poster Presentation?\n\nShowcases your work in a concise yet visually appealing format\nEncourages interactive discussion with conference attendees\nDemonstrates your mastery of predictive analytics concepts"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-4",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poster",
    "text": "Poster\n\n\n\n\nHow to Write Good!"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-5",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poster",
    "text": "Poster\n\n\n\n\n\n\n\nWhy a Poster Presentation?\n\nShowcases your work in a concise yet visually appealing format\nEncourages interactive discussion with conference attendees\nDemonstrates your mastery of predictive analytics concepts"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#designing-an-effective-predictive-analytics-poster",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#designing-an-effective-predictive-analytics-poster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Designing an Effective Predictive Analytics Poster",
    "text": "Designing an Effective Predictive Analytics Poster\n\n\n\n\n\n\n\nPoster Objectives\n\nCommunicate key findings and impact of your project\nHighlight the predictive approach, methodology, and novel insights\nProvide a visually engaging, easy-to-navigate summary"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#why-a-poster-presentation",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#why-a-poster-presentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Why a Poster Presentation?",
    "text": "Why a Poster Presentation?\n\n\n\n\n\n\n\n\n\n\nShowcases your work\nOrganizes your ideas and results\nIs visually appealing\nEncourages interactive discussion\nDemonstrates your mastery of predictive analytics concepts"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-objectives",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-objectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poster Objectives",
    "text": "Poster Objectives\n\nCommunicate key findings and impact of your project\nHighlight the predictive approach, methodology, and novel insights\nProvide a visually engaging, easy-to-navigate summary"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster",
    "text": "Planning Your Poster"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#organizing-your-content",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#organizing-your-content",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Organizing Your Content",
    "text": "Organizing Your Content\n\nTitle & Authors: Clear, concise, and visible\nIntroduction / Background: Why your problem is important\nObjectives & Hypotheses: What you aim to predict or explain\nData & Methods: Key variables, data collection, modeling approach\nResults & Discussion: Main findings, performance metrics, interpretation\nConclusion & Future Work: Wrap-up and potential next steps\nAcknowledgments & References: Recognize collaborators and sources"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#visual-hierarchy",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#visual-hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Visual Hierarchy",
    "text": "Visual Hierarchy\n\nHeadings: Large and bold to guide the reader\nColor & Contrast: Choose a simple palette that highlights main points\nFont Size: Text should be legible from ~3 feet away\nFlow: Logical reading order from top-left to bottom-right"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#designing-for-predictive-analytics",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#designing-for-predictive-analytics",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Designing for Predictive Analytics",
    "text": "Designing for Predictive Analytics\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\n\nEmphasize the Predictive Component\n\nModel Choice: Clearly state if you used linear regression, random forests, neural networks, etc.\nMetrics: Include accuracy, RMSE, AUC, or other relevant metrics\nModel Interpretation: Highlight key features or coefficients that drove the predictions\n\nData Visualization Tips\n\nUse labeled graphs (e.g., bar charts, scatter plots, confusion matrices)\nShow before-and-after comparisons if you performed feature engineering\nExplain the importance of training vs. testing sets (or cross-validation)"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#crafting-a-clear-narrative",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#crafting-a-clear-narrative",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Crafting a Clear Narrative",
    "text": "Crafting a Clear Narrative"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#research-design-flow",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#research-design-flow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Research Design Flow",
    "text": "Research Design Flow\n\n\nShow each step as a section in your poster or as bullet points under Methodology"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-layout-design",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#poster-layout-design",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Poster Layout & Design",
    "text": "Poster Layout & Design\nKeep It Simple\n\nLimit text and rely on figures, charts, and bullet points\nUse consistent color schemes, fonts, and alignment\n\nUse of Space\n\nWhite space is your friend—avoid clutter\nGroup related sections in boxes or areas for clarity\n\nKey Text Guidelines\n\nTitle: ~85–100 pt\nHeadings/Subheadings: ~36–48 pt\nBody Text: ~24–32 pt\nCaptions: ~18–24 pt"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#effective-figures-and-tables",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#effective-figures-and-tables",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Effective Figures and Tables",
    "text": "Effective Figures and Tables\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\nCharts and Graphs\n\nLabel axes, include legends only if necessary\nHighlight the data that supports your main argument\nProvide short, descriptive captions beneath each figure\n\nTables\n\nKeep tables simple, emphasize key results\nConsider whether a chart might be more impactful than a table"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#results-interpretation",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#results-interpretation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results & Interpretation",
    "text": "Results & Interpretation\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\n\n\nShowcase Predictive Performance\n\nProvide confusion matrices for classification models\nCompare model performance with baseline or benchmark\nDiscuss practical implications of the performance metrics\n\nCritical Thinking\n\nExplain why the model performed well or poorly\nReflect on limitations (e.g., small dataset, missing variables, etc.)\nSuggest ways to improve performance or replicate your findings"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#conclusion-future-work",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#conclusion-future-work",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusion & Future Work",
    "text": "Conclusion & Future Work\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\nMain Takeaways\n\nRecap key predictions and findings in plain language\nEmphasize the impact and potential applications\n\nNext Steps\n\nPossible avenues for further research or new data\nEncouraging further validation, deployment, or real-world testing"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-touches",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#final-touches",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Final Touches",
    "text": "Final Touches\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\nAcknowledgments & References\n\nRecognize supporting faculty, funding sources, and collaborators\nCite any references (including data sources and libraries) in a consistent format\n\nProofreading & Practice\nSpell-check all text, verify data accuracy, ensure images are clear\nPractice explaining your poster to a non-expert"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#template",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#template",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Template",
    "text": "Template"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-template-and-rubric",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-template-and-rubric",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster: Template and Rubric",
    "text": "Planning Your Poster: Template and Rubric\n\n\n\n\n\nTemplate and Rubric\n\n\n\n\n\n\nThanks to Professor Matthew A. Lanham for sharing the original version of this template. Checking his personal webpage is highly recommended!"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-organizing-your-content",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-organizing-your-content",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster: Organizing Your Content",
    "text": "Planning Your Poster: Organizing Your Content\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\n\nTitle & Authors: Clear, concise, and visible\nAbstract/Introduction: One clear and concise paragraph to show why your problem is important and what are the main results.\nBusiness Problem: What is your project goal?\nAnalytics Problem Framing: What you aim to predict or explain and why\nResearch Question: Summarise your project in a question you will answer\nData and EDA: Key variables, data collection.\nMethodology: Modeling approach focusing on the response variable.\nModel Building and Evaluation: Model results clear and appropriately evaluated\nBusiness Implication: Business validation of model/solution demonstrated. Main findings, performance metrics, interpretation\nConclusion & Future Work: Wrap-up and potential next steps\nAcknowledgments & References: Recognize collaborators and sources"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#designing-a-predictive-analytics-poster-objectives",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#designing-a-predictive-analytics-poster-objectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Designing a Predictive Analytics Poster: Objectives",
    "text": "Designing a Predictive Analytics Poster: Objectives\n\n\n\n\n\n\n\n\n\n\nCommunicate key findings and impact of your project\nHighlight the predictive approach, methodology, and novel insights\nHighlight the business implications and insights"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-organizing-your-content-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-organizing-your-content-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster: Organizing Your Content",
    "text": "Planning Your Poster: Organizing Your Content\n\n\n\n\n\n\n\nTemplate and Rubric Download\n\n\n\n\n\nTitle & Authors: Clear, concise, and visible\nAbstract/Introduction: One clear and concise paragraph to show why your problem is important and what are the main results.\nBusiness Problem: What is your project goal?\nAnalytics Problem Framing: What you aim to predict or explain and why\nResearch Question: Summarise your project in a question you will answer\nData and EDA: Key variables, data collection.\nMethodology: Modeling approach focusing on the response variable.\nModel Building and Evaluation: Model results clear and appropriately evaluated\nBusiness Implication: Business validation of model/solution demonstrated. Main findings, performance metrics, interpretation\nConclusion & Future Work: Wrap-up and potential next steps\nAcknowledgments & References: Recognize collaborators and sources"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-visual-hierarchy",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-visual-hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster: Visual Hierarchy",
    "text": "Planning Your Poster: Visual Hierarchy\n\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\n\nProvide a visually engaging, easy-to-navigate summary\nAlways in columns!\nHeadings: Large and bold to guide the reader\nColor & Contrast: Choose a simple palette that highlights main points\nFont Size: Text should be legible from ~3 feet away\nFlow: Logical reading order from top-left to bottom-right"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-layout-design",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#planning-your-poster-layout-design",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Planning Your Poster: Layout & Design",
    "text": "Planning Your Poster: Layout & Design\n\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\n\nKeep It Simple\n\nLimit text and rely on figures, charts, and bullet points\nUse consistent color schemes, fonts, and alignment\nUse a QR Code to drive your audience to a dashboard or the project webpage\n\nUse of Space\n\nWhite space is your friend — avoid clutter\nGroup related sections in boxes or areas for clarity\n60% Graphics / 40% Text\n\nKey Text Guidelines\n\nTitle: ~85–100 pt\nHeadings/Subheadings: ~46–55 pt\nBody Text: ~34–38 pt\nCaptions: ~26–28 pt\nShould be readble from 3 feet away"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#crafting-a-clear-narrative-1",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#crafting-a-clear-narrative-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Crafting a Clear Narrative",
    "text": "Crafting a Clear Narrative\n\n\n\n\nWord Count"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#crafting-a-clear-narrative-2",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#crafting-a-clear-narrative-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Crafting a Clear Narrative",
    "text": "Crafting a Clear Narrative\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\nIntroduction & Problem Statement\n\nPresent the real-world problem or question\nBriefly mention why it matters (e.g., business impact, social relevance)\n\nThe “So What?” Factor\n\nEmphasize the value of your predictive findings\nIllustrate how stakeholders can use predictions or insights"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#day-of-the-conference",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#day-of-the-conference",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Day of the Conference",
    "text": "Day of the Conference\n\nArrive early to set up\nEngage attendees with a 1-2 minute elevator pitch\nInvite questions to spark in-depth discussions"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#tips-for-presenting-well",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#tips-for-presenting-well",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tips for Presenting Well",
    "text": "Tips for Presenting Well\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\nArrive early to set up\nStay close and off to the side\nPrepare a 30-second, 90-second, and 2-minute elevator pitches using your poster as a visual guide\nInvite questions to spark in-depth discussions\nUse your hands to direct your listener to your poster\nPrevent you or someone else blocking the poster"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#presenting-your-poster",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html#presenting-your-poster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Presenting your Poster",
    "text": "Presenting your Poster\n\n\n\n\n\n\n\n\nTemplate, Rubric, and Video\n\n\n\n\n\nTips for Presenting Well\n\nArrive early to set up\nStay close and off to the side\nPrepare a 30-second, 90-second, and 2-minute elevator pitches using your poster as a visual guide\nInvite questions to spark in-depth discussions\nActively ask questions to your audience\nUse your hands to direct your listener to your poster\nPrevent you or someone else blocking the poster\nFollow the dress code!"
  },
  {
    "objectID": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html",
    "href": "lecture_slides/99_data_communication_poster/99_data_communication_poster.html",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Six General Principals\n\nContext Matters\nVisualization Derives From Data\nLess is More: Separate Signal From Noise\nHierarchy Among Data\nBeauty Counts: Not All Data Are Equally Important\nTelling Your Story\n\n\n\n\nPosters"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#overview",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Overview",
    "text": "Overview\n\n\n\n\nDeep Learning\nPyTorch vs. TensorFlow\nPyTorch\nNeural Networks\nSingle Layer Neural Network\nFitting Neural Networks\n\n\n\nConvolutional Neural Network — CNN\nDocument Classification\nRecurrent Neural Networks - RNN\nRNN for Document Classification\nRNN for Time Series Forecasting\nWhen to Use Deep Learning\n\n\n\n\n\n\nThis lecture content is inspired by and replicates the material from An Introduction to Statistical Learning."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deep Learning",
    "text": "Deep Learning\n\nNeural networks became popular in the 1980s.\nLots of successes, hype, and great conferences: NeurIPS, Snowbird.\nThen along came SVMs, Random Forests, and Boosting in the 1990s, and Neural Networks took a back seat.\nRe-emerged around 2010 as Deep Learning.\nBy 2020s, very dominant and successful.\nPart of success due to vast improvements in computing power, larger training sets, and software: TensorFlow and PyTorch.\nMuch of the credit goes to three pioneers and their students:\n\nYann LeCun, Geoffrey Hinton, and Yoshua Bengio,\nwho received the 2019 ACM Turing Award for their work in Neural Networks."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network",
    "text": "Single Layer Neural Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\n\nDiagram of Single Layer Neural Network"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#details",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details",
    "text": "Details\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A_k = h_k(X) = g(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j)\\) are called the activations in the hidden layer. We can think of it as a non-linear tranformation of a linear function.\n\\(g(z)\\) is called the activation function. Two popular activation functions are: the sigmoid and rectified linear (ReLU).\nActivation functions in hidden layers are typically nonlinear; otherwise, the model collapses to a linear model.\nSo the activations are like derived features — nonlinear transformations of linear combinations of the features.\nThe model is fit by minimizing \\(\\sum_{i=1}^{n} (y_i - f(x_i))^2\\) (e.g., for regression)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#example-mnist-digits",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#example-mnist-digits",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: MNIST Digits",
    "text": "Example: MNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandwritten digits\n\n\\(28 \\times 28\\) grayscale images\n60K train, 10K test images\nFeatures are the 784 pixel grayscale values \\(\\in (0, 255)\\)\nLabels are the digit class \\(0\\text{–}9\\)\n\nGoal: Build a classifier to predict the image class.\nWe build a two-layer network with:\n\n256 units at the first layer,\n128 units at the second layer, and\n10 units at the output layer.\n\nAlong with intercepts (called biases), there are 235,146 parameters (referred to as weights)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#details-of-output-layer",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#details-of-output-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Details of Output Layer",
    "text": "Details of Output Layer\n\nLet \\(Z_m = \\beta_{m0} + \\sum_{\\ell=1}^{K_2} \\beta_{m\\ell} A_\\ell^{(2)}\\), \\(m = 0, 1, \\ldots, 9\\), be 10 linear combinations of activations at the second layer.\nOutput activation function encodes the softmax function: \\[\nf_m(X) = \\Pr(Y = m \\mid X) = \\frac{e^{Z_m}}{\\sum_{\\ell=0}^{9} e^{Z_\\ell}}.\n\\]\nWe fit the model by minimizing the negative multinomial log-likelihood (or cross-entropy): \\[\n-\\sum_{i=1}^{n} \\sum_{m=0}^{9} y_{im} \\log(f_m(x_i)).\n\\]\n\\(y_{im}\\) is 1 if the true class for observation \\(i\\) is \\(m\\), else 0 — i.e., one-hot encoded."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#results",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#results",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Results",
    "text": "Results\n\n\n\nMethod\nTest Error\n\n\n\n\nNeural Network + Ridge Regularization\n2.3%\n\n\nNeural Network + Dropout Regularization\n1.8%\n\n\nMultinomial Logistic Regression\n7.2%\n\n\nLinear Discriminant Analysis\n12.7%\n\n\n\n\nEarly success for neural networks in the 1990s.\nWith so many parameters, regularization is essential.\nSome details of regularization and fitting will come later.\nVery overworked problem — best reported rates are \\(&lt; 0.5\\%\\)!\nHuman error rate is reported to be around \\(0.2\\%\\), or 20 of the 10K test images."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolutional-neural-network-cnn",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolutional-neural-network-cnn",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolutional Neural Network — CNN",
    "text": "Convolutional Neural Network — CNN\n\nMajor success story for classifying images.\nShown are samples from CIFAR100 database: \\(32 \\times 32\\) color natural images, with 100 classes.\n\\(50K\\) training images, \\(10K\\) test images.\nEach image is a three-dimensional array or feature map:\n\\(32 \\times 32 \\times 3\\) array of 8-bit numbers.\nThe last dimension represents the three color channels for red, green, and blue."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#how-cnns-work",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#how-cnns-work",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "How CNNs Work",
    "text": "How CNNs Work\n\n\nThe CNN builds up an image in a hierarchical fashion.\nEdges and shapes are recognized and pieced together to form more complex shapes, eventually assembling the target image.\nThis hierarchical construction is achieved using convolution and pooling layers."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-filter",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-filter",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Filter",
    "text": "Convolution Filter\n\\[\n\\text{Input Image} =\n\\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l\n\\end{bmatrix}\n\\quad \\text{Convolution Filter} =\n\\begin{bmatrix}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{bmatrix}.\n\\]\n\\[\n\\text{Convolved Image} =\n\\begin{bmatrix}\na\\alpha + b\\beta + d\\gamma + e\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\nd\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\ng\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n\\end{bmatrix}.\n\\]\n\nThe filter is itself an image and represents a small shape, edge, etc.\nWe slide it around the input image, scoring for matches.\nThe scoring is done via dot-products, illustrated above.\nIf the subimage of the input image is similar to the filter, the score is high; otherwise, it is low.\nThe filters are learned during training."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-example",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-example",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Example",
    "text": "Convolution Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe idea of convolution with a filter is to find common patterns that occur in different parts of the image.\nThe two filters shown here highlight vertical and horizontal stripes.\nThe result of the convolution is a new feature map.\nSince images have three color channels, the filter does as well: one filter per channel, and dot-products are summed.\nThe weights in the filters are learned by the network."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#pooling",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#pooling",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pooling",
    "text": "Pooling\n\\[\n\\text{Max pool}\n\\begin{bmatrix}\n1 & 2 & 5 & 3 \\\\\n3 & 0 & 1 & 2 \\\\\n2 & 1 & 3 & 4 \\\\\n1 & 1 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n3 & 5 \\\\\n2 & 4\n\\end{bmatrix}\n\\]\n\nEach non-overlapping \\(2 \\times 2\\) block is replaced by its maximum.\nThis sharpens the feature identification.\nAllows for locational invariance.\nReduces the dimension by a factor of 4 — i.e., factor of 2 in each dimension."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#architecture-of-a-cnn",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#architecture-of-a-cnn",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Architecture of a CNN",
    "text": "Architecture of a CNN\n\n\nMany convolve + pool layers.\nFilters are typically small, e.g., each channel \\(3 \\times 3\\).\nEach filter creates a new channel in the convolution layer.\nAs pooling reduces size, the number of filters/channels is typically increased.\nNumber of layers can be very large.\nE.g., resnet50 trained on imagenet 1000-class image database has 50 layers!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#using-pretrained-networks-to-classify-images",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#using-pretrained-networks-to-classify-images",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Using Pretrained Networks to Classify Images",
    "text": "Using Pretrained Networks to Classify Images\n\nHere we use the 50-layer resnet50 network trained on the 1000-class imagenet corpus to classify some photographs."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-imdb-movie-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-imdb-movie-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification: IMDB Movie Reviews",
    "text": "Document Classification: IMDB Movie Reviews\nThe IMDB corpus consists of user-supplied movie ratings for a large collection of movies. Each has been labeled for sentiment as positive or negative. Here is the beginning of a negative review:\n\nThis has to be one of the worst films of the 1990s. When my friends & I were watching this film (being the target audience it was aimed at) we just sat & watched the first half an hour with our jaws touching the floor at how bad it really was. The rest of the time, everyone else in the theater just started talking to each other, leaving or generally crying into their popcorn …\n\nWe have labeled training and test sets, each consisting of 25,000 reviews, and each balanced with regard to sentiment.\nGoal: We want to build a classifier to predict the sentiment of a review."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#featurization-bag-of-words",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#featurization-bag-of-words",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Featurization: Bag-of-Words",
    "text": "Featurization: Bag-of-Words\nDocuments have different lengths and consist of sequences of words. How do we create features \\(X\\) to characterize a document?\n\nFrom a dictionary, identify the 10K most frequently occurring words.\nCreate a binary vector of length \\(p = 10K\\) for each document, and score a 1 in every position that the corresponding word occurred.\nWith \\(n\\) documents, we now have an \\(n \\times p\\) sparse feature matrix \\(\\mathbf{X}\\).\nWe compare a lasso logistic regression model to a two-hidden-layer neural network on the next slide. (No convolutions here!)\nBag-of-words are unigrams. We can instead use bigrams (occurrences of adjacent word pairs) and, in general, m-grams."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#lasso-versus-neural-network-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#lasso-versus-neural-network-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Lasso versus Neural Network — IMDB Reviews",
    "text": "Lasso versus Neural Network — IMDB Reviews\n\n\nSimpler lasso logistic regression model works as well as neural network in this case.\nglmnet was used to fit the lasso model, and is very effective because it can exploit sparsity in the \\(\\mathbf{X}\\) matrix."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\nOften data arise as sequences:\n\nDocuments are sequences of words, and their relative positions have meaning.\nTime-series such as weather data or financial indices.\nRecorded speech or music.\nHandwriting, such as doctor’s notes.\n\nRNNs build models that take into account this sequential nature of the data and build a memory of the past.\n\nThe feature for each observation is a sequence of vectors \\(X = \\{X_1, X_2, \\ldots, X_L\\}\\).\nThe target \\(Y\\) is often of the usual kind — e.g., a single variable such as Sentiment, or a one-hot vector for multiclass.\nHowever, \\(Y\\) can also be a sequence, such as the same document in a different language."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#simple-recurrent-neural-network-architecture",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#simple-recurrent-neural-network-architecture",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Simple Recurrent Neural Network Architecture",
    "text": "Simple Recurrent Neural Network Architecture\n\n\nThe hidden layer is a sequence of vectors \\(A_\\ell\\), receiving as input \\(X_\\ell\\) as well as \\(A_{\\ell-1}\\). \\(A_\\ell\\) produces an output \\(O_\\ell\\).\nThe same weights \\(\\mathbf{W}\\), \\(\\mathbf{U}\\), and \\(\\mathbf{B}\\) are used at each step in the sequence — hence the term recurrent.\nThe \\(A_\\ell\\) sequence represents an evolving model for the response that is updated as each element \\(X_\\ell\\) is processed."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-in-detail",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-in-detail",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN in Detail",
    "text": "RNN in Detail\n\nSuppose \\(X_\\ell = (X_{\\ell1}, X_{\\ell2}, \\ldots, X_{\\ell p})\\) has \\(p\\) components, and \\(A_\\ell = (A_{\\ell1}, A_{\\ell2}, \\ldots, A_{\\ell K})\\) has \\(K\\) components. Then the computation at the \\(k\\)-th components of hidden unit \\(A_\\ell\\) is:\n\\[\nA_{\\ell k} = g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_{\\ell j} + \\sum_{s=1}^{K} u_{ks} A_{\\ell-1,s}\\right)\n\\]\n\\[\nO_\\ell = \\beta_0 + \\sum_{k=1}^{K} \\beta_k A_{\\ell k}\n\\]\nOften we are concerned only with the prediction \\(O_L\\) at the last unit. For squared error loss, and \\(n\\) sequence/response pairs, we would minimize:\n\\[\n\\sum_{i=1}^{n} (y_i - o_{iL})^2 = \\sum_{i=1}^{n} \\left(y_i - \\left(\\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} x_{iL,j} + \\sum_{s=1}^{K} u_{ks} a_{i,L-1,s}\\right)\\right)\\right)^2\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-and-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-and-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN and IMDB Reviews",
    "text": "RNN and IMDB Reviews\n\nThe document feature is a sequence of words \\(\\{\\mathcal{W}_\\ell\\}_{1}^{L}\\). We typically truncate/pad the documents to the same number \\(L\\) of words (we use \\(L = 500\\)).\nEach word \\(\\mathcal{W}_\\ell\\) is represented as a one-hot encoded binary vector \\(X_\\ell\\) (dummy variable) of length \\(10K\\), with all zeros and a single one in the position for that word in the dictionary.\nThis results in an extremely sparse feature representation and would not work well.\nInstead, we use a lower-dimensional pretrained word embedding matrix \\(\\mathbf{E}\\) (\\(m \\times 10K\\), next slide).\nThis reduces the binary feature vector of length \\(10K\\) to a real feature vector of dimension \\(m \\ll 10K\\) (e.g., \\(m\\) in the low hundreds)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#word-embedding",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#word-embedding",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Word Embedding",
    "text": "Word Embedding\n\nReview:\n\nthis is one of the best films actually the best I have ever seen the film starts one fall day…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings are pretrained on very large corpora of documents, using methods similar to principal components. word2vec and GloVe are popular."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-on-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-on-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN on IMDB Reviews",
    "text": "RNN on IMDB Reviews\n\nAfter a lot of work, the results are a disappointing 76% accuracy.\nWe then fit a more exotic RNN than the one displayed — a LSTM with long and short term memory. Here \\(A_\\ell\\) receives input from \\(A_{\\ell-1}\\) (short term memory) as well as from a version that reaches further back in time (long term memory). Now we get 87% accuracy, slightly less than the 88% achieved by glmnet.\nThese data have been used as a benchmark for new RNN architectures. The best reported result found at the time of writing (2020) was around 95%. We point to a leaderboard in Section 10.5.1."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#time-series-forecasting",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#time-series-forecasting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Time Series Forecasting",
    "text": "Time Series Forecasting"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#new-york-stock-exchange-data",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#new-york-stock-exchange-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "New-York Stock Exchange Data",
    "text": "New-York Stock Exchange Data\nShown in the previous slide are three daily time series for the period December 3, 1962, to December 31, 1986 (6,051 trading days):\n\nLog trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.\nDow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.\nLog volatility. This is based on the absolute values of daily price movements.\n\nGoal: predict Log trading volume tomorrow, given its observed values up to today, as well as those of Dow Jones return and Log volatility.\nThese data were assembled by LeBaron and Weigend (1998) IEEE Transactions on Neural Networks, 9(1): 213–220."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#autocorrelation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#autocorrelation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\nThe autocorrelation at lag \\(\\ell\\) is the correlation of all pairs \\((v_t, v_{t-\\ell})\\) that are \\(\\ell\\) trading days apart.\nThese sizable correlations give us confidence that past values will be helpful in predicting the future.\nThis is a curious prediction problem: the response \\(v_t\\) is also a feature \\(v_{t-\\ell}\\)!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-forecaster",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Forecaster",
    "text": "RNN Forecaster\nWe only have one series of data! How do we set up for an RNN?\nWe extract many short mini-series of input sequences \\(\\mathbf{X} = \\{ X_1, X_2, \\ldots, X_L \\}\\) with a predefined length \\(L\\) known as the lag:\n\\[\nX_1 = \\begin{pmatrix}\nv_{t-L} \\\\\nr_{t-L} \\\\\nz_{t-L}\n\\end{pmatrix}, \\quad\nX_2 = \\begin{pmatrix}\nv_{t-L+1} \\\\\nr_{t-L+1} \\\\\nz_{t-L+1}\n\\end{pmatrix}, \\quad\n\\cdots, \\quad\nX_L = \\begin{pmatrix}\nv_{t-1} \\\\\nr_{t-1} \\\\\nz_{t-1}\n\\end{pmatrix}, \\quad \\text{and} \\quad Y = v_t.\n\\]\nSince \\(T = 6,051\\), with \\(L = 5\\), we can create 6,046 such \\((X, Y)\\) pairs.\nWe use the first 4,281 as training data, and the following 1,770 as test data. We fit an RNN with 12 hidden units per lag step (i.e., per \\(A_\\ell\\))."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-results-for-nyse-data",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Results for NYSE Data",
    "text": "RNN Results for NYSE Data\n\nThe figure shows predictions and truth for the test period.\n\\[\nR^2 = 0.42 \\text{ for RNN}\n\\]\n\\(R^2 = 0.18\\) for the naive approach — uses yesterday’s value of Log trading volume to predict that of today.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-forecaster",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-forecaster",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Forecaster",
    "text": "Autoregression Forecaster\nThe RNN forecaster is similar in structure to a traditional autoregression procedure.\n\\[\n\\mathbf{y} =\n\\begin{bmatrix}\nv_{L+1} \\\\\nv_{L+2} \\\\\nv_{L+3} \\\\\n\\vdots \\\\\nv_T\n\\end{bmatrix}, \\quad\n\\mathbf{M} =\n\\begin{bmatrix}\n1 & v_L & v_{L-1} & \\cdots & v_1 \\\\\n1 & v_{L+1} & v_L & \\cdots & v_2 \\\\\n1 & v_{L+2} & v_{L+1} & \\cdots & v_3 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & v_{T-1} & v_{T-2} & \\cdots & v_{T-L}\n\\end{bmatrix}.\n\\]\nFit an OLS regression of \\(\\mathbf{y}\\) on \\(\\mathbf{M}\\), giving:\n\\[\n\\hat{v}_t = \\hat{\\beta}_0 + \\hat{\\beta}_1 v_{t-1} + \\hat{\\beta}_2 v_{t-2} + \\cdots + \\hat{\\beta}_L v_{t-L}.\n\\]\nKnown as an order-\\(L\\) autoregression model or \\(AR(L)\\).\nFor the NYSE data, we can include lagged versions of DJ_return and log_volatility in matrix \\(\\mathbf{M}\\), resulting in \\(3L + 1\\) columns."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-results-for-nyse-data",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#autoregression-results-for-nyse-data",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Autoregression Results for NYSE Data",
    "text": "Autoregression Results for NYSE Data\n\n\\(R^2 = 0.41 \\text{ for } AR(5) \\text{ model (16 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for RNN model (205 parameters)}\\)\n\\(R^2 = 0.42 \\text{ for } AR(5) \\text{ model fit by neural network.}\\)\n\\(R^2 = 0.46 \\text{ for all models if we include } \\textbf{day_of_week} \\text{ of day being predicted.}\\)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#summary-of-rnns",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#summary-of-rnns",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary of RNNs",
    "text": "Summary of RNNs\n\nWe have presented the simplest of RNNs. Many more complex variations exist.\nOne variation treats the sequence as a one-dimensional image, and uses CNNs for fitting. For example, a sequence of words using an embedding representation can be viewed as an image, and the CNN convolves by sliding a convolutional filter along the sequence.\nCan have additional hidden layers, where each hidden layer is a sequence, and treats the previous hidden layer as an input sequence.\nCan have output also be a sequence, and input and output share the hidden units. So called seq2seq learning are used for language translation."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#when-to-use-deep-learning",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#when-to-use-deep-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "When to Use Deep Learning",
    "text": "When to Use Deep Learning\n\nCNNs have had enormous successes in image classification and modeling, and are starting to be used in medical diagnosis. Examples include digital mammography, ophthalmology, MRI scans, and digital X-rays.\nRNNs have had big wins in speech modeling, language translation, and forecasting.\n\nShould we always use deep learning models?\n\nOften the big successes occur when the signal to noise ratio is high — e.g., image recognition and language translation. Datasets are large, and overfitting is not a big problem.\nFor noisier data, simpler models can often work better:\n\nOn the NYSE data, the AR(5) model is much simpler than an RNN, and performed as well.\nOn the IMDB review data, the linear model fit by glmnet did as well as the neural network, and better than the RNN.\n\nWe endorse the Occam’s razor principle — we prefer simpler models if they work as well. More interpretable!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#fitting-neural-networks",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#fitting-neural-networks",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fitting Neural Networks",
    "text": "Fitting Neural Networks\n\n\\[\n\\min_{\\{w_k\\}_{1}^K, \\beta} \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2, \\quad \\text{where}\n\\]\n\\[\nf(x_i) = \\beta_0 + \\sum_{k=1}^K \\beta_k g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}\\right).\n\\]\nThis problem is difficult because the objective is non-convex.\nDespite this, effective algorithms have evolved that can optimize complex neural network problems efficiently."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#non-convex-functions-and-gradient-descent",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#non-convex-functions-and-gradient-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Non Convex Functions and Gradient Descent",
    "text": "Non Convex Functions and Gradient Descent\nLet \\(R(\\theta) = \\frac{1}{2} \\sum_{i=1}^n (y_i - f_\\theta(x_i))^2\\) with \\(\\theta = (\\{w_k\\}_{1}^K, \\beta)\\).\n\n\nStart with a guess \\(\\theta^0\\) for all the parameters in \\(\\theta\\), and set \\(t = 0\\).\nIterate until the objective \\(R(\\theta)\\) fails to decrease:\n\nFind a vector \\(\\delta\\) that reflects a small change in \\(\\theta\\), such that \\(\\theta^{t+1} = \\theta^t + \\delta\\) reduces the objective; i.e., \\(R(\\theta^{t+1}) &lt; R(\\theta^t)\\).\nSet \\(t \\gets t + 1\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent-continued",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent-continued",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent Continued",
    "text": "Gradient Descent Continued\n\nIn this simple example, we reached the global minimum.\nIf we had started a little to the left of \\(\\theta^0\\), we would have gone in the other direction and ended up in a local minimum.\nAlthough \\(\\theta\\) is multi-dimensional, we have depicted the process as one-dimensional. It is much harder to identify whether one is in a local minimum in high dimensions.\nHow to find a direction \\(\\delta\\) that points downhill? We compute the gradient vector: \\[\n\\nabla R(\\theta^t) = \\frac{\\partial R(\\theta)}{\\partial \\theta} \\bigg|_{\\theta = \\theta^t}\n\\]\ni.e., the vector of partial derivatives at the current guess \\(\\theta^t\\).\nThe gradient points uphill, so our update is \\(\\delta = - \\rho \\nabla R(\\theta^t)\\) or \\[\n\\theta^{t+1} \\gets \\theta^t - \\rho \\nabla R(\\theta^t),\n\\] where \\(\\rho\\) is the learning rate (typically small, e.g., \\(\\rho = 0.001\\))."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradients-and-backpropagation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradients-and-backpropagation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradients and Backpropagation",
    "text": "Gradients and Backpropagation\n\n\\[\nR(\\theta) = \\sum_{i=1}^n R_i(\\theta) \\text{ is a sum, so gradient is sum of gradients.}\n\\]\n\\[\nR_i(\\theta) = \\frac{1}{2}(y_i - f_\\theta(x_i))^2 = \\frac{1}{2} \\left( y_i - \\beta_0 - \\sum_{k=1}^K \\beta_k g\\left( w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij} \\right) \\right)^2\n\\]\nFor ease of notation, let\n\\[\nz_{ik} = w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}.\n\\]\nBackpropagation uses the chain rule for differentiation:\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial \\beta_k} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial \\beta_k}\n= -(y_i - f_\\theta(x_i)) \\cdot g(z_{ik}).\n\\]\n\\[\n\\frac{\\partial R_i(\\theta)}{\\partial w_{kj}} = \\frac{\\partial R_i(\\theta)}{\\partial f_\\theta(x_i)} \\cdot \\frac{\\partial f_\\theta(x_i)}{\\partial g(z_{ik})} \\cdot \\frac{\\partial g(z_{ik})}{\\partial z_{ik}} \\cdot \\frac{\\partial z_{ik}}{\\partial w_{kj}}\n= -(y_i - f_\\theta(x_i)) \\cdot \\beta_k \\cdot g'(z_{ik}) \\cdot x_{ij}.\n\\]"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#tricks-of-the-trade",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#tricks-of-the-trade",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tricks of the Trade",
    "text": "Tricks of the Trade\n\nSlow learning. Gradient descent is slow, and a small learning rate \\(\\rho\\) slows it even further. With early stopping, this is a form of regularization.\nStochastic gradient descent. Rather than compute the gradient using all the data, use a small minibatch drawn at random at each step. E.g. for MNIST data, with \\(n = 60K\\), we use minibatches of 128 observations.\nAn epoch is a count of iterations and amounts to the number of minibatch updates such that \\(n\\) samples in total have been processed; i.e. \\(60K/128 \\approx 469\\) for MNIST.\nRegularization. Ridge and lasso regularization can be used to shrink the weights at each layer. Two other popular forms of regularization are dropout and augmentation, discussed next."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#dropout-learning",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#dropout-learning",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Dropout Learning",
    "text": "Dropout Learning\n\n\nAt each Stochastic Gradient Descent (SGD) update, randomly remove units with probability \\(\\phi\\), and scale up the weights of those retained by \\(1/(1-\\phi)\\) to compensate.\nIn simple scenarios like linear regression, a version of this process can be shown to be equivalent to ridge regularization.\nAs in ridge, the other units stand in for those temporarily removed, and their weights are drawn closer together.\nSimilar to randomly omitting variables when growing trees in random forests."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#ridge-and-data-augmentation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#ridge-and-data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Ridge and Data Augmentation",
    "text": "Ridge and Data Augmentation\n\n\nMake many copies of each \\((x_i, y_i)\\) and add a small amount of Gaussian noise to the \\(x_i\\) — a little cloud around each observation — but leave the copies of \\(y_i\\) alone!\nThis makes the fit robust to small perturbations in \\(x_i\\), and is equivalent to ridge regularization in an OLS setting."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation-on-the-fly",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation-on-the-fly",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation on the Fly",
    "text": "Data Augmentation on the Fly\n\n\nData augmentation is especially effective with SGD, here demonstrated for a CNN and image classification.\nNatural transformations are made of each training image when it is sampled by SGD, thus ultimately making a cloud of images around each original training image.\nThe label is left unchanged — in each case still tiger.\nImproves performance of CNN and is similar to ridge."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#double-descent",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#double-descent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Double Descent",
    "text": "Double Descent\n\nWith neural networks, it seems better to have too many hidden units than too few.\nLikewise more hidden layers better than few.\nRunning stochastic gradient descent till zero training error often gives good out-of-sample error.\nIncreasing the number of units or layers and again training till zero error sometimes gives even better out-of-sample error.\nWhat happened to overfitting and the usual bias-variance trade-off?\n\nBelkin, Hsu, Ma, and Mandal (arXiv 2018) Reconciling Modern Machine Learning and the Bias-Variance Trade-off."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#the-double-descent-error-curve",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#the-double-descent-error-curve",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Double-Descent Error Curve",
    "text": "The Double-Descent Error Curve\n\n\nWhen \\(d \\leq 20\\), model is OLS, and we see usual bias-variance trade-off.\nWhen \\(d &gt; 20\\), we revert to minimum-norm. As \\(d\\) increases above 20, \\(\\sum_{j=1}^d \\hat{\\beta}_j^2\\) decreases since it is easier to achieve zero error, and hence less wiggly solutions."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#less-wiggly-solutions",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#less-wiggly-solutions",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Less Wiggly Solutions",
    "text": "Less Wiggly Solutions\n\n\nTo achieve a zero-residual solution with \\(d = 20\\) is a real stretch!\nEasier for larger \\(d\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#some-facts",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#some-facts",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Some Facts",
    "text": "Some Facts\n\nIn a wide linear model (\\(p \\gg n\\)) fit by least squares, SGD with a small step size leads to a minimum norm zero-residual solution.\nStochastic gradient flow — i.e. the entire path of SGD solutions — is somewhat similar to ridge path.\nBy analogy, deep and wide neural networks fit by SGD down to zero training error often give good solutions that generalize well.\nIn particular cases with high signal-to-noise ratio — e.g. image recognition — are less prone to overfitting; the zero-error solution is mostly signal!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#software",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#software",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Software",
    "text": "Software\n\nWonderful software available for neural networks and deep learning. Tensorflow from Google and PyTorch from Facebook. Both are Python packages.\nIn the Chapter 10 lab we demonstrate tensorflow and keras packages in R, which interface to Python. See textbook and online resources for Rmarkdown and Jupyter notebooks for these and all labs for the second edition of ISLR book.\nThe torch package in R is available as well, and implements the PyTorch dialect. The Chapter 10 lab will be available in this dialect as well; watch the resources page at www.statlearning.com."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#summary",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#summary",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nDeep Learning Renaissance\n\nNeural networks first rose to prominence in the 1980s, waned in the 1990s, then surged again around 2010.\nAdvances in computing (GPUs) and availability of massive labeled datasets propelled deep learning success.\n\nFrameworks (PyTorch vs. TensorFlow)\n\nPyTorch is known for its dynamic graph and Pythonic syntax; widely used in research.\nTensorFlow has an extensive production ecosystem, ideal for enterprise and deployment.\n\nEssential Concepts\n\nAutomatic differentiation, gradient descent, and backpropagation are at the core of training neural networks.\n\n\n\n\n\n\nCNNs and RNNs\n\nCNNs excel in image classification by learning local patterns via convolution and pooling layers.\n\nRNNs (and variants like LSTM, GRU) handle sequential data for tasks like language modeling and time-series forecasting.\n\nWhen to Use Deep Learning\n\nWorks best on large datasets with high signal-to-noise ratio (e.g., image, text).\n\nSimpler models often perform well on noisier tasks or smaller datasets.\n\nOver-parameterization can still generalize due to “double-descent” effects.\n\nPractical Tips\n\nUse regularization (dropout, data augmentation, weight decay) to mitigate overfitting.\n\nMonitor convergence with appropriate learning rates and consider mini-batch stochastic gradient descent."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deep Learning",
    "text": "Deep Learning\n\n\n\n\n\nEarly Rise (1980s)\n\nNeural networks first gained popularity.\n\nHigh levels of excitement, with dedicated conferences (e.g., NeurIPS, Snowbird).\n\n1990s Shift\n\nEmergence of other methods (SVMs, Random Forests, Boosting).\n\nNeural networks receded into the background.\n\nResurgence (2010)\n\nRebranded and refined under the banner of Deep Learning.\n\nBy the 2020s, became extremely successful and widely adopted.\n\nKey Drivers of Success\n\nRapid increases in computing power (GPUs, parallel computing).\n\nAvailability of large-scale datasets.\n\nUser-friendly deep learning libraries (e.g., TensorFlow, PyTorch).\n\n\n\n\nMuch of the credit goes to three pioneers and their students:\n\nYann LeCun, Geoffrey Hinton, and Yoshua Bengio,\nwho received the 2019 ACM Turing Award for their work in Neural Networks."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network",
    "text": "Single Layer Neural Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\n\nNetwork Diagram of Single Layer Neural Network"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-introduction-and-layers-overview",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Introduction and Layers Overview",
    "text": "Single Layer Neural Network: Introduction and Layers Overview\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nNeural networks are often displayed using network diagrams, as shown in the figure.\n\nInput Layer (Orange Circles):\n\n\\(X_1, X_2, X_3, X_4\\)\nThese are observed variables from the dataset.\n\nHidden Layer (Blue Circles):\n\n\\(A_1, A_2, A_3, A_4, A_5\\)\nThese are transformations (activations) computed from the inputs.\n\nOutput Layer (Pink Circle):\n\n\\(f(X) \\to Y\\)\n\\(Y\\) is also observed, e.g., a label or continuous response."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-observed-vs.-latent",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Observed vs. Latent",
    "text": "Single Layer Neural Network: Observed vs. Latent\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nWhere is the observed data?\n\n\\(X_j\\) are observed (the input features).\n\\(Y\\) is observed (the response or label).\nThe hidden units (\\(A_k\\)) are not observed; they’re learned transformations."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-hidden-layer-as-transformations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Hidden Layer as Transformations",
    "text": "Single Layer Neural Network: Hidden Layer as Transformations\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\nIn the hidden layer, each activation \\(A_k\\) is computed as:\n\\[\nA_k = g\\Bigl(w_{k0} + \\sum_{j=1}^4 w_{kj} X_j\\Bigr),\n\\]\n\nIn the formula, these \\(h_k(X)\\) are the same as the activations \\(A_k\\).\n\\(h_k(X)\\) = \\(g(w_{k0} + \\sum_{j=1}^p w_{kj} X_j)\\).\n\\(g(\\cdot)\\) is a nonlinear function (e.g., ReLU, sigmoid, tanh).\n\\(w_{kj}\\) are the weights learned during training.\nEach hidden unit has a different set of weights, hence different transformations."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-training-the-network",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-training-the-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Training the Network",
    "text": "Single Layer Neural Network: Training the Network\n\n\n\n\\[\n\\begin{align*}\nf(X) &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k h_k(X) \\\\\n     &= \\beta_0 + \\sum_{k=1}^{K} \\beta_k g\\left(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j \\right).\n\\end{align*}\n\\]\nNetwork Diagram of Single Layer Neural Network\n\n\n\n\n\n\n\n\n\n\n\nThe network learns all weights \\(w_{kj}, w_{k0}, \\beta_k, \\beta_0\\) during training.\nObjective: predict \\(Y\\) from \\(X\\) accurately.\nKey insight: Hidden layer learns useful transformations on the fly to help approximate the true function mapping \\(X\\) to \\(Y\\)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-details",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#single-layer-neural-network-details",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Single Layer Neural Network: Details",
    "text": "Single Layer Neural Network: Details\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A_k = h_k(X) = g(w_{k0} + \\sum_{j=1}^{p} w_{kj} X_j)\\) are called the activations in the hidden layer. We can think of it as a non-linear tranformation of a linear function.\n\\(g(z)\\) is called the activation function. Two popular activation functions are: the sigmoid and rectified linear (ReLU).\nActivation functions in hidden layers are typically nonlinear; otherwise, the model collapses to a linear model.\nSo the activations are like derived features — nonlinear transformations of linear combinations of the features.\nThe model is fit by minimizing \\(\\sum_{i=1}^{n} (y_i - f(x_i))^2\\) (e.g., for regression)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#example-mnist-digits-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#example-mnist-digits-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: MNIST Digits",
    "text": "Example: MNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandwritten digits\n\n\\(28 \\times 28\\) grayscale images\n60K train, 10K test images\nFeatures are the 784 pixel grayscale values \\(\\in (0, 255)\\)\nLabels are the digit class \\(0\\text{–}9\\)\n\nGoal: Build a classifier to predict the image class.\nWe build a two-layer network with:\n\n256 units at the first layer,\n128 units at the second layer, and\n10 units at the output layer.\n\nAlong with intercepts (called biases), there are 235,146 parameters (referred to as weights).\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#example-mnist-digits-2",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#example-mnist-digits-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Example: MNIST Digits",
    "text": "Example: MNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandwritten digits\n\n\\(28 \\times 28\\) grayscale images\n60K train, 10K test images\nFeatures are the 784 pixel grayscale values \\(\\in (0, 255)\\)\nLabels are the digit class \\(0\\text{–}9\\)\n\nGoal: Build a classifier to predict the image class.\nWe build a two-layer network with:\n\n256 units at the first layer,\n128 units at the second layer, and\n10 units at the output layer.\n\nAlong with intercepts (called biases), there are 235,146 parameters (referred to as weights)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#neural-networks---video",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#neural-networks---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Neural Networks - Video",
    "text": "Neural Networks - Video\n\n\n\n\n\n\nBut what is a neural network?"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning---video",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deep Learning - Video",
    "text": "Deep Learning - Video\n\n\n\n\n\n\n“Godfather of AI” Geoffrey Hinton: The 60 Minutes Interview"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#additional-material",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#additional-material",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Additional Material",
    "text": "Additional Material\n\n3Blue1Brown: Neural Networks\nDeep Learning, by Ian Goodfellow and Yoshua Bengio and Aaron Courvill\nWelch Labs: Neural Networks Demystified\nWelch Labs: Learning To See\nDistill: A Gentle Introduction to Graph Neural Networks\nNeural Networks and Deep Learning, by Michael Nielsen\nCITS4012 Natural Language Processing\nDeep Learning with PyTorch Step-by-Step"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolutional-neural-network-cnn-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolutional-neural-network-cnn-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolutional Neural Network — CNN",
    "text": "Convolutional Neural Network — CNN\n\nMajor success story for classifying images.\nShown are samples from CIFAR100 database: \\(32 \\times 32\\) color natural images, with 100 classes.\n\\(50K\\) training images, \\(10K\\) test images.\nEach image is a three-dimensional array or feature map:\n\\(32 \\times 32 \\times 3\\) array of 8-bit numbers.\nThe last dimension represents the three color channels for red, green, and blue."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#section",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#section",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "Backpropagation, intuitively"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning---video-ii",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#deep-learning---video-ii",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Deep Learning - Video II",
    "text": "Deep Learning - Video II\n\n\n\n\n\n\n“Godfather of AI” Geoffrey Hinton: The 60 Minutes Interview"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#section-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#section-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "",
    "text": "The future of artificial intelligence - Yann LeCun"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#ai-visionaries-three-perspectives",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#ai-visionaries-three-perspectives",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AI Visionaries: Three Perspectives",
    "text": "AI Visionaries: Three Perspectives\n\n\n\n\n\n\nYann LeCun: The Future of Artificial Intelligence\n\n\n\nGeoffrey Hinton: “Godfather of AI” - 60 Minutes Interview\n\n\n\nYoshua Bengio: The Path to Human-Level AI"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#ai-visionaries-three-perspectives-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#ai-visionaries-three-perspectives-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AI Visionaries: Three Perspectives",
    "text": "AI Visionaries: Three Perspectives\n\n\n\n\n\n\nYann LeCun: The Future of Artificial Intelligence\n\n\n\nGeoffrey Hinton: “Godfather of AI” - 60 Minutes Interview\n\n\n\nYoshua Bengio: The Path to Human-Level AI"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#ai-visionaries-interviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#ai-visionaries-interviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "AI Visionaries: Interviews",
    "text": "AI Visionaries: Interviews\n\n\n\n\n\n\n\n\n Yann LeCunThe Future of AIDec 16, 2023 \n\n\n\n\n\n\n Geoffrey Hinton60 Minutes InterviewOct 9, 2023 \n\n\n\n\n\n\n Yoshua BengioPath to Human-Level AIApr 24, 2024"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#what-are-deep-learning-frameworks",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#what-are-deep-learning-frameworks",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What Are Deep Learning Frameworks?",
    "text": "What Are Deep Learning Frameworks?\n\nDeep learning frameworks reduce boilerplate code, handle tensor operations efficiently, and make it easier to prototype and iterate on new architectures.\nSoftware libraries designed to streamline the creation, training, and deployment of neural networks.\n\nProvide pre-built functions, automatic differentiation, and GPU/TPU support.\n\nNecessity: They allow researchers and developers to focus on model design rather than low-level implementation details."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#what-is-pytorch",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#what-is-pytorch",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is PyTorch?",
    "text": "What is PyTorch?\n\nDeveloped primarily by Facebook (Meta).\nEmphasizes a dynamic computation graph (eager execution).\nHighly “Pythonic”: feels natural for Python developers.\nStrong community presence in academia and research.\n\nWhy is PyTorch Necessary?\n\nEase of Use & Debugging\n\nEvaluate expressions immediately without building a separate graph.\n\nMore intuitive for experimenting with complex, innovative models.\n\nResearch Focus\n\nQuickly prototype new ideas and iterate.\n\nActive Ecosystem\n\nLibraries like torchvision, torchaudio, and others for specialized tasks.\n\n\n\nPyTorch’s eager execution model aligns well with Python’s interactive style—making it popular among research communities."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#what-is-tensorflow",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#what-is-tensorflow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "What is TensorFlow?",
    "text": "What is TensorFlow?\n\nDeveloped primarily by Google.\nHistorically used a static graph approach (with an “eager mode” added later).\nComes with extensive tools for deployment (mobile, web, and production).\nLarge ecosystem with well-integrated components (e.g., TensorBoard, TFX, TensorFlow Lite).\n\nWhy is TensorFlow Necessary?\n\nProduction-Ready\n\nStrong support for model serving at scale in enterprise environments.\n\nComprehensive Ecosystem\n\nVisualization (TensorBoard), data processing (TFX), and model deployment pipelines.\n\n\nCross-Platform & Hardware Support\n\nEasily deploy models to cloud infrastructures, mobile devices, and specialized hardware (TPUs).\n\n\n\nTensorFlow’s ecosystem is appealing to those who need an end-to-end solution, from training to large-scale serving."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#key-differences",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#key-differences",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Key Differences",
    "text": "Key Differences\n\n\n\n\n\n\n\n\nAspect\nPyTorch\nTensorFlow\n\n\n\n\nComputation Graph\nDynamic graph (eager execution by default).\nHistorically static graph with a build-and-execute phase (now supports eager execution).\n\n\nDebugging & Development Style\nMore straightforward for Python developers, immediate error feedback.\nCan be trickier to debug in graph mode; eager mode helps but is relatively newer.\n\n\nDeployment & Production\nTorchServe and growing enterprise support, but historically overshadowed by TensorFlow’s tools.\nTensorFlow Serving, TensorFlow Lite, and easy Google Cloud integration.\n\n\n\n\nWhile the fundamental math and building blocks are similar, the biggest difference typically lies in how you prototype, debug, and deploy models."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#similarities",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#similarities",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Similarities",
    "text": "Similarities\n\n\n\n\n\n\n\nSimilarity\nDescription\n\n\n\n\nWide Range of Neural Network Layers\nConvolutional, Recurrent, Transformers, etc. Both frameworks maintain robust libraries for standard and advanced layers.\n\n\nAuto-Differentiation\nNo need to manually compute gradients; backpropagation is handled automatically.\n\n\nGPU Acceleration\nBoth leverage CUDA (NVIDIA GPUs) or other backends to speed up training.\n\n\nRich Communities\nAbundant tutorials, example code, pretrained models, and Q&A forums.\n\n\n\n\nDespite differing philosophies, PyTorch and TensorFlow share many core functionalities and have large, supportive user communities."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#comparison-of-advantages-and-disadvantages",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Advantages and Disadvantages",
    "text": "Comparison of Advantages and Disadvantages\n\n\n\n\n\n\n\n\n\nPyTorch\nTensorFlow\n\n\n\n\nAdvantages\n- Intuitive, Pythonic Syntax: Feels like standard Python, reducing friction for experimentation  - Dynamic Graph Execution: Simplifies debugging and model design  - Research & Academia Favorite: widely used in cutting-edge papers\n- Static Graph Optimization: Graph-based execution can be highly optimized for speed and memory usage  - Extensive Production Ecosystem: Includes TensorFlow Serving, TensorFlow Lite, TFX for data pipelines  - Large Corporate Adoption: Backed by Google, widely used in enterprise settings\n\n\nDisadvantages\n- Deployment Maturity: Production tooling and ecosystem are improving but still behind TensorFlow  - Smaller Enterprise Adoption: Historically overshadowed by TensorFlow’s widespread adoption\n- Learning Curve: The graph-based approach can be challenging for newcomers  - Historically Less Intuitive: Older APIs and tutorials can be confusing, though Eager Mode improves usability"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#advantages-of-pytorch",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#advantages-of-pytorch",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advantages of PyTorch",
    "text": "Advantages of PyTorch\n\nIntuitive, Pythonic Syntax\n\nFeels like standard Python, reducing friction for experimentation.\n\nDynamic Graph Execution\n\nSimplifies debugging and allows more flexible model definitions.\n\nResearch & Academia Favorite\n\nMany cutting-edge papers provide PyTorch code."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#disadvantages-of-pytorch",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#disadvantages-of-pytorch",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Disadvantages of PyTorch",
    "text": "Disadvantages of PyTorch\n\nDeployment Maturity\n\nProduction tooling and ecosystem are still catching up, though improving.\n\n\nSmaller Enterprise Adoption\n\nHistorically overshadowed by TensorFlow’s widespread adoption in industry.\n\n\n\nPyTorch usage in production is growing; major players like AWS have made big investments in PyTorch-based services."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#advantages-of-tensorflow",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#advantages-of-tensorflow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Advantages of TensorFlow",
    "text": "Advantages of TensorFlow\n\nStatic Graph Optimization\n\nGraph-based execution can be highly optimized for speed and memory usage.\n\nExtensive Production Ecosystem\n\nTensorFlow Serving, TensorFlow Lite, TensorFlow Extended (TFX) for data pipelines.\n\nLarge Corporate Adoption\n\nBacked by Google; widely used in enterprise scenarios."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#disadvantages-of-tensorflow",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#disadvantages-of-tensorflow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Disadvantages of TensorFlow",
    "text": "Disadvantages of TensorFlow\n\nLearning Curve\n\nGraph-based approach can be challenging for newcomers.\n\n\nHistorically Less Intuitive\n\nLegacy code and tutorials often rely on older APIs; can be confusing.\n\n\n\nWhile the static graph approach can be more complex, TensorFlow has made strides with eager execution to mirror PyTorch’s interactivity."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#recommendations",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#recommendations",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommendations",
    "text": "Recommendations\n\n\nChoose PyTorch if:\n\nYour focus is on rapid experimentation and academic research\nYou prioritize a Pythonic workflow and easy debugging\nYou prefer a dynamic graph approach (about it).\nYou are working on cutting-edge models with high flexibility\nYou value seamless interaction with Python libraries\n\n\nChoose TensorFlow if:\n\nYou need robust production and deployment pipelines\nYou plan to integrate with Google Cloud services\nYou require support for mobile/edge devices (e.g., TensorFlow Lite)\nYou benefit from static graph optimization for performance\nYou want an end-to-end ecosystem (TFX, TensorBoard, Serving)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#conclusion",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#conclusion",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Conclusion",
    "text": "Conclusion\n\nBoth Frameworks Are Necessary\n\nThey address core problems in deep learning: high-level APIs, automatic differentiation, and efficient hardware usage.\n\nDecision Factors\n\nProject goals (research vs. production), team expertise, ecosystem preference, and existing infrastructure.\n\nStay Flexible\n\nBoth PyTorch and TensorFlow evolve rapidly, so keep an eye on new releases and community developments.\n\n\n\nFramework preference often comes down to familiarity and project scope. Ultimately, both PyTorch and TensorFlow are powerful, widely adopted, and actively improving."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#recommendations-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#recommendations-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recommendations",
    "text": "Recommendations\n\nChoose PyTorch if:\n\nYour focus is on rapid experimentation and academic research.\n\nYou prioritize a more Pythonic workflow and easy debugging.\n\nYou prefer a dynamic graph approach.\n\nChoose TensorFlow if:\n\nYou need robust production and deployment pipelines from day one.\n\nYou plan to integrate with Google Cloud or rely on the larger enterprise ecosystem.\n\nYou appreciate or need the benefits of a static graph (e.g., performance optimizations, complex architecture)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#pytorch-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#pytorch-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PyTorch",
    "text": "PyTorch"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#tensors-in-pytorch",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#tensors-in-pytorch",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Tensors in PyTorch",
    "text": "Tensors in PyTorch"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#importing-libraries",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#importing-libraries",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\nimport torch\nimport numpy as np"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#initializing-a-tensor",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#initializing-a-tensor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Initializing a Tensor",
    "text": "Initializing a Tensor\nDirectly from data\nTensors can be created directly from data. The data type is automatically inferred.\n\ndata = [[1, 2], [3, 4]]\nx_data = torch.tensor(data)\n\nFrom a NumPy array\nTensors can be created from NumPy arrays (and vice versa).\n\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#attributes-of-a-tensor",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#attributes-of-a-tensor",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Attributes of a Tensor",
    "text": "Attributes of a Tensor\nTensor attributes describe their shape, datatype, and the device on which they are stored.\n\ntensor = torch.rand(3, 4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Operations on Tensors",
    "text": "Operations on Tensors\n\nOver 1200 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling, and more are comprehensively described in the PyTorch documentation.\nEach of these operations can run on the CPU or an accelerator such as CUDA, MPS, MTIA, or XPU. By default, tensors are created on the CPU. We can explicitly move tensors to an accelerator using the .to method (after checking for accelerator availability).\n\n\n\n# Example of moving a tensor to a current accelerator (if available)\nif torch.accelerator.is_available():\n    tensor = tensor.to(torch.accelerator.current_accelerator())"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#bridge-with-numpy",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#bridge-with-numpy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bridge with NumPy",
    "text": "Bridge with NumPy\nTensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.\nTensor to NumPy array\n\nt = torch.ones(5)\nprint(f\"t: {t}\")\nn = t.numpy()\nprint(f\"n: {n}\")\n\nA change in the tensor reflects in the NumPy array:\n\nt.add_(1)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#pytorch-and-tensor-flow",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#pytorch-and-tensor-flow",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PyTorch and Tensor Flow",
    "text": "PyTorch and Tensor Flow\n\n\n\nWhat is PyTorch?\n\nDeveloped primarily by Facebook (Meta) and released on September 2016.\nEmphasizes a dynamic computation graph (eager execution).\nHighly “Pythonic”: feels natural for Python developers.\nStrong community presence in academia and research.\n\n\n\nWhy is PyTorch Necessary?\n\nEase of Use & Debugging\n\nEvaluate expressions immediately without building a separate graph.\n\nMore intuitive for experimenting with complex, innovative models.\n\nResearch Focus\n\nQuickly prototype new ideas and iterate.\n\nActive Ecosystem\n\nLibraries like torchvision, torchaudio, and others for specialized tasks.\n\n\n\n\n\nHow to begin\n\nhttps://pytorch.org/tutorials/beginner/basics/intro.html.\nThere is also a YouTube Series (PyTorch Beginner Series) also here (Introduction to PyTorch)\n\n\n\n\n\nWhat is TensorFlow?\n\nDeveloped primarily by Google and released in November 2015.\nHistorically used a static graph approach (with an “eager mode” added later).\nComes with extensive tools for deployment (mobile, web, and production).\nLarge ecosystem with well-integrated components (e.g., TensorBoard, TFX, TensorFlow Lite).\n\n\n\n\nWhy is TensorFlow Necessary?\n\nProduction-Ready\n\nStrong support for model serving at scale in enterprise environments.\n\nComprehensive Ecosystem\n\nVisualization (TensorBoard), data processing (TFX), and model deployment pipelines.\n\n\nCross-Platform & Hardware Support\n\nEasily deploy models to cloud infrastructures, mobile devices, and specialized hardware (TPUs).\n\n\n\n\n\nHow to begin\n\nhttps://www.tensorflow.org/tutorials. There is also a Quick Start!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#comparison-of-advantages-and-disadvantages-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#comparison-of-advantages-and-disadvantages-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Advantages and Disadvantages",
    "text": "Comparison of Advantages and Disadvantages\n\n\nPyTorch\nAdvantages\n- Intuitive, Pythonic Syntax: Feels like standard Python, reducing friction for experimentation\n- Dynamic Graph Execution: Simplifies debugging and model design\n- Research & Academia Favorite: Widely used in cutting-edge papers\nDisadvantages\n- Deployment Maturity: Production tooling and ecosystem are improving but still lag behind TensorFlow\n- Smaller Enterprise Adoption: Historically overshadowed by TensorFlow’s widespread adoption\n\nTensorFlow\nAdvantages\n- Static Graph Optimization: Graph-based execution can be highly optimized for speed and memory usage\n- Extensive Production Ecosystem: Includes TensorFlow Serving, TensorFlow Lite, TFX for data pipelines\n- Large Corporate Adoption: Backed by Google, widely used in enterprise settings\nDisadvantages\n- Learning Curve: The graph-based approach can be challenging for newcomers\n- Historically Less Intuitive: Older APIs and tutorials can be confusing, though Eager Mode improves usability"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#pytorch-2",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#pytorch-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "PyTorch",
    "text": "PyTorch"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#comparison-of-advantages-and-disadvantages-2",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#comparison-of-advantages-and-disadvantages-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Comparison of Advantages and Disadvantages",
    "text": "Comparison of Advantages and Disadvantages\n\n\n\n\n\n\n\n\nFramework\nAdvantages\nDisadvantages\n\n\n\n\nPyTorch\n- Intuitive, Pythonic Syntax: Feels like standard Python, reduces friction for experimentation  - Dynamic Graph Execution: Simplifies debugging and model design  - Research & Academia Favorite: Widely used in cutting-edge papers\n- Deployment Maturity: Production tooling and ecosystem are improving but still behind TensorFlow  - Smaller Enterprise Adoption: Historically overshadowed by TensorFlow’s widespread adoption\n\n\nTensorFlow\n- Static Graph Optimization: Graph-based execution can be highly optimized for speed and memory usage  - Extensive Production Ecosystem: Includes TensorFlow Serving, TensorFlow Lite, TFX for data pipelines  - Large Corporate Adoption: Backed by Google, widely used in enterprise settings\n- Learning Curve: The graph-based approach can be challenging for newcomers  - Historically Less Intuitive: Older APIs and tutorials can be confusing, though eager mode improves usability\n\n\n\n\nWhile both frameworks excel at deep learning tasks, the choice often depends on your focus:\n- Research vs. Production\n- Existing Ecosystem & Tooling\n- Developer Familiarity\n\n\nPyTorch usage in production is growing; major players like AWS have made big investments in PyTorch-based services.\n\n\nWhile the static graph approach can be more complex, TensorFlow has made strides with eager execution to mirror PyTorch’s interactivity."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#initializing-a-tensor-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#initializing-a-tensor-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Initializing a Tensor",
    "text": "Initializing a Tensor\nFrom another tensor\nThe new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n\nx_ones = torch.ones_like(x_data)  # retains the properties of x_data\nprint(f\"Ones Tensor: \\n{x_ones}\\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float)  # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n{x_rand}\\n\")"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#initializing-a-tensor-2",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#initializing-a-tensor-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Initializing a Tensor",
    "text": "Initializing a Tensor\nWith random or constant values\nshape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n\nshape = (2, 3)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor}\\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor}\\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Operations on Tensors",
    "text": "Operations on Tensors\nIndexing and slicing\n\ntensor = torch.ones(4, 4)\nprint(f\"First row: {tensor[0]}\")\nprint(f\"First column: {tensor[:, 0]}\")\nprint(f\"Last column: {tensor[..., -1]}\")\n\ntensor[:, 1] = 0\nprint(tensor)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-2",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Operations on Tensors",
    "text": "Operations on Tensors\nJoining tensors\nYou can use torch.cat to concatenate a sequence of tensors along a given dimension. See also torch.stack, another tensor joining operator that is subtly different from torch.cat.\n\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-3",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Operations on Tensors",
    "text": "Operations on Tensors\nArithmetic operations\n\n# Matrix multiplication\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\ny3 = torch.rand_like(y1)\ntorch.matmul(tensor, tensor.T, out=y3)\n\n# Element-wise multiplication\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-4",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Operations on Tensors",
    "text": "Operations on Tensors\nSingle-element tensors\nIf you have a one-element tensor, for instance by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item():\n\nagg = tensor.sum()\nagg_item = agg.item()\nprint(agg_item, type(agg_item))"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-5",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#operations-on-tensors-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Operations on Tensors",
    "text": "Operations on Tensors\nIn-place operations\nOperations that store the result into the operand are called in-place. They are denoted by an _ suffix (e.g. x.copy_(y), x.t_()).\n\nprint(f\"{tensor}\\n\")\ntensor.add_(5)\nprint(tensor)\n\n\nNote: In-place operations save some memory but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#bridge-with-numpy-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#bridge-with-numpy-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Bridge with NumPy",
    "text": "Bridge with NumPy\nNumPy array to Tensor\n\nn = np.ones(5)\nt = torch.from_numpy(n)\n\nChanges in the NumPy array reflect in the tensor:\n\nnp.add(n, 1, out=n)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")\n\nXXXXXXXXXXXXXXXXXXXXXXXXXXx"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#datasets-dataloaders",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#datasets-dataloaders",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#transforms",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#transforms",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Transforms",
    "text": "Transforms"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#automatic-differentiation-with-torch.autograd",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Automatic Differentiation with torch.autograd",
    "text": "Automatic Differentiation with torch.autograd"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#optimizing-model-parameters",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#optimizing-model-parameters",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Optimizing Model Parameters",
    "text": "Optimizing Model Parameters"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#save-and-load-the-model",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#save-and-load-the-model",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Save and Load the Model",
    "text": "Save and Load the Model"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#xxx-youtube-pytorch-xxx",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#xxx-youtube-pytorch-xxx",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "XXX youtube pytorch XXX",
    "text": "XXX youtube pytorch XXX"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#introduction-to-pytorch---youtube-series",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#introduction-to-pytorch---youtube-series",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Introduction to PyTorch - YouTube Series",
    "text": "Introduction to PyTorch - YouTube Series\n\n\n\nPro tip: Use Colab with a GPU runtime to speed up operations Runtime &gt; Change runtime type &gt; GPU"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#cnn-introduction",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#cnn-introduction",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN: Introduction",
    "text": "CNN: Introduction\n\nNeural networks rebounded around 2010 with big successes in image classification.\nAround that time, massive databases of labeled images were being accumulated, with ever-increasing numbers of classes."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#the-cifar100-database",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#the-cifar100-database",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The CIFAR100 Database",
    "text": "The CIFAR100 Database\n\n\n\nThe figure shows 75 images drawn from the CIFAR100 database.\nThis database consists of 60,000 images labeled according to 20 superclasses (e.g. aquatic mammals), with five classes per superclass (beaver, dolphin, otter, seal, whale).\nEach image has a resolution of 32 × 32 pixels, with three eight-bit numbers per pixel representing red, green, and blue. The numbers for each image are organized in a three-dimensional array called a feature map.\nThe first two axes are spatial (both 32-dimensional), and the third is the channel axis, representing the three (blue, green or red) colors.\nThere is a designated training set of 50,000 images, and a test set of 10,000."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#the-convolutional-network-hierarchy",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#the-convolutional-network-hierarchy",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "The Convolutional Network Hierarchy",
    "text": "The Convolutional Network Hierarchy\n\n\n\nCNNs mimic, to some degree, how humans classify images, by recognizing specific features or patterns anywhere in the image that distinguish each particular object class.\nThe network first identifies low-level features in the input image, such as small edges or patches of color.\nThese low-level features are then combined to form higher-level features, such as parts of ears or eyes. Eventually, the presence or absence of these higher-level features contributes to the probability of any given output class.\nThis hierarchical construction is achieved by combining two specialized types of hidden layers: convolution layers and pooling layers:\nConvolution layers search for instances of small patterns in the image.\nPooling layers downsample these results to select a prominent subset.\nTo achieve state-of-the-art results, contemporary neural network architectures often use many convolution and pooling layers."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-layer",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#convolution-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Convolution Layer",
    "text": "Convolution Layer\n\n\nA convolution layer is made up of a large number of convolution filters, each of which is a template that determines whether a particular local feature is present in an image.\nA convolution filter relies on a very simple operation, called a convolution, which basically amounts to repeatedly multiplying matrix elements and then adding the results. \\[\n\\text{Input Image} =\n\\begin{bmatrix}\na & b & c \\\\\nd & e & f \\\\\ng & h & i \\\\\nj & k & l\n\\end{bmatrix}\n\\quad \\text{Convolution Filter} =\n\\begin{bmatrix}\n\\alpha & \\beta \\\\\n\\gamma & \\delta\n\\end{bmatrix}.\n\\]\nWhen we convolve the image with the filter, we get the result: \\[\n\\text{Convolved Image} =\n\\begin{bmatrix}\na\\alpha + b\\beta + d\\gamma + e\\delta & b\\alpha + c\\beta + e\\gamma + f\\delta \\\\\nd\\alpha + e\\beta + g\\gamma + h\\delta & e\\alpha + f\\beta + h\\gamma + i\\delta \\\\\ng\\alpha + h\\beta + j\\gamma + k\\delta & h\\alpha + i\\beta + k\\gamma + l\\delta\n\\end{bmatrix}.\n\\]\nthe convolution filter is applied to every 2 × 2 submatrix of the original image in order to obtain the convolved image.\nIf a 2 × 2 submatrix of the original image resembles the convolution filter, then it will have a large value in the convolved image; otherwise, it will have a small value. Thus, the convolved image highlights regions of the original image that resemble the convolution filter.\nThe filter is itself an image and represents a small shape, edge, etc.\nThe filters are learned during training."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#pooling-layer",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#pooling-layer",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Pooling Layer",
    "text": "Pooling Layer\nA pooling layer provides a way to condense a large image into a smaller summary image.\n\\[\n\\text{Max pool}\n\\begin{bmatrix}\n1 & 2 & 5 & 3 \\\\\n3 & 0 & 1 & 2 \\\\\n2 & 1 & 3 & 4 \\\\\n1 & 1 & 2 & 0\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n3 & 5 \\\\\n2 & 4\n\\end{bmatrix}\n\\]\n\nEach non-overlapping \\(2 \\times 2\\) block is replaced by its maximum.\nThis sharpens the feature identification.\nAllows for locational invariance.\nReduces the dimension by a factor of 4 — i.e., factor of 2 in each dimension."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#data-augmentation",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\n\n\nAn additional important trick used with image modeling is data augmentation.\nEssentially, each training image is replicated many times, with each replicate randomly distorted in a natural way such that human recognition is unaffected.\nTypical distortions are zoom, horizontal and vertical shift, shear, small rotations, and in this case horizontal flips.\nAt face value this is a way of increasing the training set considerably with somewhat different examples, and thus protects against overfitting.\nIn fact we can see this as a form of regularization: we build a cloud of images around each original image, all with the same label."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#cnn-example-pretrained-networks-to-classify-images",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "CNN Example: Pretrained Networks to Classify Images",
    "text": "CNN Example: Pretrained Networks to Classify Images\n\nHere we use the 50-layer resnet50 network trained on the 1000-class imagenet corpus to classify some photographs.\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#nn-example-mnist-digits",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#nn-example-mnist-digits",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "NN Example: MNIST Digits",
    "text": "NN Example: MNIST Digits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandwritten digits\n\n\\(28 \\times 28\\) grayscale images\n60K train, 10K test images\nFeatures are the 784 pixel grayscale values \\(\\in (0, 255)\\)\nLabels are the digit class \\(0\\text{–}9\\)\n\nGoal: Build a classifier to predict the image class.\nWe build a two-layer network with:\n\n256 units at the first layer,\n128 units at the second layer, and\n10 units at the output layer.\n\nAlong with intercepts (called biases), there are 235,146 parameters (referred to as weights).\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#document-classification-example-lasso-versus-neural-network-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews",
    "text": "Document Classification Example: Lasso versus Neural Network — IMDB Reviews\n\n\n\nSimpler lasso logistic regression model works as well as neural network in this case.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recurrent Neural Networks",
    "text": "Recurrent Neural Networks\n\nOften data arise as sequences:\n\nDocuments are sequences of words, and their relative positions have meaning.\nTime-series such as weather data or financial indices.\nRecorded speech or music.\nHandwriting, such as doctor’s notes.\n\nRNNs build models that take into account this sequential nature of the data and build a memory of the past.\n\nThe feature for each observation is a sequence of vectors \\(X = \\{X_1, X_2, \\ldots, X_L\\}\\).\nThe target \\(Y\\) is often of the usual kind — e.g., a single variable such as Sentiment, or a one-hot vector for multiclass.\nHowever, \\(Y\\) can also be a sequence, such as the same document in a different language."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks---rnn",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks---rnn",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recurrent Neural Networks - RNN",
    "text": "Recurrent Neural Networks - RNN\n\nOften data arise as sequences:\n\nDocuments are sequences of words, and their relative positions have meaning.\nTime-series such as weather data or financial indices.\nRecorded speech or music.\n\nRNNs build models that take into account this sequential nature of the data and build a memory of the past.\n\nThe feature for each observation is a sequence of vectors \\(X = \\{X_1, X_2, \\ldots, X_L\\}\\).\nThe target \\(Y\\) is often of the usual kind — e.g., a single variable such as Sentiment, or a one-hot vector for multiclass.\nHowever, \\(Y\\) can also be a sequence, such as the same document in a different language."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-for-document-classification-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-for-document-classification-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN for Document Classification",
    "text": "RNN for Document Classification\n\nThe document feature is a sequence of words \\(\\{\\mathcal{W}_\\ell\\}_{1}^{L}\\). We typically truncate/pad the documents to the same number \\(L\\) of words (we use \\(L = 500\\)).\nEach word \\(\\mathcal{W}_\\ell\\) is represented as a one-hot encoded binary vector \\(X_\\ell\\) (dummy variable) of length \\(10K\\), with all zeros and a single one in the position for that word in the dictionary.\nThis results in an extremely sparse feature representation and would not work well.\nInstead, we use a lower-dimensional pretrained word embedding matrix \\(\\mathbf{E}\\) (\\(m \\times 10K\\), next slide).\nThis reduces the binary feature vector of length \\(10K\\) to a real feature vector of dimension \\(m \\ll 10K\\) (e.g., \\(m\\) in the low hundreds)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-example-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-example-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN Example: IMDB Reviews",
    "text": "RNN Example: IMDB Reviews\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-time-series-forecasting-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-time-series-forecasting-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN: Time Series Forecasting",
    "text": "RNN: Time Series Forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew-York Stock Exchange Data\nThree daily time series for the period December 3, 1962, to December 31, 1986 (6,051 trading days):\n\nLog trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.\nDow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.\nLog volatility. This is based on the absolute values of daily price movements.\n\n\nGoal: predict Log trading volume tomorrow, given its observed values up to today, as well as those of Dow Jones return and Log volatility.\nThese data were assembled by LeBaron and Weigend (1998) IEEE Transactions on Neural Networks, 9(1): 213–220."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#when-to-use-deep-learning-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#when-to-use-deep-learning-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "When to Use Deep Learning",
    "text": "When to Use Deep Learning\n\n\nCNNs have had enormous successes in image classification and modeling, and are starting to be used in medical diagnosis. Examples include digital mammography, ophthalmology, MRI scans, and digital X-rays.\nRNNs have had big wins in speech modeling, language translation, and forecasting.\n\n\nShould we always use deep learning models?\n\nOften the big successes occur when the signal to noise ratio is high — e.g., image recognition and language translation. Datasets are large, and overfitting is not a big problem.\nFor noisier data, simpler models can often work better:\n\nOn the NYSE data, the AR(5) model is much simpler than an RNN, and performed as well.\nOn the IMDB review data, a linear model fit (e.g. with glmnet) did as well as the neural network, and better than the RNN."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#fitting-neural-networks-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#fitting-neural-networks-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Fitting Neural Networks",
    "text": "Fitting Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\min_{\\{w_k\\}_{1}^K, \\beta} \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2, \\quad \\text{where}\n\\]\n\\[\nf(x_i) = \\beta_0 + \\sum_{k=1}^K \\beta_k g\\left(w_{k0} + \\sum_{j=1}^p w_{kj} x_{ij}\\right).\n\\]\nThis problem is difficult because the objective is non-convex.\nDespite this, effective algorithms have evolved that can optimize complex neural network problems efficiently."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent---video",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent - Video",
    "text": "Gradient Descent - Video\n\n\n\n\n\n\nGradient descent, how neural networks learn"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-intuition---video",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-intuition---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Intuition - Video",
    "text": "Backpropagation Intuition - Video\n\n\n\n\n\n\nBackpropagation, intuitively"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-calculus---video",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-calculus---video",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Calculus - Video",
    "text": "Backpropagation Calculus - Video\n\n\n\n\n\n\nBackpropagation calculus"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent---video-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent---video-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent - Video",
    "text": "Gradient Descent - Video\n\n\n\n\n\n\n Gradient descent, how neural networks learn"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-intuition---video-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#backpropagation-intuition---video-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Backpropagation Intuition - Video",
    "text": "Backpropagation Intuition - Video\n\n\n\n\n\n\nBackpropagation, intuitively"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent---video-2",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#gradient-descent---video-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Gradient Descent - Video",
    "text": "Gradient Descent - Video\n\n\n\n\n\n\nGradient descent, how neural networks learn"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#word-embedding---rnn-example-imdb-reviews",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Word Embedding - RNN Example: IMDB Reviews",
    "text": "Word Embedding - RNN Example: IMDB Reviews\n\nReview:\n\nthis is one of the best films actually the best I have ever seen the film starts one fall day…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings are pretrained on very large corpora of documents, using methods similar to principal components. word2vec and GloVe are popular.\n\n\n\nLet’s code!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks---rnn-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#recurrent-neural-networks---rnn-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Recurrent Neural Networks - RNN",
    "text": "Recurrent Neural Networks - RNN\n\nOften data arise as sequences:\n\nDocuments are sequences of words, and their relative positions have meaning.\nTime-series such as weather data or financial indices.\nRecorded speech or music.\n\nRNNs build models that take into account this sequential nature of the data and build a memory of the past.\n\nThe feature for each observation is a sequence of vectors \\(X = \\{X_1, X_2, \\ldots, X_L\\}\\).\nThe target \\(Y\\) is often of the usual kind — e.g., a single variable such as Sentiment, or a one-hot vector for multiclass.\nHowever, \\(Y\\) can also be a sequence, such as the same document in a different language."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-time-series-forecasting",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#rnn-time-series-forecasting",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "RNN: Time Series Forecasting",
    "text": "RNN: Time Series Forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew-York Stock Exchange Data\nThree daily time series for the period December 3, 1962, to December 31, 1986 (6,051 trading days):\n\nLog trading volume. This is the fraction of all outstanding shares that are traded on that day, relative to a 100-day moving average of past turnover, on the log scale.\nDow Jones return. This is the difference between the log of the Dow Jones Industrial Index on consecutive trading days.\nLog volatility. This is based on the absolute values of daily price movements.\n\n\nGoal: predict Log trading volume tomorrow, given its observed values up to today, as well as those of Dow Jones return and Log volatility.\nThese data were assembled by LeBaron and Weigend (1998) IEEE Transactions on Neural Networks, 9(1): 213–220."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#flexibility-vs.-interpretability",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#flexibility-vs.-interpretability",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Flexibility vs. Interpretability",
    "text": "Flexibility vs. Interpretability\n\nTrade-offs between flexibility and interpretability:\n\n\n\n\n\n\n\n\n\nAs the authors suggest, I also endorse the Occam’s razor principle — we prefer simpler models if they work as well. More interpretable!"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nWhat Are We Doing?\nWe are defining a neural network class using PyTorch. This network is designed to work with images, specifically 28×28 grayscale images like those from the FashionMNIST dataset. The network will output 10 values, one for each digit from 0 to 9.\nStep-by-Step Breakdown\n\nclass NeuralNetwork(nn.Module):\n\nWe create a new neural network class called NeuralNetwork. It inherits from PyTorch’s nn.Module, which is the base class for all neural network models.\n\ndef __init__(self): and super().__init__()\n\n__init__ is the constructor. It’s run when we create the model.\nsuper().__init__() tells Python to also run the initialization code from the parent class (nn.Module). This is required for PyTorch to keep track of everything inside the model.\n\nself.flatten = nn.Flatten():\n\nchanges the input from a 2D image (28×28) into a 1D vector (784 values), which is easier for linear layers to handle."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-2",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-2",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\nHere we build the main body of the neural network.\nself.linear_relu_stack = nn.Sequential(\n    nn.Linear(28*28, 512),\n    nn.ReLU(),\n    nn.Linear(512, 512),\n    nn.ReLU(),\n    nn.Linear(512, 10),\n)\nIn most contexts when we say “how many layers?” we refer to the learnable ones. So this network has three fully‑connected (Linear) layers, with ReLU activations in between.\n\nYou can think of the linear layer as a filter that projects the image into a new space with 512 dimensions. These new values are not pixels anymore, but rather abstract features learned by the network."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-3",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-3",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nFirst layer nn.Linear(28*28, 512): takes the 784 values from the image and transforms them into 512 values. A Linear(784, 512) layer performs:\n\nA matrix multiplication between the input vector (length 784) and a weight matrix of size [784 × 512], followed by adding a bias vector of length 512.\nMathematically: \\[\n\\text{output} = x \\cdot W + b\n\\]\nx is the input vector: shape [784]\nW is the weight matrix: shape [784 × 512]\nb is the bias vector: shape [512]\nThe result (output) is a new vector of shape [512]\n\n\n\nEach of the 512 output values is a linear combination of all 784 pixel values in the input image. By default, PyTorch initializes weights using Kaiming Uniform Initialization (a variant of He initialization), which works well with ReLU activation functions."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-4",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-4",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nnn.ReLU(): applies the ReLU activation function, which keeps positive numbers and turns negative numbers into zero. This adds non-linearity to the model.\nSecond layernn.Linear(512, 512): takes those 512 values and again outputs 512 values. This is a hidden layer, helping the model learn more complex patterns.\nnn.ReLU(): Another non-linear transformation.\nThird (Final) layer:nn.Linear(512, 10): takes the 512 values and produces 10 output values.\n\nThese are called logits, and each one corresponds to a digit class (0 to 9)."
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-5",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-5",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\nDefine a sequence of layers:\n\nforward(self, x): This is the forward pass, the function that runs when we send data through the model.\nStep-by-step:\n\n\nx = self.flatten(x): Convert the 28×28 image into a 1D tensor with 784 values.\nlogits = self.linear_relu_stack(x): Pass the input through the series of layers.\nreturn logits: Output the final predictions (raw scores for each class).\n\n\nIn summary this neural network:\n\nTakes an image (28×28) as input,\nFlattens it into a vector,\nPasses it through two fully connected layers with ReLU,\nOutputs a vector of size 10 (one for each digit)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-6",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-6",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\ntorch.rand(1, 28, 28, device=device): Creates a random image with shape [1, 28, 28]\n\n1 is the batch size (just one image)\n28×28 is the image dimension\ndevice=device ensures the tensor goes to CPU or GPU (wherever the model is)\n\n\n\n\n# To see tensor:\nprint(X)\n\n\n\nLet’s say the tensor shown is:\nX = torch.tensor([[\n    [0.1177, 0.2669, 0.6367, 0.6148, 0.3085, ...],  # row 0\n    [0.8672, 0.3645, 0.4822, 0.9566, 0.8999, ...],  # row 1\n    ...\n]])\n\n\nThis is a 3D tensor of shape [1, 28, 28]:\n\nThe first dimension 1 is the batch size,\nThe next two are height and width of the image.\n\nThe full index of 0.2669 in the 3D tensor is: X[0, 0, 1].\n\n0 → first (and only) image in the batch\n0 → first row of the image\n1 → second column in that row"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-7",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#build-the-neural-network-7",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Build the Neural Network",
    "text": "Build the Neural Network\n\n\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data.\nExample:\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\n# To see the image:\nimport torch\nimport matplotlib.pyplot as plt\n\n# Remove the batch dimension (1, 28, 28) → (28, 28)\nimage = X[0]\n\n# Plot the image\nplt.imshow(image, cmap='gray')  # Use 'gray' colormap for grayscale image\nplt.title(\"Random 28x28 Image\")\nplt.axis('off')\nplt.show()\n\n\n\nlogits = model(X): This calls the model with input X.\n\nBehind the scenes, it runs model.forward(X)\nOutput: a vector of 10 values (called logits), one for each class (digits 0 through 9)\n\nNote: We do not call model.forward() directly — PyTorch manages hooks and gradients when we use model(X)\npred_probab = nn.Softmax(dim=1)(logits): Applies softmax to the raw output logits\n\nSoftmax turns logits into probabilities (values between 0 and 1 that sum to 1)\ndim=1 means we apply softmax across the 10 output class values (not across the batch)\n\ny_pred = pred_probab.argmax(1): Picks the index of the largest probability, i.e., the predicted class\n\nargmax(1) returns the class with the highest probability from each row (here we have just one row)\n\nprint(f\"Predicted class: {y_pred}\"): Prints the predicted digit class (0 through 9)"
  },
  {
    "objectID": "lecture_slides/10_deep_learning/10_deep_learning.html#datasets-dataloaders-1",
    "href": "lecture_slides/10_deep_learning/10_deep_learning.html#datasets-dataloaders-1",
    "title": " MGMT 47400: Predictive Analytics ",
    "section": "Datasets & DataLoaders",
    "text": "Datasets & DataLoaders\n\n\n\nThe code below extracts a single image‑tensor from the training_data used in the tutorial (you can use test_data the same way), prints its basic properties, and visualizes it.\n\n\nimport torch\nimport matplotlib.pyplot as plt\n\n# Choose the index of the image you wish to inspect\nidx = 0  # e.g., the first image; change as desired\n\n# Fetch the sample\nimage_tensor, label = training_data[idx]   # image_tensor is a 1×28×28 tensor\n\n# Inspect the raw tensor values\nprint(\"Shape :\", image_tensor.shape)  # torch.Size([1, 28, 28])\nprint(\"Label :\", label) # integer class id\nprint(\"Tensor (first 5 rows):\\n\", image_tensor[0, :5, :])\n\n# Visualize the image\nplt.imshow(image_tensor.squeeze(), cmap=\"gray\")\nplt.title(f\"Fashion‑MNIST class{label}\")\nplt.axis(\"off\")\nplt.show()\n\n\n\nHow it works\n\nIndex selection – set idx to any integer in range(len(training_data)).\n\nDataset access – indexing the dataset returns (image, label) with the transform already applied (here, ToTensor() scales to [0,1]).\n\nInspection – the printed tensor slice lets you verify pixel values, and plt.imshow renders the sample for visual confirmation.\n\n\nTo see a different image you just need to adjust the index."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Analysis of Panel Data",
    "section": "",
    "text": "This course aims to provide an overview of statistical methods appropriate for the analysis of longitudinal data, or data collected on multiple units (individuals, states, dyads, countries) at more than one point in time. The course will focus on models for the analysis of “panel data,” which by convention is used to describe data with a relatively large number of units and relatively few time points. The emphasis throughout will be on taking advantage of the benefits that panel data provide the researcher in making inferences about causal dynamics, while at the same time being sensitive to the specific problems and complexities that emerge when conducting panel analyses. On the benefits side, panel data provides the researcher with increased ability to:\n\nmodel directly individual-level change and growth in dependent variables\nestimate models that control for unmeasured unit-specific effects or ‘unobserved heterogeneity’\ntest alternative lag structures and models of reciprocal causality between variables\nestimate models that specify and account for variation in individual-level intercepts, slopes and/or rates of change over time\nestimate causal effects after controlling for the confounding effects of measurement error\n\nWe’ll begin by situating panel analysis within a general framework for causal inference. We’ll then provide an overview of panel models from each of the three traditional approaches that dominate the field: the “econometric” tradition emphasizing unobserved heterogeneity; the “structural equation” tradition emphasizing models with reciprocal causality and measurement error; and the “multilevel modeling” tradition emphasizing models with longitudinal growth and random coefficients. As will become clear, these traditions are increasingly converging, and it is more and more common now in the panel literature to see models that incorporate features of each. It is also the case that nearly all panel models from each tradition can now be estimated in Stata 15 (which we will use in the course) and a wide range of other statistical software packages.\nA note on the mathematical/statistical difficulty in the course. There will not be an emphasis on derivations of appropriate statistical estimators and so forth; rather, the emphasis will be on grasping the underlying logic of the various models, understanding how, when, and why to use them to achieve the goals specified above in your own research, and learning how to profit from, and to critique, published works in the discipline that make use of these techniques. There will be a reasonable amount of mathematics, formulas, etc., that will be needed to understand the various models and methods, but all of it will be presented in ways that, ideally, will help guide your own research endeavors. I am assuming only that you have had basic courses in regression and the linear model (i.e., the equivalent of PS2030)."
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "PS2701 - Longitudinal Analysis",
    "section": "",
    "text": "Instructor: Steven E. Finkel\nSemester: Fall 2019\n\nMeeting Time: Tuesday 9:30–11:55 AM\n\nLocation: 4430 Posvar Hall\n\nOffice: 4804 Posvar Hall\n\nOffice Hours: Tuesday 1–3 PM\n\nEmail: finkel@pitt.edu\n\nPhone: 412-648-7283"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Analysis of Panel Data",
    "section": "",
    "text": "This course aims to provide an overview of statistical methods appropriate for the analysis of longitudinal data, or data collected on multiple units (individuals, states, dyads, countries) at more than one point in time. The course will focus on models for the analysis of “panel data,” which by convention is used to describe data with a relatively large number of units and relatively few time points. The emphasis throughout will be on taking advantage of the benefits that panel data provide the researcher in making inferences about causal dynamics, while at the same time being sensitive to the specific problems and complexities that emerge when conducting panel analyses. On the benefits side, panel data provides the researcher with increased ability to:\n\nmodel directly individual-level change and growth in dependent variables\nestimate models that control for unmeasured unit-specific effects or ‘unobserved heterogeneity’\ntest alternative lag structures and models of reciprocal causality between variables\nestimate models that specify and account for variation in individual-level intercepts, slopes and/or rates of change over time\nestimate causal effects after controlling for the confounding effects of measurement error\n\nWe’ll begin by situating panel analysis within a general framework for causal inference. We’ll then provide an overview of panel models from each of the three traditional approaches that dominate the field: the “econometric” tradition emphasizing unobserved heterogeneity; the “structural equation” tradition emphasizing models with reciprocal causality and measurement error; and the “multilevel modeling” tradition emphasizing models with longitudinal growth and random coefficients. As will become clear, these traditions are increasingly converging, and it is more and more common now in the panel literature to see models that incorporate features of each. It is also the case that nearly all panel models from each tradition can now be estimated in Stata 15 (which we will use in the course) and a wide range of other statistical software packages.\nA note on the mathematical/statistical difficulty in the course. There will not be an emphasis on derivations of appropriate statistical estimators and so forth; rather, the emphasis will be on grasping the underlying logic of the various models, understanding how, when, and why to use them to achieve the goals specified above in your own research, and learning how to profit from, and to critique, published works in the discipline that make use of these techniques. There will be a reasonable amount of mathematics, formulas, etc., that will be needed to understand the various models and methods, but all of it will be presented in ways that, ideally, will help guide your own research endeavors. I am assuming only that you have had basic courses in regression and the linear model (i.e., the equivalent of PS2030)."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "PS2701 - Longitudinal Analysis",
    "section": "3 Prerequisites",
    "text": "3 Prerequisites\nBasic knowledge of regression and the linear model (e.g., PS2030 or equivalent)."
  },
  {
    "objectID": "syllabus.html#required-texts",
    "href": "syllabus.html#required-texts",
    "title": "PS2701 - Longitudinal Analysis",
    "section": "4 Required Texts",
    "text": "4 Required Texts\n\nAllison, Paul. Fixed Effects Regression Models. 2009. Sage.\nFinkel, Steven E. Causal Analysis with Panel Data. 1995. Sage.\nAndress, Golsch, & Schmidt. Applied Panel Data Analysis for Economic and Social Surveys. 2013. Springer."
  },
  {
    "objectID": "syllabus.html#supplemental-references",
    "href": "syllabus.html#supplemental-references",
    "title": "Analysis of Panel Data",
    "section": "Supplemental References",
    "text": "Supplemental References\n\n\nAcock, Alan C. 2013. Discovering Structural Equation Modeling Using Stata. College Station, Tx.: Stata Press.\nAngrist, Joshua D. and Jőrn-Steffen Pischke, Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press: 2009.\nKaplan, David. 2002. Structural Equation Modeling: Foundations and Extensions. Thousand Oaks, Ca.: Sage Publications.\nNewsom, Jason. 2015. Longitudinal Structural Equation Modeling: A Comprehensive Introduction. New York: Routledge.\nSinger, Judith D. and John B. Willett. 2003. Applied Longitudinal Data Analysis: Modeling Change and Event Occurrence. New York: Oxford University Press.\nRabe-Hesketh, Sophia, and Anders Skrondal. 2012. Multilevel and Longitudinal Analysis with Stata, Volume 1: Continuous Responses. 3rd Edition. College Station, Tx.: Stata Press."
  },
  {
    "objectID": "syllabus.html#course-requirements",
    "href": "syllabus.html#course-requirements",
    "title": "Analysis of Panel Data",
    "section": "Course Requirements",
    "text": "Course Requirements\n\nGrades will be based on a 20–25 page research paper (40%), two homework exercises which relate to specific statistical methods and problems we will discuss (25% each), and an oral presentation (with PowerPoint and/or related materials) of your research paper (10%). The paper will be a quantitative analysis, using methods from this course, of longitudinal panel data that you will collect or access from social science archives or other sources. The paper should have some substantive interest to you or be relevant to your studies in the graduate program; ideally, you can think of it as the first draft of a master’s thesis, convention paper, or possible journal publication. The paper will discuss your basic theoretical framework, your hypotheses, statistical models, results, possible problems with the analysis and what you may have done to correct or account for these problems. It will conclude with a discussion of the relevance of your findings for the general topic and for future research.\nThe homework exercises will be periodic problems or data to analyze and will illustrate aspects of the statistical techniques being covered in class."
  },
  {
    "objectID": "syllabus.html#course-outline",
    "href": "syllabus.html#course-outline",
    "title": "Analysis of Panel Data",
    "section": "Course Outline",
    "text": "Course Outline\n\nThe course is organized by units and then topics within units. We will maintain a certain amount of flexibility with the schedule, so that we can spend more time on some topics/units and scale back on others as circumstances warrant.\n\nUnit 1: Causal Inference and Econometric Panel Models\n\n1. Introduction to Panel Analysis and Causal Inference\n\nAngrist, Joshua D. and Jőrn-Steffen Pischke. 2009. Mostly Harmless Econometrics. Princeton University Press, Chapters 2 and 5.\n\nAllison, Paul D. 1994. Using Panel Data to Estimate the Effect of Events. Sociological Methods and Research 23(2): 174-199.\nAllison, Paul D. 2009. Fixed Effects Regression Models, Chapters 1-2.\n\n\n\n2. Fixed and Random Effects Models\n\nHalaby, Charles. 2004. “Panel Models in Sociological Research: Theory into Practice.” Annual Review of Sociology 30: 535–44. - Andress, Golsch and Schmidt. 2013. Applied Panel Data Analysis for Economic and Social Surveys, Chapters 3–4.\n\nRabe-Hesketh and Skrondal. Multilevel and Longitudinal Modeling Using Stata, Chapters 2–3; Chapter 5 to p.25.\n\nAllison, Paul D. 2019. “Asymmetric Fixed-effects Models for Panel Data.” Socius 5: 1–2.\n\n\n\n3. Hybrid Models\n\nFirebaugh, Glenn, Warner, C., and Massoglia, M. 2013. “Fixed Effects, Random Effects, and Hybrid Models for Causal Analysis.” In S.L. Morgan (Ed.), Handbook of Causal Inference for Social Research, pp. 113–132. New York: Springer.\n\nBell, Andrew, and Kelvyn Jones. 2015. “Explaining Fixed Effects: Random Effects Modeling of Time Series Cross-Section and Panel Data.” Political Science Research and Methods 3(1): 133–153.\n\n\n\n4. Dynamic Panel Models\n\nCastro-Schilo, Laura, and Kevin J. Grimm. 2018. “Using Residualized Change Versus Difference Scores for Longitudinal Research.” Journal of Social and Relationships 35(1): 32–58.\n\nVaisey, Stephen, and Andrew Miles. 2014. “What You Can—and Can’t—Do with Three-Wave Panel Data.” Sociological Methods and Research.\n\nWawro, Gregory. 2002. “Estimating Dynamic Panel Models in Political Science.” Political Analysis 10: 25–48.\n\nRabe-Hesketh and Skrondal. Multilevel and Longitudinal Modeling Using Stata, pp. 250–257; 269–282.\n\nImai, Kosuke, and In Song Kim. 2019. “When Should We Use Unit Fixed Effects Regression Models for Causal Inference with Longitudinal Data?” American Journal of Political Science.\n\n\n\n\nUnit 2: Structural Equation Panel Models\n\n1. Introduction to Structural Equation Modeling\n\nFinkel, Steven E. 1995. Causal Analysis with Panel Data, Chapter 1 and Appendix.\n\nKaplan, David. 2002. Structural Equation Modeling, Chapters 2 and 6.\n\nHooper, Daire, Coughlan, Joseph, and Mullen, Michael R. 2008. “Structural Equation Modelling: Guidelines for Determining Model Fit.” The Electronic Journal of Business Research Methods 6: 53–60.\n\n\n\n2. Models with Reciprocal Causality\n\nFinkel, Steven E. 1995. Causal Analysis with Panel Data, Chapters 2–3.\n\nNewsom, Jason. 2015. Longitudinal Structural Equation Modeling, Chapters 4–5.\n\n\n\n3. Measurement Error Models\n\nFinkel, Steven E. 1995. Causal Analysis with Panel Data, Chapter 4.\n\nLittle, Todd D. 2013. Longitudinal Structural Equation Modeling. New York: The Guilford Press, Chapter 5.\n\nKaplan, David. 2002. Structural Equation Modeling, Chapter 3.\n\n\n\n4. SEM Fixed Effects, Random Effects, and Dynamic Panel Models\n\nAllison, Paul D. 2009. Fixed Effects Regression Models, Chapter 6.\n\nFinkel, Steven E. 1995. Causal Analysis with Panel Data, Chapter 5.\n\nBollen, Kenneth A., and Jennie E. Brand. 2010. “A General Panel Model with Fixed and Random Effects: A Structural Equations Approach.” Social Forces 89(1): 1–34.\n\nAllison, Paul D., Richard Williams, and Enrique Moral-Benito. 2017. “Maximum Likelihood for Dynamic Panel Models with Cross-Lagged Effects.” Socius 3: 1–17.\n\nHamaker, Ellen L., Rebecca Kuiper, and Raoul Grasman. 2015. “A Critique of the Cross-Lagged Panel Model.” Psychological Methods 20(1): 102–116.\n\n\n\n\nUnit 3: Multilevel Longitudinal Models\n\n1. Longitudinal Growth and Mixed Models\n\nAndress, Golsch, and Schmidt. 2013. Applied Panel Data Analysis for Economic and Social Surveys, pp. 180–202.\n\nSinger, Judith D., and John B. Willett. 2003. Applied Longitudinal Data Analysis: Modeling Change and Event Occurrence, Chapters 2–4.\n\nRabe-Hesketh, Sophia, and Anders Skrondal. 2012. Multilevel and Longitudinal Modeling Using Stata, Chapter 5 and Chapter 7 up to p. 364.\n\nWang, Lijuan Peggy, and Scott E. Maxwell. 2015. “On Disaggregating Between-Person and Within-Person Effects with Longitudinal Data Using Multilevel Models.” Psychological Methods 20(1): 63–83.\nLudwig, Volker, and Josef Brüderl. 2018. “Is There a Male Marriage Premium? New Evidence from the United States.” American Sociological Review 83(4): 744–770.\n\n\n\n2. Latent Curve Models and Multilevel-SEM Syntheses\n\nSinger, Judith D., and John B. Willett. 2003. Applied Longitudinal Data Analysis: Modeling Change and Event Occurrence, Chapter 8.\n\nNewsom, Jason. 2015. Longitudinal Structural Equation Modeling, Chapter 7.\n\nCurran, Patrick J., Andrea L. Howard, Sierra Bainter, Stephanie T. Lane, and James S. McGinley. 2014. “The Separation of Between-Person and Within-Person Components of Individual Change Over Time: A Latent Curve Model with Structured Residuals.” Journal of Consulting and Clinical Psychology 82(5): 879–894.\n\nBianconcini, Silvia, and Kenneth A. Bollen. 2018. “The Latent Variable–Autoregressive Latent Trajectory Model: A General Framework for Longitudinal Data Analysis.” Structural Equation Modeling.\n\nUsami, Satoshi, Kou Murayama, and Ellen L. Hamaker. 2019. “A Unified Framework of Longitudinal Models to Examine Reciprocal Relations.” Psychological Methods.\n\n\n\n3. Multilevel Analysis of Repeated Cross-Sectional Data\n\nFairbrother, Malcolm. 2014. “Two Multilevel Modeling Techniques for Analyzing Comparative Longitudinal Survey Datasets.” Political Science Research and Methods 2(1): 119–140.\n\nSchmidt-Catran, Alexander W., and Malcolm Fairbrother. 2016. “The Random Effects in Multilevel Models: Getting Them Wrong and Getting Them Right.” European Sociological Review 32(1): 23–38.\n\n\n\n\nUnit 4: Topics in Longitudinal Analysis\n\n1. Panel Models for Non-Continuous Dependent Variables\n\nAndress, Golsch, and Schmidt. 2013. Applied Panel Data Analysis for Economic and Social Surveys, Chapter 5.\n\nAllison, Paul D. 2009. Fixed Effects Regression Models, Chapter 3.\nRabe-Hesketh, Sophia, and Anders Skrondal. 2012. Multilevel and Longitudinal Modeling Using Stata, Volume II: Categorical Responses, Counts and Survival, Chapters 10–11.\n\n\n\n2. Longitudinal Mediation Models\n\nSelig, James P., and Kristopher J. Preacher. 2009. “Mediation Models for Longitudinal Data in Developmental Research.” Research in Human Development 6(2–3): 144–169.\n\nPreacher, Kristopher J., Michael J. Zyphur, and Zhen Zhang. 2010. “A General Multilevel SEM Framework for Assessing Multilevel Mediation.” Psychological Methods 15(3): 209–233.\n\nMuthén, Bengt, and Tihomir Asparouhov. 2015. “Causal Effects in Mediation Modeling: An Introduction with Applications to Latent Variables.” Structural Equation Modeling 22(1): 12–23.\nBlackwell, Matthew. 2013. “A Framework for Dynamic Causal Inference in Political Science.” American Journal of Political Science 57(2): 504–520.\n\n\n\n3. Models for Panel Attrition\n\nBaulch, Bob, and Agnes Quisumbing. 2011. “Testing and Adjusting for Attrition in Household Panel Data.” Chronic Poverty Research Centre Toolkit Note.\n\nFoster, E. Michael, Grace Y. Fang, and Conduct Problems Research Group. 2004. “Alternative Methods for Handling Attrition: An Illustration Using Data from the Fast Track Evaluation.” Evaluation Review 28(5): 434–464.\n\nNewsom, Jason. 2015. Longitudinal Structural Equation Modeling, Chapter 13.\n\nEnders, Craig. 2010. “Models for Missing Not at Random Data.” In Applied Missing Data Analysis, Chapter 10. New York: The Guilford Press."
  },
  {
    "objectID": "syllabus - Copia.html#course-description-and-objectives",
    "href": "syllabus - Copia.html#course-description-and-objectives",
    "title": "Syllabus",
    "section": "Course Description and Objectives",
    "text": "Course Description and Objectives\nThe course enables students to navigate the entire predictive analytics pipeline skillfully—from data preparation and exploration to modeling, assessment, and interpretation. Throughout the course, learners engage with real-world examples and hands-on labs emphasizing essential programming and analytical skills. By exploring topics such as linear and logistic regression, classification, resampling methods, regularization techniques, tree-based approaches, support vector machines, and advanced learning paradigms (including neural networks and unsupervised methods), participants gain a robust theoretical understanding and practical experience. Ultimately, students will leave the course equipped to apply predictive models to data-driven problems, communicate their findings to diverse audiences, and critically evaluate model performance to inform strategic decision-making across various business contexts.\nCourse Website: https://davi-moreira.github.io/2025S_predictive_analytics_MGMT474/"
  },
  {
    "objectID": "syllabus - Copia.html#instructor",
    "href": "syllabus - Copia.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\n\nInstructor: Professor Davi Moreira\n\nEmail: dmoreira@purdue.edu\nOffice: Young Hall 414\nVirtual Office hours: Zoom link in your Course Brightspace Page\nIndividual Appointments: Book time with me through the link in the course syllabus on your Course Brightspace Page or by appointment."
  },
  {
    "objectID": "syllabus - Copia.html#learning-outcomes",
    "href": "syllabus - Copia.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the conclusion of this course, students will be able to:\n\nExplain Core Predictive Analytics Concepts: Articulate key principles of statistical learning and predictive analytics, including fundamental terminology, modeling strategies, and the role of data-driven insights in business contexts.\nPrepare and Explore Data Effectively: Demonstrate proficiency in cleaning, organizing, and exploring datasets, applying tools and techniques for data preprocessing, feature engineering, and exploratory analysis.\nImplement Diverse Modeling Techniques: Construct predictive models using linear and logistic regression, classification methods, resampling procedures, and regularization techniques.\nAssess and Interpret Model Performance: Evaluate the accuracy, robustness, and interpretability of predictive models, critically examining issues such as overfitting, bias-variance trade-offs, and cross-validation results.\nCommunicate Analytical Findings: Present analytical outcomes and model interpretations to technical and non-technical audiences, crafting clear, concise, and visually effective reports or presentations.\nIntegrate Predictive Analytics into Decision-Making: Recommend actionable strategies based on model findings, demonstrating the ability to align analytical results with organizational objectives and inform evidence-based decision processes."
  },
  {
    "objectID": "syllabus - Copia.html#course-materials",
    "href": "syllabus - Copia.html#course-materials",
    "title": "Syllabus",
    "section": "Course Materials",
    "text": "Course Materials\n\nTextbooks (Required): ISLP James, G., Witten, D., Hastie, T., & Tibshirani, R. (2023). An Introduction to Statistical Learning with Applications in Python. Springer. https://doi.org/10.1007/978-1-0716-2926-2. Download here: https://www.statlearning.com/\nComputing (Required): A laptop or desktop with internet access and the capability to run Python code through Google Colab: https://colab.research.google.com/.\nSoftware (Required): Google Colab is a cloud-based platform that requires no software installation on your local machine; it is accessible through a modern web browser such as Google Chrome, Mozilla Firefox, Microsoft Edge, or Safari. To use Google Colab, you need a Google account and a stable internet connection. While optional, having tools like a local Python installation (e.g., Anaconda) or a Python IDE (e.g., Jupyter Notebook or VS Code) can be helpful for offline development. Additionally, browser extensions, such as those for VS Code integration, can enhance your experience but are not required. This makes Google Colab convenient and easy for Python programming and data science tasks.\n\n\nCourse Infra-structure\nBrightspace: The Course Brightspace Page https://purdue.brightspace.com/ should be checked on a regular basis for announcements and course material."
  },
  {
    "objectID": "syllabus - Copia.html#assessments",
    "href": "syllabus - Copia.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\nAs part of a university-wide initiative, the Business School has adopted an Official Grading Policy that caps the overall class GPA at 3.3. Final letter grades are determined by curving final percentages, subject to any extra-credit exceptions discussed in this syllabus. While you will see your final percentage in Brightspace, individual grade thresholds will not be disclosed before official submissions.\n\n\n\nAssessment\nWeight\n\n\n\n\nAttendance/Participation\n10%\n\n\nQuizzes\n20%\n\n\nHomework\n30%\n\n\nFinal Project\n40%\n\n\n\n\nAttendance and Participation\nAttend class, participate in activities, and complete any participatory exercises. Random attendance checks will be used to measure involvement. According to Purdue regulations, students are expected to attend every class/lab meeting for which they are registered.\n\n\nQuizzes\nRegular quizzes based on lecture material will be administered, with no drops. Due dates and details will be on Brightspace. Quizzes help reinforce content and maintain steady engagement.\n\n\nHomework\nHomework assignments offer practical, hands-on exposure to data mining tasks. Expect multiple-choice questions requiring analysis of provided results. Deadlines will be posted in Brightspace. These assignments are crucial for building technical and analytical skills.\n\n\nFinal Project\nIn groups, students will complete a practical predictive analytics project culminating in a poster presentation at the Undergraduate Research Conference. A comprehensive set of project guidelines will be provided, and the assessment structure will adhere to the following criteria:\n\nMilestone Deliverables (40%): Students will submit incremental project components on specific due dates. These deliverables allow for early feedback and ensure steady progress throughout the semester. Grades will reflect each milestone’s clarity, completeness, and timely submission.\nPeer Evaluation (20%): To encourage accountability and productive teamwork, students will evaluate their peers’ contributions. These assessments help ensure balanced participation and measure collaborative effectiveness.\nPoster Presentation at the Purdue Undergraduate Research Conference (40%): A poster template and assessment rubric will be shared, and you are encouraged to review previous award-winning student posters for inspiration. Your final posters must be submitted by the due date indicated in the syllabus, after which they will be printed and distributed during a dedicated Poster Presentation Preparation class. Additional details on the conference can be found at https://www.purdue.edu/undergrad-research/conferences/index.php. As the event may not coincide with our regular class time, please communicate with your other course instructors in advance regarding potential scheduling conflicts. If any issues arise, please let me know. We will not hold our usual class immediately following the Poster Presentation, allowing you time to rest and catch up on other coursework. Consult the course schedule for further details.\n\n\n\nGrade Challenges\nGrades and solutions will be posted soon after each assignment deadline. Students have 7 calendar days from the grade posting to submit any challenge (3 days for the final two quizzes and homework assignments). Challenges must be based on legitimate discrepancies regarding data mining principles or grading accuracy.\n\nReview posted solutions thoroughly.\n\nIf you suspect an error, email Dr. Moreira with:\n\nCourse name, section, and lecture day/time\n\nYour name and Student ID\n\nAssignment/Exam Title or Number\n\nSpecific deduction questioned\n\nClear rationale referencing solutions or rubrics\n\n\n\nNo grades will be discussed in-class. Please use office hours for clarifications. After the 7-day (or 3-day) window, grades are final."
  },
  {
    "objectID": "syllabus - Copia.html#course-policies-and-additional-details",
    "href": "syllabus - Copia.html#course-policies-and-additional-details",
    "title": "Syllabus",
    "section": "Course Policies and Additional Details",
    "text": "Course Policies and Additional Details\n\nExtra Credit Opportunities\n\nCheck the Course Syllabus document on Brightspace for details.\n\n\n\nAI Policy\n\nYou may use AI tools to support your learning (e.g., clarifying concepts, generating examples), but:\n\nDo not use AI for requesting solutions or exams.\n\nPractice refining prompts to get better AI outputs.\n\nVerify all AI-generated content for accuracy.\n\nCite any AI usage in your documents.\n\n\n\n\nAdditional Information\nRefer to Brightspace for deadlines, academic integrity policies, accommodations, CAPS information, and non-discrimination statements.\n\n\nSubject to Change Policy\nWhile we will endeavor to maintain the course schedule, the syllabus may be adjusted to accommodate the learning pace and needs of the class."
  },
  {
    "objectID": "syllabus - Copia.html#schedule",
    "href": "syllabus - Copia.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "syllabus.html#texts",
    "href": "syllabus.html#texts",
    "title": "Analysis of Panel Data",
    "section": "Texts",
    "text": "Texts\n\n\nAllison, Paul. 2009. Fixed Effects Regression Models. Thousand Oaks, Ca.: Sage Publications.\nFinkel, Steven E. 1995. Causal Analysis with Panel Data. Thousand Oaks, Ca.: Sage Publications. (Royalties donated to Pi Sigma Alpha, Political Science Honor Society).\nAndress, Golsch, and Schmidt. 2013. Applied Panel Data Analysis for Economic and Social Surveys. Heidelberg: Springer-Verlag."
  },
  {
    "objectID": "material.html",
    "href": "material.html",
    "title": "Materials",
    "section": "",
    "text": "Unit\nTopic\nLecture\nSupplementary Materials\n\n\n\n\nUnit 1 - Causal Inference and Econometric Panel Models\nIntroduction to Panel Analysis and Causal Inference\nLecture 1"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "Unit\n\n\nTopic\n\n\nLecture\n\n\nSupplementary Materials\n\n\n\n\n\n\nUnit 1 – Causal Inference and Econometric Panel Models\n\n\nIntroduction to Panel Analysis and Causal Inference\n\n\nLecture 1\n\n\nStata Script Data\n\n\n\n\n\n\nFixed and Random Effects Models\n\n\nLecture 2\n\n\nStata Script Data\n\n\n\n\n\n\nHybrid Models\n\n\nLecture 3\n\n\nStata Script Data\n\n\n\n\n\n\nDynamic Panel Models\n\n\nLecture 4\n\n\nStata Script Data\n\n\n\n\n\n\nUnit 2 – Structural Equation Panel Models\n\n\nIntroduction to Structural Equation Modeling\n\n\nLecture 5\n\n\nStata Script Data\n\n\n\n\n\n\nModels with Reciprocal Causality\n\n\nLecture 6\n\n\nStata Script Data\n\n\n\n\n\n\nMeasurement Error Models\n\n\nLecture 7\n\n\nStata Script Data\n\n\n\n\n\n\nSEM Fixed Effects, Random Effects, and Dynamic Panel Models\n\n\nLecture 8\n\n\nStata Script Data\n\n\n\n\n\n\nUnit 3 – Multilevel Longitudinal Models\n\n\nLongitudinal Growth and Mixed Models\n\n\nLecture 9 Lecture 10\n\n\nStata Script Data\n\n\n\n\n\n\nMultilevel Analysis of Repeated Cross-Sectional Data\n\n\nLecture 11\n\n\nStata Script Data\n\n\n\n\n\n\nLatent Curve Models and Multilevel-SEM Syntheses\n\n\nLecture 12\n\n\nStata Script Data\n\n\n\n\n\n\nUnit 4 – Topics in Longitudinal Analysis\n\n\nPanel Models for Non-Continuous Dependent Variables\n\n\nLecture 13\n\n\nStata Script Data\n\n\n\n\n\n\nLongitudinal Mediation Models\n\n\nLecture 14\n\n\nStata Script Data\n\n\n\n\n\n\nModels for Panel Attrition\n\n\nLecture 15\n\n\nStata Script Data"
  }
]